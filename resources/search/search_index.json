{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nextflow Training","text":"<p>Welcome to the Nextflow community training portal!</p> <p>These training materials are open source and free to use for anyone. They are hosted in a GitHub repository along with example scripts and development configuration.</p> <p>Whilst you can follow the materials any time, you'll probably get most out of them by joining an organised training event. Free online events are organised regularly with the nf-core community, see the nf-core events page for more.</p> <p></p>"},{"location":"#available-workshops","title":"Available workshops","text":"<p>We have several workshops available on this website. Find the one that's right for you:</p> <p>Basic Nextflow Training Workshop</p> <p> This is the primary Nextflow training material used in most Nextflow and nf-core training events.</p> <p>Basic training for all things Nextflow. Perfect for anyone looking to get to grips with using Nextflow to run analyses and build workflows.</p> <p>Start the Nextflow Training Workshop </p> <p>Hands on training</p> <p> This course is quite short and hands-on, great if you want to practice your Nextflow skills.</p> <p>A \"learn by doing\" tutorial with less focus on theory, instead leading through exercises of slowly increasing complexity.</p> <p>Launch the Hands on Training </p>"},{"location":"#resources","title":"Resources","text":"<p>Quick reference to some handy links:</p> Reference \u00a0Community Nextflow Docs Nextflow Slack Nextflow Homepage nf-core Seqera Labs Training source on GitHub <p>Not sure where to go? Check out the Getting help page.</p>"},{"location":"#credits-and-contributions","title":"Credits and contributions","text":"<p>All training material was originally written by Seqera Labs but has been made open-source (CC BY-NC-ND) for the community.</p> <p>We welcome fixes and improvements from the community. Every page has a  icon in the top right of the page, which will take you to GitHub where you can edit the training source material via a pull-request.</p> <p></p> <p></p>"},{"location":"help.fr/","title":"Obtenir de l'aide","text":""},{"location":"help.fr/#documentation-nextflow","title":"Documentation Nextflow","text":"<p>M\u00eame les d\u00e9veloppeurs les plus comp\u00e9tents ont besoin de documentation. Nextflow n'est pas diff\u00e9rent et dispose d'une excellente documentation. Vous pouvez trouver les documents de Nextflow \u00e0 l'adresse suivante : https://nextflow.io/docs/latest/ - nous vous recommandons de les garder ouverts dans un onglet pendant que vous suivez la formation !</p>"},{"location":"help.fr/#slack","title":"Slack","text":"<p>Si vous avez des difficult\u00e9s avec la formation, n'h\u00e9sitez pas \u00e0 demander de l'aide. Notre formidable communaut\u00e9 est l'une des grandes forces de Nextflow !</p> <p>Il existe deux instances Slack :</p> <ul> <li>Nextflow (register here)</li> <li>nf-core (register here)</li> </ul> <p>En g\u00e9n\u00e9ral, le slack Nextflow est le meilleur car il s'adresse \u00e0 l'ensemble de la communaut\u00e9. La seule exception est si vous suivez la formation dans le cadre d'un atelier organis\u00e9 par nf-core, auquel cas on vous aura indiqu\u00e9 o\u00f9 poser vos questions.</p>"},{"location":"help.fr/#demandez-aux-professionnels","title":"Demandez aux professionnels","text":"<p>Nextflow est un logiciel libre et gratuit, d\u00e9velopp\u00e9 par Seqera Labs. Seqera offre un service de support professionnel pour Nextflow et les produits associ\u00e9s, ainsi que des sessions de formation sur mesure.</p> <p>Si cela vous int\u00e9resse, veuillez Prendre contact.</p>"},{"location":"help/","title":"Getting Help","text":""},{"location":"help/#nextflow-documentation","title":"Nextflow Documentation","text":"<p>Even the most proficient developers need documentation. Nextflow is no different, and has an excellent set of docs. You can find the Nextflow docs at https://nextflow.io/docs/latest/ - we recommend keeping them open in a tab whilst you follow the training!</p>"},{"location":"help/#slack","title":"Slack","text":"<p>If you're struggling with the training please don't hesitate to reach out for help. Our amazing community is one of the great strengths of Nextflow!</p> <p>There are two relevant Slack instances:</p> <ul> <li>Nextflow (register here)</li> <li>nf-core (register here)</li> </ul> <p>Generally, the Nextflow slack is best as it caters for the entire community. The exception is if you're following the training with a workshop organised by nf-core, in which case you should have been told about where to ask questions.</p>"},{"location":"help/#ask-the-professionals","title":"Ask the professionals","text":"<p>Nextflow is a free and open-source software, developed by Seqera Labs. Seqera offers a professional support service for Nextflow and associated products, as well as running bespoke training sessions.</p> <p>If this sounds like something that might be of interest, please Get in touch.</p>"},{"location":"help.pt/","title":"Conseguindo ajuda","text":""},{"location":"help.pt/#documentacao-do-nextflow","title":"Documenta\u00e7\u00e3o do Nextflow","text":"<p>Mesmo os desenvolvedores mais proficientes precisam de documenta\u00e7\u00e3o. Com o Nextflow n\u00e3o funciona diferente, e ele possui uma excelente documenta\u00e7\u00e3o. Voc\u00ea pode encontrar a documenta\u00e7\u00e3o do Nextflow em https://nextflow.io/docs/latest/ - recomendamos mant\u00ea-la aberta em uma aba enquanto voc\u00ea segue o treinamento!</p>"},{"location":"help.pt/#slack","title":"Slack","text":"<p>Se voc\u00ea est\u00e1 tendo dificuldades com o treinamento, n\u00e3o hesite em pedir ajuda. Nossa incr\u00edvel comunidade \u00e9 um dos grandes pontos fortes do Nextflow!</p> <p>Existem duas inst\u00e2ncias relevantes do Slack:</p> <ul> <li>Nextflow (inscreva-se aqui)</li> <li>nf-core (inscreva-se aqui)</li> </ul> <p>Geralmente, o Slack do Nextflow \u00e9 melhor, pois atende a toda a comunidade. A exce\u00e7\u00e3o \u00e9 se voc\u00ea estiver seguindo o treinamento atrav\u00e9s de um workshop organizado pelo projeto nf-core. Nesse caso, voc\u00ea deve ter sido informado sobre onde fazer perguntas.</p>"},{"location":"help.pt/#pergunte-aos-profissionais","title":"Pergunte aos profissionais","text":"<p>O Nextflow \u00e9 um software gratuito e de c\u00f3digo aberto, desenvolvido pela Seqera Labs. A Seqera oferece um servi\u00e7o de suporte profissional para o Nextflow e produtos associados, al\u00e9m de realizar sess\u00f5es de treinamento sob medida.</p> <p>Se isso soa como algo que pode ser do seu interesse, por favor entre em contato conosco.</p>"},{"location":"index.es/","title":"Capacitaci\u00f3n en Nextflow","text":"<p>\u00a1Bienvenido al portal de capacitaci\u00f3n de la comunidad Nextflow!</p> <p>Estos materiales de capacitaci\u00f3n son de c\u00f3digo abierto y de uso gratuito para cualquier persona. Est\u00e1n alojados en un repositorio de GitHub junto con scripts de ejemplo y configuraci\u00f3n de desarrollo.</p> <p>Si bien puedes aprender con estos materiales en cualquier momento, probablemente los aproveches al m\u00e1ximo al participar en una sesi\u00f3n de entrenamiento. La comunidad de nf-core organiza regularmente eventos online gratuitos; consulta la p\u00e1gina de eventos de nf-core para obtener m\u00e1s informaci\u00f3n.</p> <p></p>"},{"location":"index.es/#talleres-disponibles","title":"Talleres disponibles","text":"<p>Tenemos varios talleres disponibles en este sitio web. Encuentra el adecuado para ti:</p> <p>Taller de Entrenamiento B\u00e1sico de Nextflow</p> <p> Este es el principal material de capacitaci\u00f3n de Nextflow utilizado en la mayor\u00eda de los eventos de capacitaci\u00f3n de Nextflow y nf-core.</p> <p>Entrenamiento b\u00e1sico para todo lo relacionado con Nextflow. Perfecto para cualquiera que busque familiarizarse con el uso de Nextflow para ejecutar an\u00e1lisis y construir pipelines.</p> <p>Comienza ahora el entrenamiento de Nextflow </p> <p>Entrenamiento pr\u00e1ctico</p> <p> Este material es bastante corto y pr\u00e1ctico, excelente si desea practicar sus habilidades de Nextflow.</p> <p>Un tutorial para \"aprender haciendo\" con menos enfoque en la teor\u00eda, gui\u00e1ndote hacia ejercicios que aumentan en complejidad.</p> <p>Comienza el entrenamiento pr\u00e1ctico </p>"},{"location":"index.es/#recursos","title":"Recursos","text":"<p>Referencia r\u00e1pida a algunos enlaces \u00fatiles:</p> Referencias Comunidad Documentaci\u00f3n de Nextflow Slack de Nextflow P\u00e1gina principal de Nextflow nf-core Seqera Labs C\u00f3digo fuente en GitHub <p>\u00bfNo est\u00e1s seguro por d\u00f3nde comenzar? Consulta la p\u00e1gina de ayuda.</p>"},{"location":"index.es/#creditos-y-contribuciones","title":"Cr\u00e9ditos y contribuciones","text":"<p>Todo el material de capacitaci\u00f3n fue escrito originalmente por Seqera Labs pero se ha hecho de c\u00f3digo abierto (CC BY-NC-ND) para la comunidad.</p> <p>Damos la bienvenida a las correcciones y mejoras de la comunidad. Cada p\u00e1gina tiene un \u00edcono  en la parte superior derecha de la p\u00e1gina, que te llevar\u00e1 a GitHub, donde puedes editar el material a trav\u00e9s de un Pull Request.</p> <p></p> <p></p>"},{"location":"index.fr/","title":"Formation Nextflow","text":"<p>Bienvenue sur le portail de formation de la communaut\u00e9 Nextflow !</p> <p>Ces supports de formation sont libres et gratuits pour tout le monde. Ils sont h\u00e9berg\u00e9s dans un repositoire GitHub avec des scripts d'exemple et une configuration de d\u00e9veloppement.</p> <p>Bien que vous puissiez suivre le mat\u00e9riel \u00e0 tout moment, vous en tirerez probablement le meilleur parti en participant \u00e0 un \u00e9v\u00e9nement de formation organis\u00e9. Des \u00e9v\u00e9nements gratuits en ligne sont organis\u00e9s r\u00e9guli\u00e8rement avec la communaut\u00e9 nf-core, voir la page des \u00e9v\u00e9nements nf-core pour en savoir plus.</p> <p></p>"},{"location":"index.fr/#ateliers-disponibles","title":"Ateliers disponibles","text":"<p>Plusieurs ateliers sont disponibles sur ce site. Trouvez celui qui vous convient le mieux :</p> <p>Atelier de formation de base Nextflow</p> <p> Il s'agit du principal mat\u00e9riel de formation Nextflow utilis\u00e9 dans la plupart des formations Nextflow et nf-core.</p> <p>Formation de base pour tout ce qui concerne Nextflow. Parfaite pour toute personne souhaitant se familiariser avec l'utilisation de Nextflow pour effectuer des analyses et construire des workflows.</p> <p>D\u00e9marrer l'atelier de formation Nextflow </p> <p>Formation pratique</p> <p> Ce cours est assez court et pratique, id\u00e9al si vous souhaitez mettre en pratique vos comp\u00e9tences Nextflow.</p> <p>Un didacticiel \"learn by doing\" qui met moins l'accent sur la th\u00e9orie et propose plut\u00f4t des exercices d'une complexit\u00e9 croissante.</p> <p>Lancer la formation pratique </p>"},{"location":"index.fr/#resources","title":"Resources","text":"<p>R\u00e9f\u00e9rence rapide de quelques liens utiles :</p> Reference \u00a0Communit\u00e9 Documents Nextflow Slack Nextflow  Page d'accueil de Nextflow nf-core Seqera Labs Lien de la formation sur GitHub <p>Vous ne savez pas o\u00f9 aller ? Consultez la page Obtenir de l'aide.</p>"},{"location":"index.fr/#credits-et-contributions","title":"Cr\u00e9dits et contributions","text":"<p>Tous les documents de formation ont \u00e9t\u00e9 r\u00e9dig\u00e9s \u00e0 l'origine par Seqera Labs mais il a \u00e9t\u00e9 mis en libre acc\u00e8s (CC BY-NC-ND) pour la communaut\u00e9.</p> <p>Les corrections et am\u00e9liorations apport\u00e9es par la communaut\u00e9 sont les bienvenues. Chaque page a un  l'ic\u00f4ne en haut \u00e0 droite de la page, qui vous conduira \u00e0 GitHub o\u00f9 vous pourrez \u00e9diter le mat\u00e9riel source de la formation par le biais d'une pull-request.</p> <p></p> <p></p>"},{"location":"index.pt/","title":"Treinamentos Nextflow","text":"<p>Seja bem vindo ao portal de treinamento da comunidade do Nextflow!</p> <p>Esses materiais de treinamento s\u00e3o de c\u00f3digo aberto e gratuitos para qualquer pessoa utilizar. Eles est\u00e3o hospedados em um reposit\u00f3rio do GitHub junto a exemplos de scripts e de configura\u00e7\u00f5es de desenvolvimento.</p> <p>Ainda que voc\u00ea possa consultar e estudar esse material quando bem quiser, voc\u00ea provavelmente ir\u00e1 aproveit\u00e1-lo mais ao participar de um evento de treinamento. Eventos online e gratuitos s\u00e3o regularmente organizados pela comunidade do nf-core. Veja mais na p\u00e1gina de eventos do nf-core.</p> <p></p>"},{"location":"index.pt/#treinamentos-disponiveis","title":"Treinamentos dispon\u00edveis","text":"<p>N\u00f3s temos v\u00e1rios treinamentos dispon\u00edveis neste site. Encontre o que se adequa mais ao que voc\u00ea est\u00e1 procurando:</p> <p>Treinamento B\u00e1sico de Nextflow</p> <p> Esse \u00e9 o material de treinamento prim\u00e1rio do Nextflow utilizado na maior parte dos eventos de treinamento do Nextflow e do nf-core.</p> <p>Treinamento b\u00e1sico cobrindo um pouquinho de tudo do Nextflow. Perfeito para quem quer se familiarizar com o uso do Nextflow para executar an\u00e1lises e construir fluxos de trabalho.</p> <p>Comece j\u00e1 o treinamento do Nextflow </p> <p>Treinamento pr\u00e1tico</p> <p> Esse material \u00e9 mais curto e bem pr\u00e1tico, o que \u00e9 perfeito se voc\u00ea quiser praticar suas habilidades com o Nextflow.</p> <p>Um tutorial de \"aprender fazendo\" com menos foco na teoria, atrav\u00e9s de exerc\u00edcios de complexidade cada vez maior.</p> <p>Comece o treinamento pr\u00e1tico </p>"},{"location":"index.pt/#leia-mais","title":"Leia mais","text":"<p>De refer\u00eancias r\u00e1pidas \u00e0 links \u00fateis:</p> Refer\u00eancias \u00a0Comunidade Documenta\u00e7\u00e3o oficial do Nextflow Slack do Nextflow P\u00e1gina ofiical do Nextflow nf-core Seqera Labs C\u00f3digo-fonte deste portal no GitHub <p>N\u00e3o sabe por onde come\u00e7ar? Confira a p\u00e1gina Conseguindo ajuda.</p>"},{"location":"index.pt/#creditos-e-contribuicoes","title":"Cr\u00e9ditos e contribui\u00e7\u00f5es","text":"<p>Todo o material de treinamento foi escrito originalmente pela Seqera Labs mas foi compartilhado livremente (CC BY-NC-ND) para a comunidade.</p> <p>Corre\u00e7\u00f5es e melhorias pela comunidade s\u00e3o bem vindas. Toda p\u00e1gina tem o \u00edcone  em seu topo direito, que ir\u00e1 te levar ao GitHub onde voc\u00ea poder\u00e1 edit\u00e1-la atrav\u00e9s de um Pull Request.</p> <p></p> <p></p>"},{"location":"basic_training/","title":"Welcome","text":"<p>We are excited to have you on the path to writing reproducible and scalable scientific workflows using Nextflow. This guide complements the full Nextflow documentation - if you ever have any doubts, please refer to that.</p>"},{"location":"basic_training/#objectives","title":"Objectives","text":"<p>By the end of this course you should:</p> <ol> <li>Be proficient in writing Nextflow workflows</li> <li>Know the basic Nextflow concepts of Channels, Processes and Operators</li> <li>Have an understanding of containerized workflows</li> <li>Understand the different execution platforms supported by Nextflow</li> <li>Be introduced to the Nextflow community and ecosystem</li> </ol>"},{"location":"basic_training/#follow-the-training-videos","title":"Follow the training videos","text":"<p>In our latest nf-core training in March, 2023, we used this training material to teach Nextflow and the recording is available for you to watch at your pace in multiple languages.</p> <p>Check the links below for the YouTube videos in the available languages:</p> <ul> <li> English recording</li> <li> Hindi recording</li> <li> Spanish recording</li> <li> Portuguese recording</li> <li> French recording</li> </ul>"},{"location":"basic_training/#overview","title":"Overview","text":"<p>To get you started with Nextflow as quickly as possible, we will walk through the following steps:</p> <ol> <li>Set up a development environment to run Nextflow</li> <li>Explore Nextflow concepts using some basic workflows, including a multi-step RNA-Seq analysis</li> <li>Build and use Docker containers to encapsulate all workflow dependencies</li> <li>Dive deeper into the core Nextflow syntax, including Channels, Processes, and Operators</li> <li>Cover cluster and cloud deployment scenarios and explore Nextflow Tower capabilities</li> </ol> <p>This will give you a broad understanding of Nextflow, to start writing your own workflows. We hope you enjoy the course! This is an ever-evolving document - feedback is always welcome.</p>"},{"location":"basic_training/cache_and_resume.fr/","title":"Cache d'ex\u00e9cution et reprise","text":"<p>Le m\u00e9canisme de mise en cache de Nextflow fonctionne en attribuant un identifiant unique \u00e0 chaque t\u00e2che qui est utilis\u00e9 pour cr\u00e9er un r\u00e9pertoire d'ex\u00e9cution distinct o\u00f9 les t\u00e2ches sont ex\u00e9cut\u00e9es et les r\u00e9sultats stock\u00e9s.</p> <p>L'identifiant unique de la t\u00e2che est g\u00e9n\u00e9r\u00e9 sous la forme d'une valeur de hachage de 128 bits compos\u00e9e des valeurs d'entr\u00e9e de la t\u00e2che, des fichiers et de la cha\u00eene de commande.</p> <p>Le workflow du r\u00e9pertoire work est organis\u00e9 comme indiqu\u00e9 ci-dessous :</p> <pre><code>work/\n\u251c\u2500\u2500 12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1adacb582d2198cd32db0e6f808bce\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.fa -&gt; /data/../genome.fa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 hash.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 header.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 indexing.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 quasi_index.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 refInfo.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 rsd.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sa.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 txpInfo.bin\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 versionInfo.json\n\u251c\u2500\u2500 19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 663679d1d87bfeafacf30c1deaf81b\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aux_info\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ambig_info.tsv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fld.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 observed_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 observed_bias_3p.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 cmd_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 libParams\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 flenDist.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 lib_format_counts.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 salmon_quant.log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 quant.sf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_1.fq -&gt; /data/../ggal_gut_1.fq\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_2.fq -&gt; /data/../ggal_gut_2.fq\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index -&gt; /data/../asciidocs/day2/work/12/1adacb582d2198cd32db0e6f808bce/index\n</code></pre> <p>Info</p> <p>Vous pouvez cr\u00e9er ces plots en utilisant la fonction <code>tree</code> si vous l'avez install\u00e9e. Sur les syst\u00e8mes d'exploitation bas\u00e9s sur Debian, il suffit de <code>sudo apt install -y tree</code> ou, pour macOS, d'utiliser Homebrew : <code>brew install tree</code></p>"},{"location":"basic_training/cache_and_resume.fr/#comment-fonctionne-la-reprise","title":"Comment fonctionne la reprise","text":"<p>L'option de ligne de commande <code>-resume</code> permet de poursuivre l'ex\u00e9cution d'un workflow \u00e0 partir de la derni\u00e8re \u00e9tape qui s'est achev\u00e9e avec succ\u00e8s :</p> <pre><code>nextflow run &lt;script&gt; -resume\n</code></pre> <p>En pratique, le workflow est ex\u00e9cut\u00e9 depuis le d\u00e9but. Cependant, avant de lancer l'ex\u00e9cution d'un processus, Nextflow utilise l'identifiant unique de la t\u00e2che pour v\u00e9rifier si le r\u00e9pertoire work existe d\u00e9j\u00e0 et s'il contient un \u00e9tat de sortie de commande valide avec les fichiers de sortie attendus.</p> <p>Si cette condition est remplie, l'ex\u00e9cution de la t\u00e2che est saut\u00e9e et les r\u00e9sultats calcul\u00e9s pr\u00e9c\u00e9demment sont utilis\u00e9s comme r\u00e9sultats du processus.</p> <p>La premi\u00e8re t\u00e2che pour laquelle une nouvelle sortie est calcul\u00e9e invalide toutes les ex\u00e9cutions en aval dans le DAG restant.</p>"},{"location":"basic_training/cache_and_resume.fr/#repertoire-work","title":"repertoire Work","text":"<p>Les r\u00e9pertoires de travail des t\u00e2ches sont cr\u00e9\u00e9s par d\u00e9faut dans le dossier <code>work</code> du chemin de lancement. Ce dossier est cens\u00e9 \u00eatre une zone de stockage scratch qui peut \u00eatre nettoy\u00e9e une fois le calcul termin\u00e9.</p> <p>Note</p> <p>Les r\u00e9sultats finaux du workflow sont cens\u00e9s \u00eatre stock\u00e9s dans un emplacement diff\u00e9rent sp\u00e9cifi\u00e9 \u00e0 l'aide d'une ou plusieurs directives publishDir.</p> <p>Warning</p> <p>Veillez \u00e0 supprimer votre r\u00e9pertoire work de temps en temps, sinon votre machine/environnement risque d'\u00eatre rempli de fichiers inutilis\u00e9s.</p> <p>Un emplacement diff\u00e9rent pour le r\u00e9pertoire d'ex\u00e9cution work peut \u00eatre sp\u00e9cifi\u00e9 en utilisant l'option de ligne de commande <code>-w</code>. Par exemple :</p> <pre><code>nextflow run &lt;script&gt; -w /some/scratch/dir\n</code></pre> <p>Warning</p> <p>Si vous supprimez ou d\u00e9placez le workflow du r\u00e9pertoire work, cela emp\u00eachera l'utilisation de la fonction de resume lors des ex\u00e9cutions suivantes.</p> <p>Le code de hachage des fichiers d'entr\u00e9e est calcul\u00e9 en utilisant :</p> <ul> <li>le chemin d'acc\u00e8s complet au fichier</li> <li>la taille du fichier</li> <li>l'horodatage de la derni\u00e8re modification</li> </ul> <p>Par cons\u00e9quent, le simple fait de  toucher un fichier invalidera l'ex\u00e9cution de la t\u00e2che correspondante.</p>"},{"location":"basic_training/cache_and_resume.fr/#comment-organiser-des-experiences-in-silico","title":"Comment organiser des exp\u00e9riences in-silico ?","text":"<p>Il est conseill\u00e9 d'organiser chaque exp\u00e9rience dans son propre dossier. Les principaux param\u00e8tres d'entr\u00e9e de l'exp\u00e9rience doivent \u00eatre sp\u00e9cifi\u00e9s en utilisant un fichier de configuration Nextflow. Cela facilite le suivi et la r\u00e9plication d'une exp\u00e9rience dans le temps.</p> <p>Note</p> <p>Dans la m\u00eame exp\u00e9rience, le m\u00eame flux de travail peut \u00eatre ex\u00e9cut\u00e9 plusieurs fois, cependant, le lancement simultan\u00e9 de deux (ou plus) instances Nextflow dans le m\u00eame r\u00e9pertoire doit \u00eatre \u00e9vit\u00e9.</p> <p>La commande <code>nextflow log</code> liste les ex\u00e9cutions effectu\u00e9es dans le dossier courant :</p> <pre><code>$ nextflow log\n\nTIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND\n2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello\n2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker\n2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf\n2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker\n</code></pre> <p>Vous pouvez utiliser l'identifiant de session ou le nom d'ex\u00e9cution pour r\u00e9cup\u00e9rer une ex\u00e9cution sp\u00e9cifique. Par exemple:</p> <pre><code>nextflow run rnaseq-nf -resume mighty_boyd\n</code></pre>"},{"location":"basic_training/cache_and_resume.fr/#provenance-de-lexecution","title":"Provenance de l'ex\u00e9cution","text":"<p>La commande <code>log</code>, lorsqu'elle est fournie avec un nom d'ex\u00e9cution ou un identifiant de session, peut renvoyer de nombreuses informations utiles sur l'ex\u00e9cution d'un workflow qui peuvent \u00eatre utilis\u00e9es pour cr\u00e9er un rapport de provenance.</p> <p>Par d\u00e9faut, il \u00e9num\u00e8re les r\u00e9pertoires de travail utilis\u00e9s pour calculer chaque t\u00e2che. Par exemple :</p> <pre><code>$ nextflow log tiny_fermat\n\n/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99\n/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d\n/data/.../work/3b/3485d00b0115f89e4c202eacf82eba\n</code></pre> <p>L'option <code>-f</code> (fields) peut \u00eatre utilis\u00e9e pour sp\u00e9cifier quelles m\u00e9tadonn\u00e9es doivent \u00eatre imprim\u00e9es par la commande <code>log</code>. Par exemple :</p> <pre><code>$ nextflow log tiny_fermat -f 'process,exit,hash,duration'\nindex    0   7b/3753ff  2.0s\nfastqc   0   c1/56a36d  9.3s\nfastqc   0   f7/659c65  9.1s\nquant    0   82/ba67e3  2.7s\nquant    0   e5/2816b9  3.2s\nmultiqc  0   3b/3485d0  6.3s\n</code></pre> <p>La liste compl\u00e8te des domaines disponibles peut \u00eatre consult\u00e9e \u00e0 l'aide de la commande :</p> <pre><code>nextflow log -l\n</code></pre> <p>L'option <code>-F</code> permet de sp\u00e9cifier des crit\u00e8res de filtrage pour n'imprimer qu'un sous-ensemble de t\u00e2ches. Par exemple :</p> <pre><code>$ nextflow log tiny_fermat -F 'process =~ /fastqc/'\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n</code></pre> <p>Cela peut \u00eatre utile pour localiser les r\u00e9pertoires de travail d'une t\u00e2che sp\u00e9cifique.</p> <p>Enfin, l'option <code>-t</code> permet de cr\u00e9er un rapport de provenance personnalis\u00e9 de base, en affichant un fichier mod\u00e8le dans le format de votre choix. Par exemple:</p> <pre><code>&lt;div&gt;\n&lt;h2&gt;${name}&lt;/h2&gt;\n&lt;div&gt;\n        Script:\n        &lt;pre&gt;${script}&lt;/pre&gt;\n&lt;/div&gt;\n&lt;ul&gt;\n&lt;li&gt;Exit: ${exit}&lt;/li&gt;\n&lt;li&gt;Status: ${status}&lt;/li&gt;\n&lt;li&gt;Work dir: ${workdir}&lt;/li&gt;\n&lt;li&gt;Container: ${container}&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> <p>Exercise</p> <p>Sauvegardez l'extrait ci-dessus dans un fichier nomm\u00e9 <code>template.html</code>. Lancez ensuite cette commande (en utilisant l'identifiant correct pour votre ex\u00e9cution, par exemple <code>tiny_fermat</code>) :</p> <pre><code>nextflow log tiny_fermat -t template.html &gt; prov.html\n</code></pre> <p>Enfin, ouvrez le fichier <code>prov.html</code> avec un navigateur.</p>"},{"location":"basic_training/cache_and_resume.fr/#depanner-la-reprise","title":"D\u00e9panner la reprise","text":"<p>La possibilit\u00e9 de reprendre les workflows est une fonctionnalit\u00e9 cl\u00e9 de Nextflow, mais elle ne fonctionne pas toujours comme vous l'attendez. Dans cette section, nous allons passer en revue quelques raisons courantes pour lesquelles Nextflow peut ignorer vos r\u00e9sultats mis en cache.</p> <p>Tip</p> <p>Pour en savoir plus sur le m\u00e9canisme de reprise et sur la mani\u00e8re de r\u00e9soudre les probl\u00e8mes, veuillez consulter les trois articles de blog suivants :</p> <ol> <li>D\u00e9mystifier la reprise de  Nextflow</li> <li>Depanner la reprise Nextflow resume</li> <li>Analyse du comportement des pipelines en mati\u00e8re de mise en cache</li> </ol>"},{"location":"basic_training/cache_and_resume.fr/#fichier-dentree-modifie","title":"Fichier d'entr\u00e9e modifi\u00e9","text":"<p>Assurez-vous qu'il n'y a pas de changement dans le(s) fichier(s) d'entr\u00e9e. N'oubliez pas que le hachage unique de la t\u00e2che est calcul\u00e9 en tenant compte du chemin complet du fichier, de l'horodatage de la derni\u00e8re modification et de la taille du fichier. Si l'une de ces informations a chang\u00e9, le workflow sera r\u00e9ex\u00e9cut\u00e9 m\u00eame si le contenu d'entr\u00e9e est identique.</p>"},{"location":"basic_training/cache_and_resume.fr/#un-processus-modifie-une-entree","title":"Un processus modifie une entr\u00e9e","text":"<p>Un processus ne doit jamais modifier les fichiers d'entr\u00e9e, sinon le <code>resume</code> pour les ex\u00e9cutions futures sera invalid\u00e9 pour la m\u00eame raison que celle expliqu\u00e9e dans le point pr\u00e9c\u00e9dent.</p>"},{"location":"basic_training/cache_and_resume.fr/#attributs-de-fichiers-incoherents","title":"Attributs de fichiers incoh\u00e9rents","text":"<p>Certains syst\u00e8mes de fichiers partag\u00e9s, tels que NFS, peuvent signaler un horodatage incoh\u00e9rent (c'est-\u00e0-dire un horodatage diff\u00e9rent pour le m\u00eame fichier) m\u00eame s'il n'a pas \u00e9t\u00e9 modifi\u00e9. Pour \u00e9viter ce probl\u00e8me, utilisez la strat\u00e9gie de cache indulgente.</p>"},{"location":"basic_training/cache_and_resume.fr/#condition-de-course-dans-une-variable-globale","title":"Condition de course dans une variable globale","text":"<p>Nextflow est con\u00e7u pour simplifier la programmation parall\u00e8le sans se pr\u00e9occuper des conditions de course et de l'acc\u00e8s aux ressources partag\u00e9es. L'un des rares cas o\u00f9 une condition de course peut survenir est l'utilisation d'une variable globale avec deux (ou plus) op\u00e9rateurs. Par exemple:</p> <pre><code>Channel\n.of(1, 2, 3)\n.map { it -&gt; X = it; X += 2 }\n.view { \"ch1 = $it\" }\nChannel\n.of(1, 2, 3)\n.map { it -&gt; X = it; X *= 2 }\n.view { \"ch2 = $it\" }\n</code></pre> <p>Le probl\u00e8me dans cet extrait est que la variable <code>X</code> dans la d\u00e9finition de la fermeture est d\u00e9finie dans la port\u00e9e globale. Par cons\u00e9quent, puisque les op\u00e9rateurs sont ex\u00e9cut\u00e9s en parall\u00e8le, la valeur <code>X</code> peut \u00eatre \u00e9cras\u00e9e par l'autre invocation de <code>map</code>.</p> <p>L'impl\u00e9mentation correcte n\u00e9cessite l'utilisation du mot-cl\u00e9 <code>def</code> pour d\u00e9clarer la variable locale.</p> <pre><code>Channel\n.of(1, 2, 3)\n.map { it -&gt; def X = it; X += 2 }\n.println { \"ch1 = $it\" }\nChannel\n.of(1, 2, 3)\n.map { it -&gt; def X = it; X *= 2 }\n.println { \"ch2 = $it\" }\n</code></pre>"},{"location":"basic_training/cache_and_resume.fr/#canaux-dentree-non-deterministes","title":"Canaux d'entr\u00e9e non d\u00e9terministes","text":"<p>Si l'ordre des canaux de flux de donn\u00e9es est garanti - les donn\u00e9es sont lues dans l'ordre dans lequel elles sont \u00e9crites dans le canal - il faut savoir qu'il n'y a aucune garantie que les \u00e9l\u00e9ments conservent leur ordre dans le canal de sortie du processus. En effet, un processus peut engendrer plusieurs t\u00e2ches, qui peuvent s'ex\u00e9cuter en parall\u00e8le. Par exemple, l'op\u00e9ration sur le deuxi\u00e8me \u00e9l\u00e9ment peut se terminer plus t\u00f4t que l'op\u00e9ration sur le premier \u00e9l\u00e9ment, ce qui modifie l'ordre du canal de sortie.</p> <p>En pratique, consid\u00e9rons l'extrait suivant :</p> <pre><code>process FOO {\ninput:\nval x\noutput:\ntuple val(task.index), val(x)\nscript:\n\"\"\"\n    sleep \\$((RANDOM % 3))\n    \"\"\"\n}\nworkflow {\nchannel.of('A', 'B', 'C', 'D') | FOO | view\n}\n</code></pre> <p>Tout comme nous l'avons vu au d\u00e9but de ce tutoriel avec HELLO WORLD ou WORLD HELLO, la sortie de l'extrait ci-dessus peut \u00eatre :</p> <pre><code>[3, C]\n[4, D]\n[2, B]\n[1, A]\n</code></pre> <p>... et cet ordre sera probablement diff\u00e9rent \u00e0 chaque fois que le flux de travail sera ex\u00e9cut\u00e9.</p> <p>Imaginons maintenant que nous ayons deux processus de ce type, dont les canaux de sortie servent de canaux d'entr\u00e9e \u00e0 un troisi\u00e8me processus. Les deux canaux seront ind\u00e9pendamment al\u00e9atoires, de sorte que le troisi\u00e8me processus ne doit pas s'attendre \u00e0 ce qu'ils conservent une s\u00e9quence appari\u00e9e. S'il suppose que le premier \u00e9l\u00e9ment du canal de sortie du premier processus est li\u00e9 au premier \u00e9l\u00e9ment du canal de sortie du deuxi\u00e8me processus, il y aura inad\u00e9quation.</p> <p>Une solution courante consiste \u00e0 utiliser ce que l'on appelle commun\u00e9ment une meta map. Un objet groovy contenant des informations sur les \u00e9chantillons est transmis avec les r\u00e9sultats du fichier dans un canal de sortie sous la forme d'un tuple. Cet objet peut ensuite \u00eatre utilis\u00e9 pour associer des \u00e9chantillons provenant de canaux distincts en vue d'une utilisation en aval. Par exemple, au lieu de mettre juste <code>/some/path/myoutput.bam</code> dans un canal, vous pouvez utiliser <code>['SRR123', '/some/path/myoutput.bam']</code> pour vous assurer que les processus ne sont pas en conflit. Regardez l'exemple ci-dessous :</p> <pre><code>// For example purposes only.\n// These would normally be outputs from upstream processes.\nChannel\n.of(\n[[id: 'sample_1'], '/path/to/sample_1.bam'],\n[[id: 'sample_2'], '/path/to/sample_2.bam']\n)\n.set { bam }\n// NB: sample_2 is now the first element, instead of sample_1\nChannel\n.of(\n[[id: 'sample_2'], '/path/to/sample_2.bai'],\n[[id: 'sample_1'], '/path/to/sample_1.bai']\n)\n.set { bai }\n// Instead of feeding the downstream process with these two channels separately, we can\n// join them and provide a single channel where the sample meta map is implicitly matched:\nbam\n.join(bai)\n| PROCESS_C\n</code></pre> <p>Si les m\u00e9ta-cartes ne sont pas possibles, une alternative est d'utiliser la directive de processus <code>fair</code>. Lorsque cette directive est sp\u00e9cifi\u00e9e, Nextflow garantira que l'ordre des sorties correspondra \u00e0 l'ordre des entr\u00e9es. Il est important de mentionner que l'ordre dans lequel les t\u00e2ches seront termin\u00e9es ne suivra pas n\u00e9cessairement l'ordre dans le canal d'entr\u00e9e, mais Nextflow garantit qu'\u00e0 la fin de celui-ci, le canal de sortie contiendra les \u00e9l\u00e9ments dans l'ordre respectif.</p> <p>Warning</p> <p>En fonction de votre situation, l'utilisation de la directive <code>fair</code> peut entra\u00eener une diminution des performances.</p>"},{"location":"basic_training/cache_and_resume/","title":"Execution cache and resume","text":"<p>The Nextflow caching mechanism works by assigning a unique ID to each task which is used to create a separate execution directory where the tasks are executed and the results stored.</p> <p>The task unique ID is generated as a 128-bit hash value composing the task input values, files and command string.</p> <p>The workflow work directory is organized as shown below:</p> <pre><code>work/\n\u251c\u2500\u2500 12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1adacb582d2198cd32db0e6f808bce\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.fa -&gt; /data/../genome.fa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 hash.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 header.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 indexing.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 quasi_index.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 refInfo.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 rsd.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sa.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 txpInfo.bin\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 versionInfo.json\n\u251c\u2500\u2500 19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 663679d1d87bfeafacf30c1deaf81b\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aux_info\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ambig_info.tsv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fld.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 observed_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 observed_bias_3p.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 cmd_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 libParams\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 flenDist.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 lib_format_counts.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 salmon_quant.log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 quant.sf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_1.fq -&gt; /data/../ggal_gut_1.fq\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_2.fq -&gt; /data/../ggal_gut_2.fq\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index -&gt; /data/../asciidocs/day2/work/12/1adacb582d2198cd32db0e6f808bce/index\n</code></pre> <p>Info</p> <p>You can create these plots using the <code>tree</code> function if you have it installed. On Debian-based operating systems, simply <code>sudo apt install -y tree</code> or for macOS use Homebrew: <code>brew install tree</code></p>"},{"location":"basic_training/cache_and_resume/#how-resume-works","title":"How resume works","text":"<p>The <code>-resume</code> command-line option allows the continuation of a workflow execution from the last step that was completed successfully:</p> <pre><code>nextflow run &lt;script&gt; -resume\n</code></pre> <p>In practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files.</p> <p>If this condition is satisfied the task execution is skipped and previously computed results are used as the process results.</p> <p>The first task for which a new output is computed invalidates all downstream executions in the remaining DAG.</p>"},{"location":"basic_training/cache_and_resume/#work-directory","title":"Work directory","text":"<p>The task work directories are created in the folder <code>work</code> in the launching path by default. This is supposed to be a scratch storage area that can be cleaned up once the computation is completed.</p> <p>Note</p> <p>Workflow final output(s) are supposed to be stored in a different location specified using one or more publishDir directive.</p> <p>Warning</p> <p>Make sure to delete your work directory occasionally, else your machine/environment may be filled with unused files.</p> <p>A different location for the execution work directory can be specified using the command line option <code>-w</code>. For example:</p> <pre><code>nextflow run &lt;script&gt; -w /some/scratch/dir\n</code></pre> <p>Warning</p> <p>If you delete or move the workflow work directory, it will prevent the use of the resume feature in subsequent runs.</p> <p>The hash code for input files is computed using:</p> <ul> <li>The complete file path</li> <li>The file size</li> <li>The last modified timestamp</li> </ul> <p>Therefore, just touching a file will invalidate the related task execution.</p>"},{"location":"basic_training/cache_and_resume/#how-to-organize-in-silico-experiments","title":"How to organize in-silico experiments","text":"<p>It\u2019s good practice to organize each experiment in its own folder. The main experiment input parameters should be specified using a Nextflow config file. This makes it simple to track and replicate an experiment over time.</p> <p>Note</p> <p>In the same experiment, the same workflow can be executed multiple times, however, launching two (or more) Nextflow instances in the same directory concurrently should be avoided.</p> <p>The <code>nextflow log</code> command lists the executions run in the current folder:</p> <pre><code>$ nextflow log\n\nTIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND\n2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello\n2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker\n2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf\n2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker\n</code></pre> <p>You can use either the session ID or the run name to recover a specific execution. For example:</p> <pre><code>nextflow run rnaseq-nf -resume mighty_boyd\n</code></pre>"},{"location":"basic_training/cache_and_resume/#execution-provenance","title":"Execution provenance","text":"<p>The <code>log</code> command, when provided with a run name or session ID, can return many useful bits of information about a workflow execution that can be used to create a provenance report.</p> <p>By default, it will list the work directories used to compute each task. For example:</p> <pre><code>$ nextflow log tiny_fermat\n\n/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99\n/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d\n/data/.../work/3b/3485d00b0115f89e4c202eacf82eba\n</code></pre> <p>The <code>-f</code> (fields) option can be used to specify which metadata should be printed by the <code>log</code> command. For example:</p> <pre><code>$ nextflow log tiny_fermat -f 'process,exit,hash,duration'\nindex    0   7b/3753ff  2.0s\nfastqc   0   c1/56a36d  9.3s\nfastqc   0   f7/659c65  9.1s\nquant    0   82/ba67e3  2.7s\nquant    0   e5/2816b9  3.2s\nmultiqc  0   3b/3485d0  6.3s\n</code></pre> <p>The complete list of available fields can be retrieved with the command:</p> <pre><code>nextflow log -l\n</code></pre> <p>The <code>-F</code> option allows the specification of filtering criteria to print only a subset of tasks. For example:</p> <pre><code>$ nextflow log tiny_fermat -F 'process =~ /fastqc/'\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n</code></pre> <p>This can be useful to locate specific task work directories.</p> <p>Finally, the <code>-t</code> option enables the creation of a basic custom provenance report, showing a template file in any format of your choice. For example:</p> <pre><code>&lt;div&gt;\n&lt;h2&gt;${name}&lt;/h2&gt;\n&lt;div&gt;\n        Script:\n        &lt;pre&gt;${script}&lt;/pre&gt;\n&lt;/div&gt;\n&lt;ul&gt;\n&lt;li&gt;Exit: ${exit}&lt;/li&gt;\n&lt;li&gt;Status: ${status}&lt;/li&gt;\n&lt;li&gt;Work dir: ${workdir}&lt;/li&gt;\n&lt;li&gt;Container: ${container}&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> <p>Exercise</p> <p>Save the above snippet in a file named <code>template.html</code>. Then run this command (using the correct id for your run, e.g. <code>tiny_fermat</code>):</p> <pre><code>nextflow log tiny_fermat -t template.html &gt; prov.html\n</code></pre> <p>Finally, open the <code>prov.html</code> file with a browser.</p>"},{"location":"basic_training/cache_and_resume/#resume-troubleshooting","title":"Resume troubleshooting","text":"<p>Being able to resume workflows is a key feature of Nextflow, but it doesn't always work as you expect. In this section we go through a few common reasons why Nextflow may be ignoring your cached results.</p> <p>Tip</p> <p>To learn more details about the resume mechanism and how to troubleshoot please refer to the following three blog posts:</p> <ol> <li>Demystifying Nextflow resume</li> <li>Troubleshooting Nextflow resume</li> <li>Analyzing caching behavior of pipelines</li> </ol>"},{"location":"basic_training/cache_and_resume/#input-file-changed","title":"Input file changed","text":"<p>Make sure that there\u2019s no change in your input file(s). Don\u2019t forget the task unique hash is computed by taking into account the complete file path, the last modified timestamp and the file size. If any of this information has changed, the workflow will be re-executed even if the input content is the same.</p>"},{"location":"basic_training/cache_and_resume/#a-process-modifies-an-input","title":"A process modifies an input","text":"<p>A process should never alter input files, otherwise the <code>resume</code> for future executions will be invalidated for the same reason explained in the previous point.</p>"},{"location":"basic_training/cache_and_resume/#inconsistent-file-attributes","title":"Inconsistent file attributes","text":"<p>Some shared file systems, such as NFS, may report an inconsistent file timestamp (i.e. a different timestamp for the same file) even if it has not been modified. To prevent this problem use the lenient cache strategy.</p>"},{"location":"basic_training/cache_and_resume/#race-condition-in-global-variable","title":"Race condition in global variable","text":"<p>Nextflow is designed to simplify parallel programming without taking care about race conditions and the access to shared resources. One of the few cases in which a race condition can arise is when using a global variable with two (or more) operators. For example:</p> <pre><code>Channel\n.of(1, 2, 3)\n.map { it -&gt; X = it; X += 2 }\n.view { \"ch1 = $it\" }\nChannel\n.of(1, 2, 3)\n.map { it -&gt; X = it; X *= 2 }\n.view { \"ch2 = $it\" }\n</code></pre> <p>The problem in this snippet is that the <code>X</code> variable in the closure definition is defined in the global scope. Therefore, since operators are executed in parallel, the <code>X</code> value can be overwritten by the other <code>map</code> invocation.</p> <p>The correct implementation requires the use of the <code>def</code> keyword to declare the variable local.</p> <pre><code>Channel\n.of(1, 2, 3)\n.map { it -&gt; def X = it; X += 2 }\n.println { \"ch1 = $it\" }\nChannel\n.of(1, 2, 3)\n.map { it -&gt; def X = it; X *= 2 }\n.println { \"ch2 = $it\" }\n</code></pre>"},{"location":"basic_training/cache_and_resume/#non-deterministic-input-channels","title":"Non-deterministic input channels","text":"<p>While dataflow channel ordering is guaranteed \u2013 data is read in the same order in which it\u2019s written in the channel \u2013 be aware that there is no guarantee that the elements will maintain their order in the process output channel. This is because a process may spawn multiple tasks, which can run in parallel. For example, the operation on the second element may end sooner than the operation on the first element, changing the output channel order.</p> <p>In practical terms, consider the following snippet:</p> <pre><code>process FOO {\ninput:\nval x\noutput:\ntuple val(task.index), val(x)\nscript:\n\"\"\"\n    sleep \\$((RANDOM % 3))\n    \"\"\"\n}\nworkflow {\nchannel.of('A', 'B', 'C', 'D') | FOO | view\n}\n</code></pre> <p>Just like we saw at the beginning of this tutorial with HELLO WORLD or WORLD HELLO, the output of the snippet above can be:</p> <pre><code>[3, C]\n[4, D]\n[2, B]\n[1, A]\n</code></pre> <p>..and that order will likely be different every time the workflow is run.</p> <p>Imagine now that we have two processes like this, whose output channels are acting as input channels to a third process. Both channels will be independently random, so the third process must not expect them to retain a paired sequence. If it does assume that the first element in the first process output channel is related to the first element in the second process output channel, there will be a mismatch.</p> <p>A common solution for this is to use what is commonly referred to as a meta map. A groovy object with sample information is passed out together with the file results within an output channel as a tuple. This can then be used to pair samples from separate channels together for downstream use. For example, instead of putting just <code>/some/path/myoutput.bam</code> into a channel, you could use <code>['SRR123', '/some/path/myoutput.bam']</code> to make sure the processes are not incurring into a mismatch. Check the example below:</p> <pre><code>// For example purposes only.\n// These would normally be outputs from upstream processes.\nChannel\n.of(\n[[id: 'sample_1'], '/path/to/sample_1.bam'],\n[[id: 'sample_2'], '/path/to/sample_2.bam']\n)\n.set { bam }\n// NB: sample_2 is now the first element, instead of sample_1\nChannel\n.of(\n[[id: 'sample_2'], '/path/to/sample_2.bai'],\n[[id: 'sample_1'], '/path/to/sample_1.bai']\n)\n.set { bai }\n// Instead of feeding the downstream process with these two channels separately, we can\n// join them and provide a single channel where the sample meta map is implicitly matched:\nbam\n.join(bai)\n| PROCESS_C\n</code></pre> <p>If meta maps are not possible, an alternative is to use the <code>fair</code> process directive. When this directive is specified, Nextflow will guarantee that the order of outputs will match the order of inputs (not the order in which the tasks run, only the order of the output channel).</p> <p>Warning</p> <p>Depending on your situation, using the <code>fair</code> directive will lead to a decrease in performance.</p>"},{"location":"basic_training/cache_and_resume.pt/","title":"Execu\u00e7\u00e3o de cache e de reentr\u00e2ncia","text":"<p>O mecanismo de caching do Nextflow funciona atribuindo uma ID \u00fanica para cada tarefa que \u00e9 usada para criar um diret\u00f3rio de execu\u00e7\u00e3o separado onde as tarefas s\u00e3o executadas e os resultados guardados.</p> <p>A ID \u00fanica de tarefa \u00e9 gerada como uma hash de 128-bit compondo os valores de entrada da tarefa, arquivos e a string de comando.</p> <p>O diret\u00f3rio de trabalho do fluxo de trabalho \u00e9 organizado como mostrado abaixo:</p> <pre><code>work/\n\u251c\u2500\u2500 12\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 1adacb582d2198cd32db0e6f808bce\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 genome.fa -&gt; /data/../genome.fa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 hash.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 header.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 indexing.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 quasi_index.log\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 refInfo.json\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 rsd.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 sa.bin\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 txpInfo.bin\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 versionInfo.json\n\u251c\u2500\u2500 19\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 663679d1d87bfeafacf30c1deaf81b\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 aux_info\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ambig_info.tsv\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 expected_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fld.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 observed_bias.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 observed_bias_3p.gz\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 cmd_info.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 libParams\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 flenDist.txt\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 lib_format_counts.json\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 salmon_quant.log\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 quant.sf\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_1.fq -&gt; /data/../ggal_gut_1.fq\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ggal_gut_2.fq -&gt; /data/../ggal_gut_2.fq\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 index -&gt; /data/../asciidocs/day2/work/12/1adacb582d2198cd32db0e6f808bce/index\n</code></pre> <p>Info</p> <p>Voc\u00ea pode criar esse plot usando a fun\u00e7\u00e3o <code>tree</code> se voc\u00ea a tiver instalada. No unix, simplesmente use <code>sudo apt install -y tree</code> ou com Homebrew: <code>brew install tree</code></p>"},{"location":"basic_training/cache_and_resume.pt/#como-funciona-a-reentrancia","title":"Como funciona a reentr\u00e2ncia","text":"<p>A op\u00e7\u00e3o de linha de comando <code>-resume</code> permite a continua\u00e7\u00e3o da execu\u00e7\u00e3o do fluxo de trabalho pelo \u00faltimo passo que foi completado com sucesso:</p> <pre><code>nextflow run &lt;script&gt; -resume\n</code></pre> <p>Em termos pr\u00e1ticos, o fluxo de trabalho \u00e9 executado do in\u00edcio. Entretanto, antes do lan\u00e7amento da execu\u00e7\u00e3o de um processo, o Nextflow usa a ID \u00fanica da tarefa para checar se o diret\u00f3rio de trabalho existe e se ele cont\u00e9m um estado de sa\u00edda v\u00e1lido do comando com os esperados arquivos de sa\u00edda.</p> <p>Se esta condi\u00e7\u00e3o for satisfeita a tarefa \u00e9 ignorada e os resultados computados previamente s\u00e3o usados como resultados do processo.</p> <p>A primeira tarefa que tem uma nova sa\u00edda computada invalida todas execu\u00e7\u00f5es posteriores no que resta do Grafo Ac\u00edclico Direcionado (DAG, do ingl\u00eas Directed Acyclic Graph).</p>"},{"location":"basic_training/cache_and_resume.pt/#diretorio-de-trabalho","title":"Diret\u00f3rio de trabalho","text":"<p>O diret\u00f3rio de trabalho das tarefas \u00e9 criado por padr\u00e3o na pasta <code>work</code> no mesmo diret\u00f3rio onde o fluxo de trabalho foi executado. Essa localiza\u00e7\u00e3o \u00e9 supostamente uma \u00e1rea de armazenamento provis\u00f3ria que pode ser limpada quando a execu\u00e7\u00e3o do fluxo de trabalho for finalizado.</p> <p>Note</p> <p>As sa\u00eddas finais do fluxo de trabalho geralmente s\u00e3o guardadas em uma localiza\u00e7\u00e3o diferente especificada usando uma ou mais diretivas publishDir.</p> <p>Warning</p> <p>Certifique-se de deletar o diret\u00f3rio de trabalho ocasionalmente, se n\u00e3o sua m\u00e1quina ou ambiente estar\u00e1 cheia de arquivos sem uso.</p> <p>Uma localiza\u00e7\u00e3o diferente para o diret\u00f3rio de trabalho pode ser especificada usando a op\u00e7\u00e3o <code>-w</code>. Por exemplo:</p> <pre><code>nextflow run &lt;script&gt; -w /algum/diretorio/de/scratch\n</code></pre> <p>Warning</p> <p>Se voc\u00ea deletar ou mover o diret\u00f3rio de trabalho do fluxo de trabalho, isso ir\u00e1 impedir que voc\u00ea use o recurso de reentr\u00e2ncia nas execu\u00e7\u00f5es posteriores.</p> <p>O c\u00f3digo hash para os arquivos de entrada s\u00e3o computados usando:</p> <ul> <li>O caminho completo do arquivo</li> <li>O tamanho do arquivo</li> <li>A \u00faltima marca\u00e7\u00e3o de tempo de modifica\u00e7\u00e3o</li> </ul> <p>Portanto, o simples uso do touch em um arquivo ir\u00e1 invalidar a execu\u00e7\u00e3o da tarefa relacionada.</p>"},{"location":"basic_training/cache_and_resume.pt/#como-organizar-experimentos-in-silico","title":"Como organizar experimentos in-silico","text":"<p>\u00c9 uma boa pr\u00e1tica organizar cada experimento em sua pr\u00f3pria pasta. Os par\u00e2metros de entrada do experimento principal devem ser especificados usando o arquivo de configura\u00e7\u00e3o do Nextflow. Isso torna simples acompanhar e replicar o experimento ao longo do tempo.</p> <p>Note</p> <p>No mesmo experimento, o mesmo fluxo de trabalho pode ser executado diversas vezes, entretanto, iniciar duas (ou mais) inst\u00e2ncias do Nextflow no mesmo diret\u00f3rio ao mesmo tempo deve ser evitado.</p> <p>O comando <code>nextflow log</code> lista todas as execu\u00e7\u00f5es na pasta atual:</p> <pre><code>$ nextflow log\n\nTIMESTAMP            DURATION  RUN NAME          STATUS  REVISION ID  SESSION ID                            COMMAND\n2019-05-06 12:07:32  1.2s      focused_carson    ERR     a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run hello\n2019-05-06 12:08:33  21.1s     mighty_boyd       OK      a9012339ce   7363b3f0-09ac-495b-a947-28cf430d0b85  nextflow run rnaseq-nf -with-docker\n2019-05-06 12:31:15  1.2s      insane_celsius    ERR     b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf\n2019-05-06 12:31:24  17s       stupefied_euclid  OK      b9aefc67b4   4dc656d2-c410-44c8-bc32-7dd0ea87bebf  nextflow run rnaseq-nf -resume -with-docker\n</code></pre> <p>Voc\u00ea pode usar tanto o ID da sess\u00e3o ou o nome da execu\u00e7\u00e3o para recuperar uma execu\u00e7\u00e3o espec\u00edfica. Por exemplo:</p> <pre><code>nextflow run rnaseq-nf -resume mighty_boyd\n</code></pre>"},{"location":"basic_training/cache_and_resume.pt/#proveniencia-da-execucao","title":"Proveni\u00eancia da execu\u00e7\u00e3o","text":"<p>O comando <code>log</code>, quando provido do nome da execu\u00e7\u00e3o ou ID da sess\u00e3o, pode retornar algumas informa\u00e7\u00f5es importantes sobre um fluxo de trabalho em execu\u00e7\u00e3o que podem ser usadas para criar um relat\u00f3rio de proveni\u00eancia.</p> <p>Por padr\u00e3o, o comando ir\u00e1 listar todos diret\u00f3rios de trabalho usados em cada tarefa. Por exemplo:</p> <pre><code>$ nextflow log tiny_fermat\n\n/data/.../work/7b/3753ff13b1fa5348d2d9b6f512153a\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n/data/.../work/82/ba67e3175bd9e6479d4310e5a92f99\n/data/.../work/e5/2816b9d4e7b402bfdd6597c2c2403d\n/data/.../work/3b/3485d00b0115f89e4c202eacf82eba\n</code></pre> <p>A op\u00e7\u00e3o <code>-f</code> (do ingl\u00eas, fields) pode ser usada para especificar qual metadado deve ser impresso pelo comando <code>log</code>. Por exemplo:</p> <pre><code>$ nextflow log tiny_fermat -f 'process,exit,hash,duration'\nindex    0   7b/3753ff  2.0s\nfastqc   0   c1/56a36d  9.3s\nfastqc   0   f7/659c65  9.1s\nquant    0   82/ba67e3  2.7s\nquant    0   e5/2816b9  3.2s\nmultiqc  0   3b/3485d0  6.3s\n</code></pre> <p>A lista completa dos campos dispon\u00edveis pode ser recuperada com o comando:</p> <pre><code>nextflow log -l\n</code></pre> <p>A op\u00e7\u00e3o <code>-F</code> permite a especifica\u00e7\u00e3o de um crit\u00e9rio de filtro para imprimir apenas um subconjunto de tarefas. Por exemplo:</p> <pre><code>$ nextflow log tiny_fermat -F 'process =~ /fastqc/'\n/data/.../work/c1/56a36d8f498c99ac6cba31e85b3e0c\n/data/.../work/f7/659c65ef60582d9713252bcfbcc310\n</code></pre> <p>Isso pode ser \u00fatil para localizar um diret\u00f3rio de trabalho de uma tarefa espec\u00edfica.</p> <p>Finalmente, a op\u00e7\u00e3o <code>-t</code> permite a cria\u00e7\u00e3o de um relat\u00f3rio b\u00e1sico e customiz\u00e1vel de proveni\u00eancia, mostrando um modelo de arquivo em qualquer formato de sua escolha. Por exemplo:</p> <pre><code>&lt;div&gt;\n&lt;h2&gt;${name}&lt;/h2&gt;\n&lt;div&gt;\n        Script:\n        &lt;pre&gt;${script}&lt;/pre&gt;\n&lt;/div&gt;\n&lt;ul&gt;\n&lt;li&gt;Exit: ${exit}&lt;/li&gt;\n&lt;li&gt;Status: ${status}&lt;/li&gt;\n&lt;li&gt;Work dir: ${workdir}&lt;/li&gt;\n&lt;li&gt;Cont\u00eainer: ${container}&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> <p>Exercise</p> <p>Salve o trecho acima em um arquivo chamado <code>template.html</code>. Ent\u00e3o execute o comando (usando o ID correto para sua execu\u00e7\u00e3o, ex. <code>tiny_fermat</code>):</p> <pre><code>nextflow log tiny_fermat -t template.html &gt; prov.html\n</code></pre> <p>Finalmente, abra o arquivo <code>prov.html</code> com um navegador.</p>"},{"location":"basic_training/cache_and_resume.pt/#resolucao-de-problemas-de-reentrancia","title":"Resolu\u00e7\u00e3o de problemas de reentr\u00e2ncia","text":"<p>Ser capaz de retomar fluxos de trabalho \u00e9 um recurso importante do Nextflow, mas nem sempre funciona como voc\u00ea espera. Nesta se\u00e7\u00e3o, analisamos alguns motivos comuns pelos quais o Nextflow pode estar ignorando seus resultados em cache.</p> <p>Tip</p> <p>Para saber mais detalhes sobre o mecanismo de reentr\u00e2ncia e como solucionar problemas, consulte as tr\u00eas postagens de blog a seguir:</p> <ol> <li>Demystifying Nextflow resume</li> <li>Troubleshooting Nextflow resume</li> <li>Analyzing caching behavior of pipelines</li> </ol>"},{"location":"basic_training/cache_and_resume.pt/#arquivos-de-entrada-mudados","title":"Arquivos de entrada mudados","text":"<p>Tenha certeza que n\u00e3o h\u00e1 nenhuma mudan\u00e7a no(s) arquivo(s) de entrada. N\u00e3o esque\u00e7a que cada tarefa tem seu hash \u00fanico que \u00e9 computado levando em conta o caminho completo do arquivo, a \u00faltima marca\u00e7\u00e3o de tempo de modifica\u00e7\u00e3o e o tamanho do arquivo. Se alguma dessas informa\u00e7\u00f5es foi alterada, o fluxo de trabalho deve ser re-executado mesmo que o conte\u00fado do arquivo seja o mesmo.</p>"},{"location":"basic_training/cache_and_resume.pt/#um-processo-modifica-uma-entrada","title":"Um processo modifica uma entrada","text":"<p>Um processo nunca deve alterar os arquivos de entrada, se n\u00e3o a fun\u00e7\u00e3o <code>resume</code> em execu\u00e7\u00f5es futuras ser\u00e1 invalidada pela mesma raz\u00e3o explicada no ponto anterior.</p>"},{"location":"basic_training/cache_and_resume.pt/#atributos-de-arquivos-inconsistentes","title":"Atributos de arquivos inconsistentes","text":"<p>Alguns sistemas de arquivos compartilhados, como o NFS, podem reportar uma marca\u00e7\u00e3o de tempo inconsistente para os arquivos (por exemplo, uma diferente marca\u00e7\u00e3o de tempo para um mesmo arquivo) at\u00e9 mesmo quando este n\u00e3o foi modificado. Para prevenir esse problema use a estrat\u00e9gia de cache leniente.</p>"},{"location":"basic_training/cache_and_resume.pt/#condicao-de-corrida-em-uma-variavel-global","title":"Condi\u00e7\u00e3o de corrida em uma vari\u00e1vel global","text":"<p>O Nextflow \u00e9 desenvolvido para simplificar programa\u00e7\u00e3o paralela, de modo que voc\u00ea n\u00e3o precise se preocupar com condi\u00e7\u00f5es de corrida e acesso a recursos compartilhados. Um dos poucos casos que uma condi\u00e7\u00e3o de corrida pode surgir \u00e9 quando uma vari\u00e1vel global \u00e9 utilizada com dois (ou mais) operadores. Por exemplo:</p> <pre><code>Channel\n.of(1, 2, 3)\n.map { it -&gt; X = it; X += 2 }\n.view { \"canal1 = $it\" }\nChannel\n.of(1, 2, 3)\n.map { it -&gt; X = it; X *= 2 }\n.view { \"canal2 = $it\" }\n</code></pre> <p>O problema desse trecho \u00e9 que a vari\u00e1vel <code>X</code> na clausura \u00e9 definida no escopo global. Portanto, como operadores s\u00e3o executados em paralelo, o valor de <code>X</code> pode ser sobrescrito pela outra invoca\u00e7\u00e3o do <code>map</code>.</p> <p>A implementa\u00e7\u00e3o correta requer o uso da palavra chave <code>def</code> para declarar a vari\u00e1vel local.</p> <pre><code>Channel\n.of(1, 2, 3)\n.map { it -&gt; def X = it; X += 2 }\n.println { \"canal1 = $it\" }\nChannel\n.of(1, 2, 3)\n.map { it -&gt; def X = it; X *= 2 }\n.println { \"canal2 = $it\" }\n</code></pre>"},{"location":"basic_training/cache_and_resume.pt/#canais-de-entrada-nao-deterministicos","title":"Canais de entrada n\u00e3o determin\u00edsticos.","text":"<p>Embora a ordem de elementos em canais dataflow seja garantida \u2013 os dados s\u00e3o lidos na mesma ordem em que s\u00e3o escritos no canal \u2013 saiba que n\u00e3o h\u00e1 garantia de que os elementos manter\u00e3o sua ordem no canal de sa\u00edda do processo. Isso ocorre porque um processo pode gerar v\u00e1rias tarefas, que podem ser executadas em paralelo. Por exemplo, a opera\u00e7\u00e3o no segundo elemento pode terminar antes da opera\u00e7\u00e3o no primeiro elemento, alterando a ordem dos elementos no canal de sa\u00edda.</p> <p>Em termos pr\u00e1ticos, considere o trecho a seguir:</p> <pre><code>process FOO {\ninput:\nval x\noutput:\ntuple val(task.index), val(x)\nscript:\n\"\"\"\n    sleep \\$((RANDOM % 3))\n    \"\"\"\n}\nworkflow {\nchannel.of('A', 'B', 'C', 'D') | FOO | view\n}\n</code></pre> <p>Assim como vimos no in\u00edcio deste tutorial com HELLO WORLD ou WORLD HELLO, a sa\u00edda do trecho acima pode ser:</p> <pre><code>[3, C]\n[4, D]\n[2, B]\n[1, A]\n</code></pre> <p>..e essa ordem provavelmente ser\u00e1 diferente toda vez que o fluxo de trabalho for executado.</p> <p>Imagine agora que temos dois processos como este, cujos canais de sa\u00edda est\u00e3o atuando como canais de entrada para um terceiro processo. Ambos os canais ser\u00e3o aleat\u00f3rios de forma independente, portanto, o terceiro processo n\u00e3o deve esperar que eles retenham uma sequ\u00eancia pareada. Se assumir que o primeiro elemento no primeiro canal de sa\u00edda do processo est\u00e1 relacionado ao primeiro elemento no segundo canal de sa\u00edda do processo, haver\u00e1 uma incompatibilidade.</p> <p>Uma solu\u00e7\u00e3o comum para isso \u00e9 usar o que \u00e9 comumente chamado de meta mapa (meta map). Um objeto groovy com informa\u00e7\u00f5es de amostra \u00e9 distribu\u00eddo junto com os resultados do arquivo em um canal de sa\u00edda como uma tupla. Isso pode ent\u00e3o ser usado para emparelhar amostras que est\u00e3o em canais separados para uso em processos posteriores. Por exemplo, em vez de colocar apenas <code>/algum/caminho/minhasaida.bam</code> em um canal, voc\u00ea pode usar <code>['SRR123', '/algum/caminho/minhasaida.bam']</code> para garantir que os processos n\u00e3o incorram em uma incompatibilidade. Verifique o exemplo abaixo:</p> <pre><code>// Apenas para fins de exemplos.\n// Estes abaixo seriam normalmente as sa\u00eddas de processos anteriores\nChannel\n.of(\n[[id: 'amostra_1'], '/caminho/para/amostra_1.bam'],\n[[id: 'amostra_2'], '/caminho/para/amostra_2.bam']\n)\n.set { bam }\n// Nota: amostra_2 \u00e9 agora o primeiro elemento, em vez de amostra_1\nChannel\n.of(\n[[id: 'amostra_2'], '/caminho/para/amostra_2.bai'],\n[[id: 'amostra_1'], '/caminho/para/amostra_1.bai']\n)\n.set { bai }\n// Em vez de alimentar o processo posterior com esses dois canais separadamente,\n// n\u00f3s podemos un\u00ed-los com o operador `join` e entregar um \u00fanico canal onde o\n// meta mapa de amostra \u00e9 correspondido implicitamente:\nbam\n.join(bai)\n| PROCESSO_C\n</code></pre> <p>Se os meta mapas n\u00e3o forem poss\u00edveis, uma alternativa \u00e9 usar a diretiva de processo <code>fair</code>. Quando especificada, o Nextflow garantir\u00e1 que a ordem dos elementos nos canais de sa\u00edda corresponder\u00e1 \u00e0 ordem dos respectivos elementos nos canais de entrada. \u00c9 importante deixar claro que a ordem em que as tarefas ser\u00e3o conclu\u00eddas n\u00e3o ser\u00e1 necessariamente a ordem dos elementos no canal entrada, mas Nextflow garante que, ao final do processamento, os elementos no canal de sa\u00edda estar\u00e3o na ordem correta.</p> <p>Warning</p> <p>Dependendo da sua situa\u00e7\u00e3o, usar a diretiva <code>fair</code> levar\u00e1 a uma queda de desempenho.</p>"},{"location":"basic_training/channels/","title":"Channels","text":"<p>Channels are a key data structure of Nextflow that allows the implementation of reactive-functional oriented computational workflows based on the Dataflow programming paradigm.</p> <p>They are used to logically connect tasks to each other or to implement functional style data transformations.</p> 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 task \u03b1file zfile yfile xtask \u03b2Channel"},{"location":"basic_training/channels/#channel-types","title":"Channel types","text":"<p>Nextflow distinguishes two different kinds of channels: queue channels and value channels.</p>"},{"location":"basic_training/channels/#queue-channel","title":"Queue channel","text":"<p>A queue channel is an asynchronous unidirectional FIFO queue that connects two processes or operators.</p> <ul> <li>asynchronous means that operations are non-blocking.</li> <li>unidirectional means that data flows from a producer to a consumer.</li> <li>FIFO means that the data is guaranteed to be delivered in the same order as it is produced. First In, First Out.</li> </ul> <p>A queue channel is implicitly created by process output definitions or using channel factories such as Channel.of or Channel.fromPath.</p> <p>Try the following snippets:</p> <p>Click the  icons in the code for explanations.</p> <pre><code>ch = Channel.of(1, 2, 3)\nprintln(ch) // (1)!\nch.view() // (2)!\n</code></pre> <ol> <li>Use the built-in print line function <code>println</code> to print the <code>ch</code> channel</li> <li>Apply the <code>view</code> channel operator to the <code>ch</code> channel prints each item emitted by the channels</li> </ol> <p>Exercise</p> <p>Try to execute this snippet. You can do that by creating a new <code>.nf</code> file or by editing an already existing <code>.nf</code> file.</p> <pre><code>ch = Channel.of(1, 2, 3)\nch.view()\n</code></pre>"},{"location":"basic_training/channels/#value-channels","title":"Value channels","text":"<p>A value channel (a.k.a. singleton channel) by definition is bound to a single value and it can be read unlimited times without consuming its contents. A <code>value</code> channel is created using the value channel factory or by operators returning a single value, such as first, last, collect, count, min, max, reduce, and sum.</p> <p>To better understand the difference between value and queue channels, save the snippet below as <code>example.nf</code>.</p> example.nf<pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\nprocess SUM {\ninput:\nval x\nval y\noutput:\nstdout\nscript:\n\"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\nworkflow {\nSUM(ch1, ch2).view()\n}\n</code></pre> <p>When you run the script, it prints only 2, as you can see below:</p> <pre><code>2\n</code></pre> <p>A process will only instantiate a task when there are elements to be consumed from all the channels provided as input to it. Because <code>ch1</code> and <code>ch2</code> are queue channels, and the single element of <code>ch2</code> has been consumed, no new process instances will be launched, even if there are other elements to be consumed in <code>ch1</code>.</p> <p>To use the single element in <code>ch2</code> multiple times, we can either use <code>Channel.value</code> as mentioned above, or use a channel operator that returns a single element such as <code>first()</code> below:</p> <pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\nprocess SUM {\ninput:\nval x\nval y\noutput:\nstdout\nscript:\n\"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\nworkflow {\nSUM(ch1, ch2.first()).view()\n}\n</code></pre> Output<pre><code>4\n3\n2\n</code></pre> <p>Besides, in many situations, Nextflow will implicitly convert variables to value channels when they are used in a process invocation. For example, when you invoke a process with a workflow parameter (<code>params.example</code>) which has a string value, it is automatically cast into a value channel.</p>"},{"location":"basic_training/channels/#channel-factories","title":"Channel factories","text":"<p>These are Nextflow commands for creating channels that have implicit expected inputs and functions.</p>"},{"location":"basic_training/channels/#value","title":"<code>value()</code>","text":"<p>The <code>value</code> channel factory is used to create a value channel. An optional not <code>null</code> argument can be specified to bind the channel to a specific value. For example:</p> <pre><code>ch1 = Channel.value() // (1)!\nch2 = Channel.value('Hello there') // (2)!\nch3 = Channel.value([1, 2, 3, 4, 5]) // (3)!\n</code></pre> <ol> <li>Creates an empty value channel</li> <li>Creates a value channel and binds a string to it</li> <li>Creates a value channel and binds a list object to it that will be emitted as a sole emission</li> </ol>"},{"location":"basic_training/channels/#of","title":"<code>of()</code>","text":"<p>The factory <code>Channel.of</code> allows the creation of a queue channel with the values specified as arguments.</p> <pre><code>ch = Channel.of(1, 3, 5, 7)\nch.view { \"value: $it\" }\n</code></pre> <p>The first line in this example creates a variable <code>ch</code> which holds a channel object. This channel emits the values specified as a parameter in the <code>of</code> channel factory. Thus the second line will print the following:</p> <pre><code>value: 1\nvalue: 3\nvalue: 5\nvalue: 7\n</code></pre> <p>The <code>Channel.of</code> channel factory works in a similar manner to <code>Channel.from</code> (which is now deprecated), fixing some inconsistent behaviors of the latter and providing better handling when specifying a range of values. For example, the following works with a range from 1 to 23:</p> <pre><code>Channel\n.of(1..23, 'X', 'Y')\n.view()\n</code></pre>"},{"location":"basic_training/channels/#fromlist","title":"<code>fromList()</code>","text":"<p>The <code>Channel.fromList</code> channel factory creates a channel emitting the elements provided by a list object specified as an argument:</p> <pre><code>list = ['hello', 'world']\nChannel\n.fromList(list)\n.view()\n</code></pre>"},{"location":"basic_training/channels/#frompath","title":"<code>fromPath()</code>","text":"<p>The <code>fromPath</code> channel factory creates a queue channel emitting one or more files matching the specified glob pattern.</p> <pre><code>Channel.fromPath('./data/meta/*.csv')\n</code></pre> <p>This example creates a channel and emits as many items as there are files with a <code>csv</code> extension in the <code>./data/meta</code> folder. Each element is a file object implementing the Path interface.</p> <p>Tip</p> <p>Two asterisks, i.e. <code>**</code>, works like <code>*</code> but cross directory boundaries. This syntax is generally used for matching complete paths. Curly brackets specify a collection of sub-patterns.</p> Name Description glob When <code>true</code> interprets characters <code>*</code>, <code>?</code>, <code>[]</code> and <code>{}</code> as glob wildcards, otherwise handles them as normal characters (default: <code>true</code>) type Type of path returned, either <code>file</code>, <code>dir</code> or <code>any</code> (default: <code>file</code>) hidden When <code>true</code> includes hidden files in the resulting paths (default: <code>false</code>) maxDepth Maximum number of directory levels to visit (default: <code>no limit</code>) followLinks When <code>true</code> symbolic links are followed during directory tree traversal, otherwise they are managed as files (default: <code>true</code>) relative When <code>true</code> return paths are relative to the top-most common directory (default: <code>false</code>) checkIfExists When <code>true</code> throws an exception when the specified path does not exist in the file system (default: <code>false</code>) <p>Learn more about the glob patterns syntax at this link.</p> <p>Exercise</p> <p>Use the <code>Channel.fromPath</code> channel factory to create a channel emitting all files with the suffix <code>.fq</code> in the <code>data/ggal/</code> directory and any subdirectory, in addition to hidden files. Then print the file names.</p> Solution <pre><code>Channel\n.fromPath('./data/ggal/**.fq', hidden: true)\n.view()\n</code></pre>"},{"location":"basic_training/channels/#fromfilepairs","title":"<code>fromFilePairs()</code>","text":"<p>The <code>fromFilePairs</code> channel factory creates a channel emitting the file pairs matching a glob pattern provided by the user. The matching files are emitted as tuples, in which the first element is the grouping key of the matching pair and the second element is the list of files (sorted in lexicographical order).</p> <pre><code>Channel\n.fromFilePairs('./data/ggal/*_{1,2}.fq')\n.view()\n</code></pre> <p>It will produce an output similar to the following:</p> <pre><code>[liver, [/workspace/gitpod/nf-training/data/ggal/liver_1.fq, /workspace/gitpod/nf-training/data/ggal/liver_2.fq]]\n[gut, [/workspace/gitpod/nf-training/data/ggal/gut_1.fq, /workspace/gitpod/nf-training/data/ggal/gut_2.fq]]\n[lung, [/workspace/gitpod/nf-training/data/ggal/lung_1.fq, /workspace/gitpod/nf-training/data/ggal/lung_2.fq]]\n</code></pre> <p>Warning</p> <p>The glob pattern must contain at least a star wildcard character (<code>*</code>).</p> Name Description type Type of paths returned, either <code>file</code>, <code>dir</code> or <code>any</code> (default: <code>file</code>) hidden When <code>true</code> includes hidden files in the resulting paths (default: <code>false</code>) maxDepth Maximum number of directory levels to visit (default: <code>no limit</code>) followLinks When <code>true</code> symbolic links are followed during directory tree traversal, otherwise they are managed as files (default: <code>true</code>) size Defines the number of files each emitted item is expected to hold (default: <code>2</code>). Set to <code>-1</code> for any flat When <code>true</code> the matching files are produced as sole elements in the emitted tuples (default: <code>false</code>) checkIfExists When <code>true</code>, it throws an exception of the specified path that does not exist in the file system (default: <code>false</code>) <p>Exercise</p> <p>Use the <code>fromFilePairs</code> channel factory to create a channel emitting all pairs of fastq read in the <code>data/ggal/</code> directory and print them. Then use the <code>flat: true</code> option and compare the output with the previous execution.</p> Solution <p>Use the following, with or without <code>flat: true</code>:</p> <pre><code>Channel\n.fromFilePairs('./data/ggal/*_{1,2}.fq', flat: true)\n.view()\n</code></pre> <p>Then check the square brackets around the file names, to see the difference with <code>flat</code>.</p>"},{"location":"basic_training/channels/#fromsra","title":"<code>fromSRA()</code>","text":"<p>The <code>Channel.fromSRA</code> channel factory makes it possible to query the NCBI SRA archive and returns a channel emitting the FASTQ files matching the specified selection criteria.</p> <p>The query can be project ID(s) or accession number(s) supported by the NCBI ESearch API.</p> <p>Info</p> <p>This function now requires an API key you can only get by logging into your NCBI account.</p> Instructions for NCBI login and key acquisition <ol> <li>Go to: https://www.ncbi.nlm.nih.gov/</li> <li>Click the top right \"Log in\" button to sign into NCBI. Follow their instructions.</li> <li>Once into your account, click the button at the top right, usually your ID.</li> <li>Go to Account settings</li> <li>Scroll down to the API Key Management section.</li> <li>Click on \"Create an API Key\".</li> <li>The page will refresh and the key will be displayed where the button was. Copy your key.</li> </ol> <p>For example, the following snippet will print the contents of an NCBI project ID:</p> <pre><code>params.ncbi_api_key = '&lt;Your API key here&gt;'\nChannel\n.fromSRA(['SRP073307'], apiKey: params.ncbi_api_key)\n.view()\n</code></pre> <p> Replace <code>&lt;Your API key here&gt;</code> with your API key.</p> <p>This should print:</p> <pre><code>[SRR3383346, [/vol1/fastq/SRR338/006/SRR3383346/SRR3383346_1.fastq.gz, /vol1/fastq/SRR338/006/SRR3383346/SRR3383346_2.fastq.gz]]\n[SRR3383347, [/vol1/fastq/SRR338/007/SRR3383347/SRR3383347_1.fastq.gz, /vol1/fastq/SRR338/007/SRR3383347/SRR3383347_2.fastq.gz]]\n[SRR3383344, [/vol1/fastq/SRR338/004/SRR3383344/SRR3383344_1.fastq.gz, /vol1/fastq/SRR338/004/SRR3383344/SRR3383344_2.fastq.gz]]\n[SRR3383345, [/vol1/fastq/SRR338/005/SRR3383345/SRR3383345_1.fastq.gz, /vol1/fastq/SRR338/005/SRR3383345/SRR3383345_2.fastq.gz]]\n// (remaining omitted)\n</code></pre> <p>Multiple accession IDs can be specified using a list object:</p> <pre><code>ids = ['ERR908507', 'ERR908506', 'ERR908505']\nChannel\n.fromSRA(ids, apiKey: params.ncbi_api_key)\n.view()\n</code></pre> <pre><code>[ERR908507, [/vol1/fastq/ERR908/ERR908507/ERR908507_1.fastq.gz, /vol1/fastq/ERR908/ERR908507/ERR908507_2.fastq.gz]]\n[ERR908506, [/vol1/fastq/ERR908/ERR908506/ERR908506_1.fastq.gz, /vol1/fastq/ERR908/ERR908506/ERR908506_2.fastq.gz]]\n[ERR908505, [/vol1/fastq/ERR908/ERR908505/ERR908505_1.fastq.gz, /vol1/fastq/ERR908/ERR908505/ERR908505_2.fastq.gz]]\n</code></pre> <p>Info</p> <p>Read pairs are implicitly managed and are returned as a list of files.</p> <p>It\u2019s straightforward to use this channel as an input using the usual Nextflow syntax. The code below creates a channel containing two samples from a public SRA study and runs FASTQC on the resulting files. See:</p> <pre><code>params.ncbi_api_key = '&lt;Your API key here&gt;'\nparams.accession = ['ERR908507', 'ERR908506']\nprocess FASTQC {\ninput:\ntuple val(sample_id), path(reads_file)\noutput:\npath(\"fastqc_${sample_id}_logs\")\nscript:\n\"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads_file}\n    \"\"\"\n}\nworkflow {\nreads = Channel.fromSRA(params.accession, apiKey: params.ncbi_api_key)\nFASTQC(reads)\n}\n</code></pre> <p>If you want to run the workflow above and do not have fastqc installed in your machine, don\u2019t forget what you learned in the previous section. Run this workflow with <code>-with-docker biocontainers/fastqc:v0.11.5</code>, for example.</p>"},{"location":"basic_training/channels/#text-files","title":"Text files","text":"<p>The <code>splitText</code> operator allows you to split multi-line strings or text file items, emitted by a source channel into chunks containing n lines, which will be emitted by the resulting channel. See:</p> <pre><code>Channel\n.fromPath('data/meta/random.txt') // (1)!\n.splitText() // (2)!\n.view() // (3)!\n</code></pre> <ol> <li>Instructs Nextflow to make a channel from the path <code>data/meta/random.txt</code></li> <li>The <code>splitText</code> operator splits each item into chunks of one line by default.</li> <li>View contents of the channel.</li> </ol> <p>You can define the number of lines in each chunk by using the parameter <code>by</code>, as shown in the following example:</p> <pre><code>Channel\n.fromPath('data/meta/random.txt')\n.splitText(by: 2)\n.subscribe {\nprint it;\nprint \"--- end of the chunk ---\\n\"\n}\n</code></pre> <p>Info</p> <p>The <code>subscribe</code> operator permits execution of user defined functions each time a new value is emitted by the source channel.</p> <p>An optional closure can be specified in order to transform the text chunks produced by the operator. The following example shows how to split text files into chunks of 10 lines and transform them into capital letters:</p> <pre><code>Channel\n.fromPath('data/meta/random.txt')\n.splitText(by: 10) { it.toUpperCase() }\n.view()\n</code></pre> <p>You can also make counts for each line:</p> <pre><code>count = 0\nChannel\n.fromPath('data/meta/random.txt')\n.splitText()\n.view { \"${count++}: ${it.toUpperCase().trim()}\" }\n</code></pre> <p>Finally, you can also use the operator on plain files (outside of the channel context):</p> <pre><code>def f = file('data/meta/random.txt')\ndef lines = f.splitText()\ndef count = 0\nfor (String row : lines) {\nlog.info \"${count++} ${row.toUpperCase()}\"\n}\n</code></pre>"},{"location":"basic_training/channels/#comma-separate-values-csv","title":"Comma separate values (.csv)","text":"<p>The <code>splitCsv</code> operator allows you to parse text items emitted by a channel, that are CSV formatted.</p> <p>It then splits them into records or groups them as a list of records with a specified length.</p> <p>In the simplest case, just apply the <code>splitCsv</code> operator to a channel emitting a CSV formatted text files or text entries. For example, to view only the first and fourth columns:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_1.csv\")\n.splitCsv()\n// row is a list object\n.view { row -&gt; \"${row[0]}, ${row[3]}\" }\n</code></pre> <p>When the CSV begins with a header line defining the column names, you can specify the parameter <code>header: true</code> which allows you to reference each value by its column name, as shown in the following example:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_1.csv\")\n.splitCsv(header: true)\n// row is a list object\n.view { row -&gt; \"${row.patient_id}, ${row.num_samples}\" }\n</code></pre> <p>Alternatively, you can provide custom header names by specifying a list of strings in the header parameter as shown below:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_1.csv\")\n.splitCsv(header: ['col1', 'col2', 'col3', 'col4', 'col5'])\n// row is a list object\n.view { row -&gt; \"${row.col1}, ${row.col4}\" }\n</code></pre> <p>You can also process multiple CSV files at the same time:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_*.csv\") // &lt;-- just use a pattern\n.splitCsv(header: true)\n.view { row -&gt; \"${row.patient_id}\\t${row.num_samples}\" }\n</code></pre> <p>Tip</p> <p>Notice that you can change the output format simply by adding a different delimiter.</p> <p>Finally, you can also operate on CSV files outside the channel context:</p> <pre><code>def f = file('data/meta/patients_1.csv')\ndef lines = f.splitCsv()\nfor (List row : lines) {\nlog.info \"${row[0]} -- ${row[2]}\"\n}\n</code></pre> <p>Exercise</p> <p>Try inputting fastq reads into the RNA-Seq workflow from earlier using <code>.splitCsv</code>.</p> Solution <p>Add a CSV text file containing the following, as an example input with the name \"fastq.csv\":</p> <pre><code>gut,/workspace/gitpod/nf-training/data/ggal/gut_1.fq,/workspace/gitpod/nf-training/data/ggal/gut_2.fq\n</code></pre> <p>Then replace the input channel for the reads in <code>script7.nf</code>. Changing the following lines:</p> <pre><code>Channel\n.fromFilePairs(params.reads, checkIfExists: true)\n.set { read_pairs_ch }\n</code></pre> <p>To a splitCsv channel factory input:</p> <pre><code>Channel\n.fromPath(\"fastq.csv\")\n.splitCsv()\n.view { row -&gt; \"${row[0]}, ${row[1]}, ${row[2]}\" }\n.set { read_pairs_ch }\n</code></pre> <p>Finally, change the cardinality of the processes that use the input data. For example, for the quantification process, change it from:</p> <pre><code>process QUANTIFICATION {\ntag \"$sample_id\"\ninput:\npath salmon_index\ntuple val(sample_id), path(reads)\noutput:\npath sample_id, emit: quant_ch\nscript:\n\"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>To:</p> <pre><code>process QUANTIFICATION {\ntag \"$sample_id\"\ninput:\npath salmon_index\ntuple val(sample_id), path(reads1), path(reads2)\noutput:\npath sample_id, emit: quant_ch\nscript:\n\"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads1} -2 ${reads2} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>Repeat the above for the fastqc step.</p> <pre><code>process FASTQC {\ntag \"FASTQC on $sample_id\"\ninput:\ntuple val(sample_id), path(reads1), path(reads2)\noutput:\npath \"fastqc_${sample_id}_logs\"\nscript:\n\"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads1} ${reads2}\n    \"\"\"\n}\n</code></pre> <p>Now the workflow should run from a CSV file.</p>"},{"location":"basic_training/channels/#tab-separated-values-tsv","title":"Tab separated values (.tsv)","text":"<p>Parsing TSV files works in a similar way, simply add the <code>sep: '\\t'</code> option in the <code>splitCsv</code> context:</p> <pre><code>Channel\n.fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n// use `sep` option to parse TAB separated files\n.splitCsv(sep: '\\t')\n.view()\n</code></pre> <p>Exercise</p> <p>Try using the tab separation technique on the file <code>data/meta/regions.tsv</code>, but print just the first column, and remove the header.</p> Solution <pre><code>Channel\n.fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n// use `sep` option to parse TAB separated files\n.splitCsv(sep: '\\t', header: true)\n// row is a list object\n.view { row -&gt; \"${row.patient_id}\" }\n</code></pre>"},{"location":"basic_training/channels/#more-complex-file-formats","title":"More complex file formats","text":""},{"location":"basic_training/channels/#json","title":"JSON","text":"<p>We can also easily parse the JSON file format using the <code>splitJson</code> channel operator.</p> <p>The <code>splitJson</code> operator supports JSON arrays:</p> Source codeOutput <pre><code>Channel\n.of('[\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: Sunday\nItem: Monday\nItem: Tuesday\nItem: Wednesday\nItem: Thursday\nItem: Friday\nItem: Saturday\n</code></pre> <p>JSON objects:</p> Source codeOutput <pre><code>Channel\n.of('{\"player\": {\"name\": \"Bob\", \"height\": 180, \"champion\": false}}')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: [key:player, value:[name:Bob, height:180, champion:false]]\n</code></pre> <p>And even a JSON array of JSON objects!</p> Source codeOutput <pre><code>Channel\n.of('[{\"name\": \"Bob\", \"height\": 180, \"champion\": false}, \\\n        {\"name\": \"Alice\", \"height\": 170, \"champion\": false}]')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: [name:Bob, height:180, champion:false]\nItem: [name:Alice, height:170, champion:false]\n</code></pre> <p>Files containing JSON content can also be parsed:</p> Source codefile.jsonOutput <pre><code>Channel\n.fromPath('file.json')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>[\n{ \"name\": \"Bob\", \"height\": 180, \"champion\": false },\n{ \"name\": \"Alice\", \"height\": 170, \"champion\": false }\n]\n</code></pre> <pre><code>Item: [name:Bob, height:180, champion:false]\nItem: [name:Alice, height:170, champion:false]\n</code></pre>"},{"location":"basic_training/channels/#yaml","title":"YAML","text":"<p>This can also be used as a way to parse YAML files:</p> Source codedata/meta/regions.ymlOutput <pre><code>import org.yaml.snakeyaml.Yaml\ndef f = file('data/meta/regions.yml')\ndef records = new Yaml().load(f)\nfor (def entry : records) {\nlog.info \"$entry.patient_id -- $entry.feature\"\n}\n</code></pre> <pre><code>- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: pass_vafqc_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: pass_stripy_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: pass_manual_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: other_region_selection_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: ace_information_gained\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: concordance_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: pass_vafqc_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: pass_stripy_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: pass_manual_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: other_region_selection_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: ace_information_gained\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: concordance_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R3\nfeature: pass_vafqc_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R3\nfeature: pass_stripy_flag\npass_flag: \"FALSE\"\n</code></pre> <pre><code>ATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\nATX-TBL-001-GB-01-105 -- pass_manual_flag\nATX-TBL-001-GB-01-105 -- other_region_selection_flag\nATX-TBL-001-GB-01-105 -- ace_information_gained\nATX-TBL-001-GB-01-105 -- concordance_flag\nATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\nATX-TBL-001-GB-01-105 -- pass_manual_flag\nATX-TBL-001-GB-01-105 -- other_region_selection_flag\nATX-TBL-001-GB-01-105 -- ace_information_gained\nATX-TBL-001-GB-01-105 -- concordance_flag\nATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\n</code></pre>"},{"location":"basic_training/channels/#storage-of-parsers-into-modules","title":"Storage of parsers into modules","text":"<p>The best way to store parser scripts is to keep them in a Nextflow module file.</p> <p>Let's say we don't have a JSON channel operator, but we create a function instead. The <code>parsers.nf</code> file should contain the <code>parseJsonFile</code> function. See the contente below:</p> Source code./modules/parsers.nfOutput <pre><code>include { parseJsonFile } from './modules/parsers.nf'\nprocess FOO {\ninput:\ntuple val(patient_id), val(feature)\noutput:\nstdout\nscript:\n\"\"\"\n    echo $patient_id has $feature as feature\n    \"\"\"\n}\nworkflow {\nChannel\n.fromPath('data/meta/regions*.json')\n| flatMap { parseJsonFile(it) }\n| map { record -&gt; [record.patient_id, record.feature] }\n| unique\n| FOO\n| view\n}\n</code></pre> <pre><code>import groovy.json.JsonSlurper\ndef parseJsonFile(json_file) {\ndef f = file(json_file)\ndef records = new JsonSlurper().parse(f)\nreturn records\n}\n</code></pre> <pre><code>ATX-TBL-001-GB-01-105 has pass_stripy_flag as feature\nATX-TBL-001-GB-01-105 has ace_information_gained as feature\nATX-TBL-001-GB-01-105 has concordance_flag as feature\nATX-TBL-001-GB-01-105 has pass_vafqc_flag as feature\nATX-TBL-001-GB-01-105 has pass_manual_flag as feature\nATX-TBL-001-GB-01-105 has other_region_selection_flag as feature\n</code></pre> <p>Nextflow will use this as a custom function within the workflow scope.</p> <p>Tip</p> <p>You will learn more about module files later in the Modularization section of this tutorial.</p>"},{"location":"basic_training/channels.pt/","title":"Canais","text":"<p>Canais s\u00e3o uma estrutura de dados chave do Nextflow que permite a implementa\u00e7\u00e3o de fluxos de trabalho computacionais utilizando paradigmas funcional e reativo com base no paradigma de programa\u00e7\u00e3o Dataflow.</p> <p>Eles s\u00e3o usados para conectar logicamente tarefas entre si ou para implementar transforma\u00e7\u00f5es de dados de estilo funcional.</p> tarefa \u03b1arquivo zarquivo yarquivo xtarefa \u03b2Canal"},{"location":"basic_training/channels.pt/#tipos-de-canais","title":"Tipos de canais","text":"<p>O Nextflow distingue dois tipos diferentes de canais: canais de fila e canais de valor.</p>"},{"location":"basic_training/channels.pt/#canal-de-fila","title":"Canal de fila","text":"<p>Um canal de fila \u00e9 uma fila ass\u00edncrona unidirecional FIFO (First-in-First-out, o primeiro a entrar, \u00e9 o primeiro a sair) que conecta dois processos ou operadores.</p> <ul> <li>ass\u00edncrono significa que as opera\u00e7\u00f5es ocorrem sem bloqueio.</li> <li>unidirecional significa que os dados fluem do gerador para o consumidor.</li> <li>FIFO significa que os dados s\u00e3o entregues na mesma ordem em que s\u00e3o produzidos. Primeiro a entrar, primeiro a sair.</li> </ul> <p>Um canal de fila \u00e9 criado implicitamente por defini\u00e7\u00f5es de sa\u00edda de um processo ou usando f\u00e1bricas de canal, como o Channel.of ou Channel.fromPath.</p> <p>Tente os seguintes trechos de c\u00f3digo:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>canal = Channel.of(1, 2, 3)\nprintln(canal) // (1)!\ncanal.view() // (2)!\n</code></pre> <ol> <li>Use a fun\u00e7\u00e3o <code>println</code> embutida no Nextflow por padr\u00e3o para imprimir o conte\u00fado do canal <code>canal</code></li> <li>Aplique o operador <code>view</code> no canal <code>canal</code> para imprimir cada emiss\u00e3o desse canal</li> </ol> <p>Exercise</p> <p>Tente executar este trecho de c\u00f3digo. Voc\u00ea pode fazer isso criando um novo arquivo <code>.nf</code> ou editando um arquivo <code>.nf</code> j\u00e1 existente.</p> <pre><code>canal = Channel.of(1, 2, 3)\ncanal.view()\n</code></pre>"},{"location":"basic_training/channels.pt/#canais-de-valor","title":"Canais de valor","text":"<p>Um canal de valor (tamb\u00e9m conhecido como canal singleton), por defini\u00e7\u00e3o, est\u00e1 vinculado a um \u00fanico valor e pode ser lido quantas vezes for necess\u00e1rio sem consumir seu conte\u00fado. Um canal de <code>valor</code> \u00e9 criado usando a f\u00e1brica de canal value ou por operadores que retornam um valor apenas, como first, last, collect, count, min, max, reduce, e sum.</p> <p>Para entender melhor a diferen\u00e7a entre canais de valor e de fila, salve o trecho abaixo como <code>exemplo.nf</code>.</p> exemplo.nf<pre><code>canal1 = Channel.of(1, 2, 3)\ncanal2 = Channel.of(1)\nprocess SUM {\ninput:\nval x\nval y\noutput:\nstdout\nscript:\n\"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\nworkflow {\nSUM(canal1, canal2).view()\n}\n</code></pre> <p>Ao rodar o script, ele imprime apenas 2, como voc\u00ea pode ver abaixo:</p> <pre><code>2\n</code></pre> <p>Um processo s\u00f3 instanciar\u00e1 uma tarefa quando houver elementos a serem consumidos de todos os canais fornecidos como entrada para ele. Como <code>canal1</code> e <code>canal2</code> s\u00e3o canais de fila, e o \u00fanico elemento de <code>canal2</code> foi consumido, nenhuma nova inst\u00e2ncia de processo ser\u00e1 iniciada, mesmo se houver outros elementos a serem consumidos em <code>canal1</code>.</p> <p>Para usar o \u00fanico elemento em <code>canal2</code> v\u00e1rias vezes, podemos usar <code>Channel.value</code> como mencionado acima, ou usar um operador de canal que retorna um \u00fanico elemento como <code>first()</code> abaixo:</p> <pre><code>canal1 = Channel.of(1, 2, 3)\ncanal2 = Channel.of(1)\nprocess SUM {\ninput:\nval x\nval y\noutput:\nstdout\nscript:\n\"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\nworkflow {\nSUM(canal1, canal2.first()).view()\n}\n</code></pre> Output<pre><code>4\n3\n2\n</code></pre> <p>Al\u00e9m disso, em muitas situa\u00e7\u00f5es, o Nextflow converter\u00e1 implicitamente vari\u00e1veis em canais de valor quando forem usadas em uma chamada de processo. Por exemplo, quando voc\u00ea chama um processo com um par\u00e2metro de fluxo de trabalho (<code>params.exemplo</code>) que possui um valor de string, ele \u00e9 automaticamente convertido em um canal de valor.</p>"},{"location":"basic_training/channels.pt/#fabricas-de-canal","title":"F\u00e1bricas de canal","text":"<p>Estes s\u00e3o comandos do Nextflow para criar canais que possuem entradas e fun\u00e7\u00f5es impl\u00edcitas esperadas.</p>"},{"location":"basic_training/channels.pt/#value","title":"<code>value()</code>","text":"<p>A f\u00e1brica de canal <code>value</code> \u00e9 utilizada para criar um canal de valor. Um argumento opcional n\u00e3o <code>nulo</code> pode ser especificado para vincular o canal a um valor espec\u00edfico. Por exemplo:</p> <pre><code>canal1 = Channel.value() // (1)!\ncanal2 = Channel.value('Ol\u00e1, voc\u00ea!') // (2)!\ncanal3 = Channel.value([1, 2, 3, 4, 5]) // (3)!\n</code></pre> <ol> <li>Cria um canal de valor vazio</li> <li>Cria um canal de valor e vincula uma string a ele</li> <li>Cria um canal de valor e vincula a ele um objeto de lista que ser\u00e1 emitido como uma \u00fanica emiss\u00e3o</li> </ol>"},{"location":"basic_training/channels.pt/#of","title":"<code>of()</code>","text":"<p>A f\u00e1brica <code>Channel.of</code> permite a cria\u00e7\u00e3o de um canal de fila com os valores especificados como argumentos.</p> <pre><code>canal = Channel.of(1, 3, 5, 7)\ncanal.view { \"numero: $it\" }\n</code></pre> <p>A primeira linha neste exemplo cria uma vari\u00e1vel <code>canal</code> que cont\u00e9m um objeto de canal. Este canal emite os valores especificados como par\u00e2metro na f\u00e1brica de canal <code>of</code>. Assim, a segunda linha imprimir\u00e1 o seguinte:</p> <pre><code>numero: 1\nnumero: 3\nnumero: 5\nnumero: 7\n</code></pre> <p>A f\u00e1brica de canal <code>Channel.of</code> funciona de maneira semelhante ao <code>Channel.from</code> (que foi descontinuado), corrigindo alguns comportamentos inconsistentes do \u00faltimo e fornecendo um melhor manuseio quando um intervalo de valores \u00e9 especificado. Por exemplo, o seguinte funciona com um intervalo de 1 a 23:</p> <pre><code>Channel\n.of(1..23, 'X', 'Y')\n.view()\n</code></pre>"},{"location":"basic_training/channels.pt/#fromlist","title":"<code>fromList()</code>","text":"<p>A f\u00e1brica de canal <code>Channel.fromList</code> cria um canal emitindo os elementos fornecidos por um objeto de lista especificado como um argumento:</p> <pre><code>list = ['ol\u00e1', 'mundo']\nChannel\n.fromList(list)\n.view()\n</code></pre>"},{"location":"basic_training/channels.pt/#frompath","title":"<code>fromPath()</code>","text":"<p>A f\u00e1brica de canal <code>fromPath</code> cria um canal de fila emitindo um ou mais arquivos correspondentes ao padr\u00e3o glob especificado.</p> <pre><code>Channel.fromPath('./data/meta/*.csv')\n</code></pre> <p>Este exemplo cria um canal e emite tantos itens quanto arquivos com extens\u00e3o <code>csv</code> existirem na pasta <code>./data/meta</code>. Cada elemento \u00e9 um objeto de arquivo implementando a interface Path do Java.</p> <p>Tip</p> <p>Dois asteriscos, ou seja, <code>**</code>, funcionam como <code>*</code>, mas cruzam os limites do diret\u00f3rio. Essa sintaxe geralmente \u00e9 usada para percorrer caminhos completos. Os colchetes especificam uma cole\u00e7\u00e3o de subpadr\u00f5es.</p> Nome Descri\u00e7\u00e3o glob Quando <code>true</code> interpreta caracteres <code>*</code>, <code>?</code>, <code>[]</code> e <code>{}</code> como glob wildcards, caso contr\u00e1rio, os trata como caracteres normais (padr\u00e3o: <code>true</code>) type Tipo de caminho retornado, ou <code>file</code>, <code>dir</code> ou <code>any</code> (padr\u00e3o: <code>file</code>) hidden Quando <code>true</code> inclui arquivos ocultos nos caminhos resultantes (padr\u00e3o: <code>false</code>) maxDepth N\u00famero m\u00e1ximo de n\u00edveis de diret\u00f3rio a serem visitados (padr\u00e3o: <code>no limit</code>) followLinks Quando <code>true</code> links simb\u00f3licos s\u00e3o seguidos durante a travessia da \u00e1rvore de diret\u00f3rios, caso contr\u00e1rio, eles s\u00e3o gerenciados como arquivos (padr\u00e3o: <code>true</code>) relative Quando <code>true</code> os caminhos de retorno s\u00e3o relativos ao diret\u00f3rio de topo mais comum (padr\u00e3o: <code>false</code>) checkIfExists Quando <code>true</code> lan\u00e7a uma exce\u00e7\u00e3o quando o caminho especificado n\u00e3o existe no sistema de arquivos (padr\u00e3o: <code>false</code>) <p>Saiba mais sobre a sintaxe dos padr\u00f5es glob neste link.</p> <p>Exercise</p> <p>Use a f\u00e1brica de canal <code>Channel.fromPath</code> para criar um canal emitindo todos os arquivos com o sufixo <code>.fq</code> no diret\u00f3rio <code>data/ggal/</code> e qualquer subdiret\u00f3rio, al\u00e9m dos arquivos ocultos. Em seguida, imprima os nomes dos arquivos.</p> Solution <pre><code>Channel\n.fromPath('./data/ggal/**.fq', hidden: true)\n.view()\n</code></pre>"},{"location":"basic_training/channels.pt/#fromfilepairs","title":"<code>fromFilePairs()</code>","text":"<p>A f\u00e1brica de canal <code>fromFilePairs</code> cria um canal emitindo os pares de arquivos correspondentes a um padr\u00e3o glob fornecido pelo usu\u00e1rio. Os arquivos correspondentes s\u00e3o emitidos como tuplas, nas quais o primeiro elemento \u00e9 a chave de agrupamento do par correspondente e o segundo elemento \u00e9 a lista de arquivos (classificados em ordem lexicogr\u00e1fica).</p> <pre><code>Channel\n.fromFilePairs('./data/ggal/*_{1,2}.fq')\n.view()\n</code></pre> <p>Ele produzir\u00e1 uma sa\u00edda semelhante \u00e0 seguinte:</p> <pre><code>[liver, [/workspace/gitpod/nf-training/data/ggal/liver_1.fq, /workspace/gitpod/nf-training/data/ggal/liver_2.fq]]\n[gut, [/workspace/gitpod/nf-training/data/ggal/gut_1.fq, /workspace/gitpod/nf-training/data/ggal/gut_2.fq]]\n[lung, [/workspace/gitpod/nf-training/data/ggal/lung_1.fq, /workspace/gitpod/nf-training/data/ggal/lung_2.fq]]\n</code></pre> <p>Warning</p> <p>O padr\u00e3o glob precisa conter pelo menos um caractere curinga de estrela (<code>*</code>).</p> Nome Descri\u00e7\u00e3o type Tipo de caminhos retornados, ou <code>file</code>, <code>dir</code> ou <code>any</code> (padr\u00e3o: <code>file</code>) hidden Quando <code>true</code> inclui arquivos ocultos nos caminhos resultantes (padr\u00e3o: <code>false</code>) maxDepth N\u00famero m\u00e1ximo de n\u00edveis de diret\u00f3rio a serem visitados (padr\u00e3o: <code>no limit</code>) followLinks Quando <code>true</code> links simb\u00f3licos s\u00e3o seguidos durante a travessia da \u00e1rvore de diret\u00f3rios, caso contr\u00e1rio, eles s\u00e3o gerenciados como arquivos (padr\u00e3o: <code>true</code>) size Define o n\u00famero de arquivos que cada item emitido deve conter (padr\u00e3o: <code>2</code>). Use <code>-1</code> para qualquer n\u00famero flat Quando <code>true</code> os arquivos correspondentes s\u00e3o produzidos como \u00fanicos elementos nas tuplas emitidas (padr\u00e3o: <code>false</code>) checkIfExists Quando <code>true</code> lan\u00e7a uma exce\u00e7\u00e3o quando o caminho especificado n\u00e3o existe no sistema de arquivos (padr\u00e3o: <code>false</code>) <p>Exercise</p> <p>Use a f\u00e1brica de canal <code>fromFilePairs</code> para criar um canal emitindo todos os pares de leituras em fastq no diret\u00f3rio <code>data/ggal/</code> e imprima-os. Em seguida, use a op\u00e7\u00e3o <code>flat: true</code> e compare a sa\u00edda com a execu\u00e7\u00e3o anterior.</p> Solution <p>Use o seguinte, com ou sem <code>flat: true</code>:</p> <pre><code>Channel\n.fromFilePairs('./data/ggal/*_{1,2}.fq', flat: true)\n.view()\n</code></pre> <p>Em seguida, verifique os colchetes ao redor dos nomes dos arquivos, para ver a diferen\u00e7a com <code>flat</code>.</p>"},{"location":"basic_training/channels.pt/#fromsra","title":"<code>fromSRA()</code>","text":"<p>A f\u00e1brica de canal <code>Channel.fromSRA</code> permite consultar o banco de dados NCBI SRA e retorna um canal que emite os arquivos FASTQ correspondentes aos crit\u00e9rios de sele\u00e7\u00e3o especificados.</p> <p>A consulta pode ser ID(s) de projeto(s) ou n\u00famero(s) de acesso suportado(s) pela API do NCBI ESearch.</p> <p>Info</p> <p>Esta fun\u00e7\u00e3o agora requer uma chave de API que voc\u00ea s\u00f3 pode obter fazendo login em sua conta NCBI.</p> Instru\u00e7\u00f5es para login do NCBI e aquisi\u00e7\u00e3o de chave <ol> <li>V\u00e1 para: https://www.ncbi.nlm.nih.gov/</li> <li>Clique no bot\u00e3o \"Login\" no canto superior direito para entrar no NCBI. Siga suas instru\u00e7\u00f5es.</li> <li>Uma vez em sua conta, clique no bot\u00e3o no canto superior direito, geralmente seu ID.</li> <li>V\u00e1 para Account settings</li> <li>Role para baixo at\u00e9 a se\u00e7\u00e3o \"API Key Management\".</li> <li>Clique em \"Create an API Key\".</li> <li>A p\u00e1gina ser\u00e1 atualizada e a chave ser\u00e1 exibida onde estava o bot\u00e3o. Copie sua chave.</li> </ol> <p>Por exemplo, o trecho a seguir imprimir\u00e1 o conte\u00fado de um ID de projeto NCBI:</p> <pre><code>params.ncbi_api_key = '&lt;Sua chave da API aqui&gt;'\nChannel\n.fromSRA(['SRP073307'], apiKey: params.ncbi_api_key)\n.view()\n</code></pre> <p> Substitua <code>&lt;Sua chave de API aqui&gt;</code> com sua chave de API.</p> <p>Isso deve imprimir:</p> <pre><code>[SRR3383346, [/vol1/fastq/SRR338/006/SRR3383346/SRR3383346_1.fastq.gz, /vol1/fastq/SRR338/006/SRR3383346/SRR3383346_2.fastq.gz]]\n[SRR3383347, [/vol1/fastq/SRR338/007/SRR3383347/SRR3383347_1.fastq.gz, /vol1/fastq/SRR338/007/SRR3383347/SRR3383347_2.fastq.gz]]\n[SRR3383344, [/vol1/fastq/SRR338/004/SRR3383344/SRR3383344_1.fastq.gz, /vol1/fastq/SRR338/004/SRR3383344/SRR3383344_2.fastq.gz]]\n[SRR3383345, [/vol1/fastq/SRR338/005/SRR3383345/SRR3383345_1.fastq.gz, /vol1/fastq/SRR338/005/SRR3383345/SRR3383345_2.fastq.gz]]\n// (o restante foi omitido)\n</code></pre> <p>V\u00e1rios IDs de acesso podem ser especificados usando um objeto lista:</p> <pre><code>ids = ['ERR908507', 'ERR908506', 'ERR908505']\nChannel\n.fromSRA(ids, apiKey: params.ncbi_api_key)\n.view()\n</code></pre> <pre><code>[ERR908507, [/vol1/fastq/ERR908/ERR908507/ERR908507_1.fastq.gz, /vol1/fastq/ERR908/ERR908507/ERR908507_2.fastq.gz]]\n[ERR908506, [/vol1/fastq/ERR908/ERR908506/ERR908506_1.fastq.gz, /vol1/fastq/ERR908/ERR908506/ERR908506_2.fastq.gz]]\n[ERR908505, [/vol1/fastq/ERR908/ERR908505/ERR908505_1.fastq.gz, /vol1/fastq/ERR908/ERR908505/ERR908505_2.fastq.gz]]\n</code></pre> <p>Info</p> <p>Os pares de leituras s\u00e3o gerenciados implicitamente e s\u00e3o retornados como uma lista de arquivos.</p> <p>\u00c9 f\u00e1cil usar este canal como uma entrada usando a sintaxe usual do Nextflow. O c\u00f3digo abaixo cria um canal contendo duas amostras de um estudo SRA p\u00fablico e executa o FASTQC nos arquivos resultantes. Veja:</p> <pre><code>params.ncbi_chave_api = '&lt;Sua chave de API aqui&gt;'\nparams.accession = ['ERR908507', 'ERR908506']\nprocess FASTQC {\ninput:\ntuple val(id_amostra), path(arquivo_de_leituras)\noutput:\npath(\"fastqc_${id_amostra}_logs\")\nscript:\n\"\"\"\n    mkdir fastqc_${id_amostra}_logs\n    fastqc -o fastqc_${id_amostra}_logs -f fastq -q ${arquivo_de_leituras}\n    \"\"\"\n}\nworkflow {\nleituras = Channel.fromSRA(params.accession, apiKey: params.ncbi_chave_api)\nFASTQC(leituras)\n}\n</code></pre> <p>Se voc\u00ea deseja executar o fluxo de trabalho acima e n\u00e3o possui o fastqc instalado em sua m\u00e1quina, n\u00e3o esque\u00e7a o que aprendeu na se\u00e7\u00e3o anterior. Execute este fluxo de trabalho com <code>-with-docker biocontainers/fastqc:v0.11.5</code>, por exemplo.</p>"},{"location":"basic_training/channels.pt/#arquivos-de-texto","title":"Arquivos de texto","text":"<p>O operador <code>splitText</code> permite dividir strings de v\u00e1rias linhas ou itens de arquivo de texto, emitidos por um canal de origem em blocos contendo n linhas, que ser\u00e3o emitidos pelo canal resultante. Veja:</p> <pre><code>Channel\n.fromPath('data/meta/random.txt') // (1)!\n.splitText() // (2)!\n.view() // (3)!\n</code></pre> <ol> <li>Instrui o Nextflow a criar um canal a partir do caminho <code>data/meta/random.txt</code></li> <li>O operador <code>splitText</code> divide cada item em peda\u00e7os de uma linha por padr\u00e3o.</li> <li>Veja o conte\u00fado do canal.</li> </ol> <p>Voc\u00ea pode definir o n\u00famero de linhas em cada bloco usando o par\u00e2metro <code>by</code>, conforme mostrado no exemplo a seguir:</p> <pre><code>Channel\n.fromPath('data/meta/random.txt')\n.splitText(by: 2)\n.subscribe {\nprint it;\nprint \"--- fim do bloco ---\\n\"\n}\n</code></pre> <p>Info</p> <p>O operador <code>subscribe</code> permite a execu\u00e7\u00e3o de fun\u00e7\u00f5es definidas pelo usu\u00e1rio cada vez que um novo valor \u00e9 emitido pelo canal de origem.</p> <p>Uma clausura opcional pode ser especificada para transformar os blocos de texto produzidos pelo operador. O exemplo a seguir mostra como dividir arquivos de texto em blocos de 10 linhas e transform\u00e1-los em letras mai\u00fasculas:</p> <pre><code>Channel\n.fromPath('data/meta/random.txt')\n.splitText(by: 10) { it.toUpperCase() }\n.view()\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode fazer contagens para cada linha:</p> <pre><code>contador = 0\nChannel\n.fromPath('data/meta/random.txt')\n.splitText()\n.view { \"${contador++}: ${it.toUpperCase().trim()}\" }\n</code></pre> <p>Por fim, voc\u00ea tamb\u00e9m pode usar o operador em arquivos simples (fora do contexto do canal):</p> <pre><code>def f = file('data/meta/random.txt')\ndef linhas = f.splitText()\ndef contador = 0\nfor (String linha : linhas) {\nlog.info \"${contador++} ${linha.toUpperCase()}\"\n}\n</code></pre>"},{"location":"basic_training/channels.pt/#valores-separados-por-virgula-csv","title":"Valores separados por v\u00edrgula (.csv)","text":"<p>O operador <code>splitCsv</code> permite analisar itens de texto formatados em CSV (Comma-separated value) emitidos por um canal.</p> <p>Em seguida, ele os divide em registros ou os agrupa como uma lista de registros com um comprimento especificado.</p> <p>No caso mais simples, basta aplicar o operador <code>splitCsv</code> a um canal que emite arquivos de texto ou entradas de texto no formato CSV. Por exemplo, para visualizar apenas a primeira e a quarta colunas:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_1.csv\")\n.splitCsv()\n// linha \u00e9 um objeto de lista\n.view { linha -&gt; \"${linha[0]}, ${linha[3]}\" }\n</code></pre> <p>Quando o CSV come\u00e7a com uma linha de cabe\u00e7alho definindo os nomes das colunas, voc\u00ea pode especificar o par\u00e2metro <code>header: true</code> que permite referenciar cada valor pelo nome da coluna, conforme mostrado no exemplo a seguir:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_1.csv\")\n.splitCsv(header: true)\n// linha \u00e9 um objeto de lista\n.view { linha -&gt; \"${linha.patient_id}, ${linha.num_samples}\" }\n</code></pre> <p>Como alternativa, voc\u00ea pode fornecer nomes de cabe\u00e7alho personalizados especificando uma lista de strings no par\u00e2metro de cabe\u00e7alho, conforme mostrado abaixo:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_1.csv\")\n.splitCsv(header: ['col1', 'col2', 'col3', 'col4', 'col5'])\n// linha \u00e9 um objeto de lista\n.view { linha -&gt; \"${linha.col1}, ${linha.col4}\" }\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode processar v\u00e1rios arquivos CSV ao mesmo tempo:</p> <pre><code>Channel\n.fromPath(\"data/meta/patients_*.csv\") // &lt;-- use um padr\u00e3o de captura\n.splitCsv(header: true)\n.view { linha -&gt; \"${linha.patient_id}\\t${linha.num_samples}\" }\n</code></pre> <p>Tip</p> <p>Observe que voc\u00ea pode alterar o formato de sa\u00edda simplesmente adicionando um delimitador diferente.</p> <p>Por fim, voc\u00ea tamb\u00e9m pode operar em arquivos CSV fora do contexto do canal:</p> <pre><code>def f = file('data/meta/patients_1.csv')\ndef linhas = f.splitCsv()\nfor (List linha : linhas) {\nlog.info \"${linha[0]} -- ${linha[2]}\"\n}\n</code></pre> <p>Exercise</p> <p>Tente inserir leituras fastq no fluxo de trabalho do RNA-Seq anterior usando <code>.splitCsv</code>.</p> Solution <p>Adicione um arquivo de texto CSV contendo o seguinte, como uma entrada de exemplo com o nome \"fastq.csv\":</p> <pre><code>gut,/workspace/gitpod/nf-training/data/ggal/gut_1.fq,/workspace/gitpod/nf-training/data/ggal/gut_2.fq\n</code></pre> <p>Em seguida, substitua o canal de entrada para as leituras em <code>script7.nf</code>, alterando as seguintes linhas:</p> <pre><code>Channel\n.fromFilePairs(params.reads, checkIfExists: true)\n.set { read_pairs_ch }\n</code></pre> <p>Para uma entrada de f\u00e1brica de canal splitCsv:</p> <pre><code>Channel\n.fromPath(\"fastq.csv\")\n.splitCsv()\n.view { linha -&gt; \"${linha[0]}, ${linha[1]}, ${linha[2]}\" }\n.set { read_pairs_ch }\n</code></pre> <p>Por fim, altere a cardinalidade dos processos que usam os dados de entrada. Por exemplo, para o processo de quantifica\u00e7\u00e3o, mude de:</p> <pre><code>process QUANTIFICATION {\ntag \"$sample_id\"\ninput:\npath salmon_index\ntuple val(sample_id), path(reads)\noutput:\npath sample_id, emit: quant_ch\nscript:\n\"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>Para:</p> <pre><code>process QUANTIFICATION {\ntag \"$sample_id\"\ninput:\npath salmon_index\ntuple val(sample_id), path(reads1), path(reads2)\noutput:\npath sample_id, emit: quant_ch\nscript:\n\"\"\"\n    salmon quant --threads $task.cpus --libType=U -i $salmon_index -1 ${reads1} -2 ${reads2} -o $sample_id\n    \"\"\"\n}\n</code></pre> <p>Repita o procedimento acima para a etapa fastqc.</p> <pre><code>process FASTQC {\ntag \"FASTQC on $sample_id\"\ninput:\ntuple val(sample_id), path(reads1), path(reads2)\noutput:\npath \"fastqc_${sample_id}_logs\"\nscript:\n\"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads1} ${reads2}\n    \"\"\"\n}\n</code></pre> <p>Agora o fluxo de trabalho deve ser executado a partir de um arquivo CSV.</p>"},{"location":"basic_training/channels.pt/#valores-separados-por-tabulacao-tsv","title":"Valores separados por tabula\u00e7\u00e3o (.tsv)","text":"<p>A an\u00e1lise de arquivos TSV funciona de maneira semelhante, basta adicionar a op\u00e7\u00e3o <code>sep: '\\t'</code> no contexto do <code>splitCsv</code>:</p> <pre><code>Channel\n.fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n// Use a op\u00e7\u00e3o `sep` para analisar arquivos com tabula\u00e7\u00e3o como separador\n.splitCsv(sep: '\\t')\n.view()\n</code></pre> <p>Exercise</p> <p>Tente usar a t\u00e9cnica de separa\u00e7\u00e3o por tabula\u00e7\u00e3o no arquivo <code>data/meta/regions.tsv</code>, mas imprima apenas a primeira coluna e remova o cabe\u00e7alho.</p> Solution <pre><code>Channel\n.fromPath(\"data/meta/regions.tsv\", checkIfExists: true)\n// Use a op\u00e7\u00e3o `sep` para analisar arquivos com tabula\u00e7\u00e3o como separador\n.splitCsv(sep: '\\t', header: true)\n// linha \u00e9 um objeto de lista\n.view { linha -&gt; \"${linha.patient_id}\" }\n</code></pre>"},{"location":"basic_training/channels.pt/#formatos-de-arquivo-mais-complexos","title":"Formatos de arquivo mais complexos","text":""},{"location":"basic_training/channels.pt/#json","title":"JSON","text":"<p>Tamb\u00e9m podemos analisar facilmente o formato de arquivo JSON usando o oeprador de canal <code>splitJson</code>.</p> <p>O operador <code>splitJson</code> suporta arranjos JSON:</p> C\u00f3digo-fonteSa\u00edda <pre><code>Channel\n.of('[\"Domingo\", \"Segunda\", \"Ter\u00e7a\", \"Quarta\", \"Quinta\", \"Sexta\", \"S\u00e1bado\"]')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: Domingo\nItem: Segunda\nItem: Ter\u00e7a\nItem: Quarta\nItem: Quinta\nItem: Sexta\nItem: S\u00e1bado\n</code></pre> <p>Objetos JSON:</p> C\u00f3digo-fonteSa\u00edda <pre><code>Channel\n.of('{\"jogador\": {\"nome\": \"Bob\", \"altura\": 180, \"venceu_campeonato\": false}}')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: [key:jogador, value:[nome:Bob, altura:180, venceu_campeonato:false]]\n</code></pre> <p>E inclusive arranjos JSON com objetos JSON!</p> C\u00f3digo-fonteSa\u00edda <pre><code>Channel\n.of('[{\"nome\": \"Bob\", \"altura\": 180, \"venceu_campeonato\": false}, \\\n        {\"nome\": \"Alice\", \"height\": 170, \"venceu_campeonato\": false}]')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>Item: [nome:Bob, altura:180, venceu_campeonato:false]\nItem: [nome:Alice, altura:170, venceu_campeonato:false]\n</code></pre> <p>Arquivos contendo dados em formato JSON tamb\u00e9m podem ser analisados:</p> C\u00f3digo-fontearquivo.jsonSa\u00edda <pre><code>Channel\n.fromPath('arquivo.json')\n.splitJson()\n.view { \"Item: ${it}\" }\n</code></pre> <pre><code>[{\"nome\": \"Bob\", \"altura\": 180, \"venceu_campeonato\": false}, {\"nome\": \"Alice\", \"altura\": 170, \"venceu_campeonato\": false}]\n</code></pre> <pre><code>Item: [nome:Bob, altura:180, venceu_campeonato:false]\nItem: [nome:Alice, altura:170, venceu_campeonato:false]\n</code></pre>"},{"location":"basic_training/channels.pt/#yaml","title":"YAML","text":"<p>Isso tamb\u00e9m pode ser usado como uma forma de analisar arquivos YAML:</p> C\u00f3digo-fontedata/meta/regions.ymlSa\u00edda <pre><code>import org.yaml.snakeyaml.Yaml\ndef f = file('data/meta/regions.yml')\ndef registros = new Yaml().load(f)\nfor (def entrada : registros) {\nlog.info \"$entrada.patient_id -- $entrada.feature\"\n}\n</code></pre> <pre><code>- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: pass_vafqc_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: pass_stripy_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: pass_manual_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: other_region_selection_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: ace_information_gained\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R1\nfeature: concordance_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: pass_vafqc_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: pass_stripy_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: pass_manual_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: other_region_selection_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: ace_information_gained\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R2\nfeature: concordance_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R3\nfeature: pass_vafqc_flag\npass_flag: \"TRUE\"\n- patient_id: ATX-TBL-001-GB-01-105\nregion_id: R3\nfeature: pass_stripy_flag\npass_flag: \"FALSE\"\n</code></pre> <pre><code>ATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\nATX-TBL-001-GB-01-105 -- pass_manual_flag\nATX-TBL-001-GB-01-105 -- other_region_selection_flag\nATX-TBL-001-GB-01-105 -- ace_information_gained\nATX-TBL-001-GB-01-105 -- concordance_flag\nATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\nATX-TBL-001-GB-01-105 -- pass_manual_flag\nATX-TBL-001-GB-01-105 -- other_region_selection_flag\nATX-TBL-001-GB-01-105 -- ace_information_gained\nATX-TBL-001-GB-01-105 -- concordance_flag\nATX-TBL-001-GB-01-105 -- pass_vafqc_flag\nATX-TBL-001-GB-01-105 -- pass_stripy_flag\n</code></pre>"},{"location":"basic_training/channels.pt/#armazenamento-em-modulos-de-analisadores-sintaticos","title":"Armazenamento em m\u00f3dulos de analisadores sint\u00e1ticos","text":"<p>A melhor maneira de armazenar scripts com analisadores \u00e9 mant\u00ea-los em um arquivo de m\u00f3dulo Nextflow.</p> <p>Digamos que n\u00e3o temos um operador de canal JSON, mas criamos uma fun\u00e7\u00e3o para isso. O arquivo <code>parsers.nf</code> deve conter a fun\u00e7\u00e3o <code>parseArquivoJson</code>. Veja o conte\u00fado abaixo:</p> C\u00f3digo-fonte./modules/parsers.nfSa\u00edda <pre><code>include { parseArquivoJson } from './modules/parsers.nf'\nprocess FOO {\ninput:\ntuple val(id_paciente), val(caracteristica)\noutput:\nstdout\nscript:\n\"\"\"\n    echo $id_paciente tem $caracteristica como coluna\n    \"\"\"\n}\nworkflow {\nChannel\n.fromPath('data/meta/regions*.json')\n| flatMap { parseArquivoJson(it) }\n| map { registro -&gt; [registro.patient_id, registro.feature] }\n| unique\n| FOO\n| view\n}\n</code></pre> <pre><code>import groovy.json.JsonSlurper\ndef parseArquivoJson(arquivo_json) {\ndef f = file(arquivo_json)\ndef registros = new JsonSlurper().parse(f)\nreturn registros\n}\n</code></pre> <pre><code>ATX-TBL-001-GB-01-105 tem pass_stripy_flag como coluna\nATX-TBL-001-GB-01-105 tem ace_information_gained como coluna\nATX-TBL-001-GB-01-105 tem concordance_flag como coluna\nATX-TBL-001-GB-01-105 tem pass_vafqc_flag como coluna\nATX-TBL-001-GB-01-105 tem pass_manual_flag como coluna\nATX-TBL-001-GB-01-105 tem other_region_selection_flag como coluna\n</code></pre> <p>O Nextflow usar\u00e1 isso como uma fun\u00e7\u00e3o personalizada dentro do escopo <code>workflow</code>.</p> <p>Tip</p> <p>Voc\u00ea aprender\u00e1 mais sobre arquivos de m\u00f3dulo posteriormente na se\u00e7\u00e3o de Modulariza\u00e7\u00e3o desse tutorial.</p>"},{"location":"basic_training/config/","title":"Nextflow configuration","text":"<p>A key Nextflow feature is the ability to decouple the workflow implementation by the configuration setting required by the underlying execution platform.</p> <p>This enables portable deployment without the need to modify the application code.</p>"},{"location":"basic_training/config/#configuration-file","title":"Configuration file","text":"<p>When a workflow script is launched, Nextflow looks for a file named <code>nextflow.config</code> in the current directory and in the script base directory (if it is not the same as the current directory). Finally, it checks for the file: <code>$HOME/.nextflow/config</code>.</p> <p>When more than one of the above files exists, they are merged, so that the settings in the first override the same settings that may appear in the second, and so on.</p> <p>The default config file search mechanism can be extended by providing an extra configuration file by using the command line option: <code>-c &lt;config file&gt;</code>.</p>"},{"location":"basic_training/config/#config-syntax","title":"Config syntax","text":"<p>A Nextflow configuration file is a simple text file containing a set of properties defined using the syntax:</p> <pre><code>name = value\n</code></pre> <p>Info</p> <p>Please note that string values need to be wrapped in quotation characters while numbers and boolean values (<code>true</code>, <code>false</code>) do not. Also, note that values are typed, meaning for example that, <code>1</code> is different from <code>'1'</code>, since the first is interpreted as the number one, while the latter is interpreted as a string value.</p>"},{"location":"basic_training/config/#config-variables","title":"Config variables","text":"<p>Configuration properties can be used as variables in the configuration file itself, by using the usual <code>$propertyName</code> or <code>${expression}</code> syntax.</p> <pre><code>propertyOne = 'world'\nanotherProp = \"Hello $propertyOne\"\ncustomPath = \"$PATH:/my/app/folder\"\n</code></pre> <p>Tip</p> <p>In the configuration file it\u2019s possible to access any variable defined in the host environment such as <code>$PATH</code>, <code>$HOME</code>, <code>$PWD</code>, etc.</p>"},{"location":"basic_training/config/#config-comments","title":"Config comments","text":"<p>Configuration files use the same conventions for comments used in the Nextflow script:</p> <pre><code>// comment a single line\n/*\n   a comment spanning\n   multiple lines\n */\n</code></pre>"},{"location":"basic_training/config/#config-scopes","title":"Config scopes","text":"<p>Configuration settings can be organized in different scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. This is shown in the following example:</p> <pre><code>alpha.x  = 1\nalpha.y  = 'string value..'\nbeta {\np = 2\nq = 'another string ..'\n}\n</code></pre>"},{"location":"basic_training/config/#config-params","title":"Config params","text":"<p>The scope <code>params</code> allows the definition of workflow parameters that override the values defined in the main workflow script.</p> <p>This is useful to consolidate one or more execution parameters in a separate file.</p> Config file<pre><code>params.foo = 'Bonjour'\nparams.bar = 'le monde!'\n</code></pre> Workflow script<pre><code>params.foo = 'Hello'\nparams.bar = 'world!'\n// print both params\nprintln \"$params.foo $params.bar\"\n</code></pre> <p>Exercise</p> <p>Save the first snippet above (Config file) as <code>nextflow.config</code> and the second one as <code>params.nf</code>. Then run:</p> <pre><code>nextflow run params.nf\n</code></pre> Solution <pre><code>Bonjour le monde!\n</code></pre> <p>Execute is again specifying the <code>foo</code> parameter on the command line:</p> <pre><code>nextflow run params.nf --foo Hola\n</code></pre> Solution <pre><code>Hola le monde!\n</code></pre> <p>Compare the result of the two executions.</p>"},{"location":"basic_training/config/#config-env","title":"Config env","text":"<p>The <code>env</code> scope allows the definition of one or more variables that will be exported into the environment where the workflow tasks will be executed.</p> <pre><code>env.ALPHA = 'some value'\nenv.BETA = \"$HOME/some/path\"\n</code></pre> <p>Save the above snippet as a file named <code>my-env.config</code>. Then save the snippet below in a file named <code>foo.nf</code>:</p> <pre><code>process FOO {\ndebug true\nscript:\n'''\n    env | egrep 'ALPHA|BETA'\n    '''\n}\nworkflow {\nFOO()\n}\n</code></pre> <p>Finally, execute the following command:</p> <pre><code>nextflow run foo.nf -c my-env.config\n</code></pre> Solution <pre><code>BETA=/home/user/some/path\nALPHA=some value\n</code></pre>"},{"location":"basic_training/config/#config-process","title":"Config process","text":"<p>Process directives allow the specification of settings for the task execution such as <code>cpus</code>, <code>memory</code>, <code>container</code>, and other resources in the workflow script.</p> <p>This is useful when prototyping a small workflow script.</p> <p>However, it\u2019s always a good practice to decouple the workflow execution logic from the process configuration settings, i.e. it\u2019s strongly suggested to define the process settings in the workflow configuration file instead of the workflow script.</p> <p>The <code>process</code> configuration scope allows the setting of any <code>process</code> directives in the Nextflow configuration file. For example:</p> <pre><code>process {\ncpus = 10\nmemory = 8.GB\ncontainer = 'biocontainers/bamtools:v2.4.0_cv3'\n}\n</code></pre> <p>The above config snippet defines the <code>cpus</code>, <code>memory</code> and <code>container</code> directives for all processes in your workflow script.</p> <p>The process selector can be used to apply the configuration to a specific process or group of processes (discussed later).</p> <p>Info</p> <p>Memory and time duration units can be specified either using a string-based notation in which the digit(s) and the unit can be separated by a blank or by using the numeric notation in which the digit(s) and the unit are separated by a dot character and are not enclosed by quote characters.</p> String syntax Numeric syntax Value <code>'10 KB'</code> <code>10.KB</code> 10240 bytes <code>'500 MB'</code> <code>500.MB</code> 524288000 bytes <code>'1 min'</code> <code>1.min</code> 60 seconds <code>'1 hour 25 sec'</code> - 1 hour and 25 seconds <p>The syntax for setting <code>process</code> directives in the configuration file requires <code>=</code> (i.e. assignment operator), whereas it should not be used when setting the process directives within the workflow script.</p> Example <pre><code>process FOO {\ncpus 4\nmemory 2.GB\ntime 1.hour\nmaxRetries 3\nscript:\n\"\"\"\n    your_command --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <p>This is especially important when you want to define a config setting using a dynamic expression using a closure. For example, in a workflow script:</p> <pre><code>process FOO {\nmemory { 4.GB * task.cpus }\n}\n</code></pre> <p>And the equivalent in the configuration file, if you choose to set it there:</p> <pre><code>process {\nwithName: FOO {\nmemory = { 4.GB * task.cpus }\n}\n}\n</code></pre> <p>Directives that require more than one value, e.g. pod, in the configuration file need to be expressed as a map object.</p> <pre><code>process {\npod = [env: 'FOO', value: '123']\n}\n</code></pre> <p>Finally, directives that are to be repeated in the process definition, in the configuration files need to be defined as a list object. For example:</p> <pre><code>process {\npod = [[env: 'FOO', value: '123'],\n[env: 'BAR', value: '456']]\n}\n</code></pre>"},{"location":"basic_training/config/#config-docker-execution","title":"Config Docker execution","text":"<p>The container image to be used for the process execution can be specified in the <code>nextflow.config</code> file:</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\ndocker.enabled = true\n</code></pre> <p>The use of unique \"SHA256\" Docker image IDs guarantees that the image content does not change over time, for example:</p> <pre><code>process.container = 'nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266'\ndocker.enabled = true\n</code></pre>"},{"location":"basic_training/config/#config-singularity-execution","title":"Config Singularity execution","text":"<p>To run a workflow execution with Singularity, a container image file path is required in the Nextflow config file using the container directive:</p> <pre><code>process.container = '/some/singularity/image.sif'\nsingularity.enabled = true\n</code></pre> <p>Info</p> <p>The container image file must be an absolute path: it must start with a <code>/</code>.</p> <p>The following protocols are supported:</p> <ul> <li><code>library://</code> download the container image from the Singularity Library service.</li> <li><code>shub://</code> download the container image from the Singularity Hub.</li> <li><code>docker://</code> download the container image from the Docker Hub and convert it to the Singularity format.</li> <li><code>docker-daemon://</code> pull the container image from a local Docker installation and convert it to a Singularity image file.</li> </ul> <p>Warning</p> <p>Singularity hub <code>shub://</code> is no longer available as a builder service. Though existing images from before 19th April 2021 will still work.</p> <p>Tip</p> <p>By specifying a plain Docker container image name, Nextflow implicitly downloads and converts it to a Singularity image when the Singularity execution is enabled.</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\nsingularity.enabled = true\n</code></pre> <p>The above configuration instructs Nextflow to use the Singularity engine to run your script processes. The container is pulled from the Docker registry and cached in the current directory to be used for further runs.</p> <p>Alternatively, if you have a Singularity image file, its absolute path location can be specified as the container name either using the <code>-with-singularity</code> option or the <code>process.container</code> setting in the config file.</p> <p>Exercise</p> <p>Try to run the script as shown below, changing the <code>nextflow.config</code> file to the one above using <code>singularity</code>:</p> <pre><code>nextflow run script7.nf\n</code></pre> <p>Note</p> <p>Nextflow will pull the container image automatically, it will require a few seconds depending on the network connection speed.</p>"},{"location":"basic_training/config/#config-conda-execution","title":"Config Conda execution","text":"<p>The use of a Conda environment can also be provided in the configuration file by adding the following setting in the <code>nextflow.config</code> file:</p> <pre><code>process.conda = \"/home/ubuntu/miniconda2/envs/nf-tutorial\"\n</code></pre> <p>You can specify the path of an existing Conda environment as either directory or the path of Conda environment YAML file.</p>"},{"location":"basic_training/config.pt/","title":"Configura\u00e7\u00e3o do Nextflow","text":"<p>Um recurso importante do Nextflow \u00e9 a capacidade de desacoplar a implementa\u00e7\u00e3o do fluxo de trabalho e as configura\u00e7\u00f5es exigidas pela plataforma onde ele ser\u00e1 executado.</p> <p>Isso permite a portabilidade da aplica\u00e7\u00e3o sem a necessidade de modificar seu c\u00f3digo quando se faz necess\u00e1ria a execu\u00e7\u00e3o em diferentes ambientes.</p>"},{"location":"basic_training/config.pt/#arquivo-de-configuracao","title":"Arquivo de configura\u00e7\u00e3o","text":"<p>Quando um script de fluxo de trabalho \u00e9 executado, o Nextflow procura um arquivo chamado <code>nextflow.config</code> no diret\u00f3rio atual e no diret\u00f3rio base do script (se n\u00e3o for o mesmo que o diret\u00f3rio atual). Caso n\u00e3o encontre, o Nextflow verifica o arquivo: <code>$HOME/.nextflow/config</code>.</p> <p>Quando existir mais de um dos arquivos acima, eles ser\u00e3o mesclados, de modo que as configura\u00e7\u00f5es do primeiro substituam as mesmas configura\u00e7\u00f5es que podem aparecer no segundo e assim por diante.</p> <p>O mecanismo de pesquisa padr\u00e3o do arquivo de configura\u00e7\u00e3o pode ser estendido fornecendo um arquivo de configura\u00e7\u00e3o extra usando a op\u00e7\u00e3o de linha de comando: <code>-c &lt;arquivo de configura\u00e7\u00e3o&gt;</code>.</p>"},{"location":"basic_training/config.pt/#sintaxe-do-arquivo-de-configuracao","title":"Sintaxe do arquivo de configura\u00e7\u00e3o","text":"<p>Um arquivo de configura\u00e7\u00e3o do Nextflow \u00e9 um arquivo de texto simples contendo um conjunto de propriedades definidas usando a sintaxe:</p> <pre><code>nome = valor\n</code></pre> <p>Info</p> <p>Observe que strings precisam ser colocadas entre aspas, enquanto n\u00fameros e valores booleanos (<code>true</code>, <code>false</code>) n\u00e3o. Al\u00e9m disso, observe que os valores s\u00e3o tipados, o que significa, por exemplo, que <code>1</code> \u00e9 diferente de <code>'1'</code>, pois o primeiro \u00e9 interpretado como o n\u00famero um, enquanto o \u00faltimo \u00e9 interpretado como uma string.</p>"},{"location":"basic_training/config.pt/#variaveis-de-configuracao","title":"Vari\u00e1veis de configura\u00e7\u00e3o","text":"<p>As propriedades de configura\u00e7\u00e3o podem ser usadas como vari\u00e1veis no pr\u00f3prio arquivo de configura\u00e7\u00e3o, usando a sintaxe usual <code>$nomePropriedade</code> ou <code>${expressao}</code>.</p> <pre><code>propriedadeUm = 'mundo'\numaOutraPropriedade = \"Ol\u00e1 $propriedadeUm\"\ncaminhoCustomizado = \"$PATH:/pasta/da/minha/app\"\n</code></pre> <p>Tip</p> <p>No arquivo de configura\u00e7\u00e3o \u00e9 poss\u00edvel acessar qualquer vari\u00e1vel definida no ambiente de execu\u00e7\u00e3o, como <code>$PATH</code>, <code>$HOME</code>, <code>$PWD</code>, etc.</p>"},{"location":"basic_training/config.pt/#comentarios-no-arquivo-de-configuracao","title":"Coment\u00e1rios no arquivo de configura\u00e7\u00e3o","text":"<p>Os arquivos de configura\u00e7\u00e3o usam as mesmas conven\u00e7\u00f5es para coment\u00e1rios usados no script Nextflow:</p> <pre><code>// comentar uma \u00fanica linha\n/*\n    um coment\u00e1rio abrangendo\n    v\u00e1rias linhas\n  */\n</code></pre>"},{"location":"basic_training/config.pt/#escopos-de-configuracao","title":"Escopos de configura\u00e7\u00e3o","text":"<p>As defini\u00e7\u00f5es de configura\u00e7\u00e3o podem ser organizadas em diferentes escopos. Pode-se utilizar a nota\u00e7\u00e3o <code>escopo.propriedade</code> ou agrupar as propriedades no mesmo escopo usando a nota\u00e7\u00e3o de chaves, como mostrado a seguir:</p> <pre><code>alfa.x = 1\nalfa.y = 'valor da string..'\nbeta {\np = 2\nq = 'outra string ..'\n}\n</code></pre>"},{"location":"basic_training/config.pt/#configurar-parametros","title":"Configurar par\u00e2metros","text":"<p>O escopo <code>params</code> permite a defini\u00e7\u00e3o de par\u00e2metros que substituem os valores definidos no script principal do fluxo de trabalho.</p> <p>Isso \u00e9 \u00fatil para refor\u00e7ar o uso de um ou mais par\u00e2metros de execu\u00e7\u00e3o em um arquivo separado.</p> Arquivo de configura\u00e7\u00e3o<pre><code>params.foo = 'Bonjour'\nparams.bar = 'le monde!'\n</code></pre> Script do fluxo de trabalho<pre><code>params.foo = 'Ol\u00e1'\nparams.bar = 'mundo!'\n// imprime ambos os par\u00e2metros\nprintln \"$params.foo $params.bar\"\n</code></pre> <p>Exercise</p> <p>Salve o primeiro trecho como <code>nextflow.config</code> e o segundo como <code>params.nf</code>. Em seguida, execute:</p> <pre><code>nextflow run params.nf\n</code></pre> Solution <pre><code>Bonjour le monde!\n</code></pre> <p>Execute novamente o comando anterior especificando o par\u00e2metro <code>foo</code> na linha de comando:</p> <pre><code>nextflow run params.nf --foo Ol\u00e1\n</code></pre> Solution <pre><code>Ol\u00e1 le monde!\n</code></pre> <p>Compare o resultado das duas execu\u00e7\u00f5es.</p>"},{"location":"basic_training/config.pt/#configurar-ambiente","title":"Configurar ambiente","text":"<p>O escopo <code>env</code> permite a defini\u00e7\u00e3o de uma ou mais vari\u00e1veis que ser\u00e3o exportadas para o ambiente onde ser\u00e3o executadas as tarefas do fluxo de trabalho.</p> <pre><code>env.ALFA = 'algum valor'\nenv.BETA = \"$HOME/algum/caminho\"\n</code></pre> <p>Salve o trecho acima como um arquivo chamado <code>meu-env.config</code>. Em seguida, salve o trecho abaixo em um arquivo chamado <code>foo.nf</code>:</p> <pre><code>process FOO {\ndebug true\nscript:\n'''\n    env | egrep 'ALFA|BETA'\n    '''\n}\nworkflow {\nFOO()\n}\n</code></pre> <p>Por fim, execute o seguinte comando:</p> <pre><code>nextflow run foo.nf -c meu-env.config\n</code></pre> Solution <pre><code>BETA=/home/usuario/algum/caminho\nALFA=algum valor\n</code></pre>"},{"location":"basic_training/config.pt/#configurar-processos","title":"Configurar processos","text":"<p>As diretivas de processos permite a especifica\u00e7\u00e3o de configura\u00e7\u00f5es para a execu\u00e7\u00e3o de uma tarefa, como <code>cpus</code>, <code>memory</code>, <code>container</code>, al\u00e9m de outros recursos, no script do fluxo de trabalho.</p> <p>Isso \u00e9 \u00fatil ao criarmos um exemplo-teste ou um prot\u00f3tipo para o nosso fluxo de trabalho.</p> <p>No entanto, \u00e9 sempre uma boa pr\u00e1tica desassociar a l\u00f3gica de execu\u00e7\u00e3o do fluxo de trabalho das configura\u00e7\u00f5es que ser\u00e3o utilizadas por ele. Assim, \u00e9 altamente recomend\u00e1vel definir as configura\u00e7\u00f5es do processo no arquivo de configura\u00e7\u00e3o do fluxo de trabalho em vez de no script do fluxo de trabalho.</p> <p>Al\u00e9m disso, quaisquer diretivas do escopo dos processos (<code>process</code>) podem ser usadas no arquivo de configura\u00e7\u00e3o.</p> <pre><code>process {\ncpus = 10\nmemory = 8.GB\ncontainer = 'biocontainers/bamtools:v2.4.0_cv3'\n}\n</code></pre> <p>O trecho de configura\u00e7\u00e3o acima define as diretivas <code>cpus</code>, <code>memory</code> e <code>container</code> para todos os processos em seu fluxo de trabalho.</p> <p>O seletor de processo pode ser usado para aplicar a configura\u00e7\u00e3o a um processo espec\u00edfico ou grupo de processos (discutido posteriormente).</p> <p>Info</p> <p>As unidades de mem\u00f3ria e a dura\u00e7\u00e3o de tempo podem ser especificadas usando uma nota\u00e7\u00e3o baseada em string na qual o(s) d\u00edgito(s) e a unidade podem ser separados por um espa\u00e7o em branco. Tamb\u00e9m pode-se usar a nota\u00e7\u00e3o num\u00e9rica na qual o(s) d\u00edgito(s) e a(s) unidade(s) s\u00e3o separados por um ponto e n\u00e3o s\u00e3o colocados entre aspas.</p> Sintaxe de string Sintaxe num\u00e9rica Valor <code>'10 KB'</code> <code>10.KB</code> 10240 bytes <code>'500 MB'</code> <code>500.MB</code> 524288000 bytes <code>'1 min'</code> <code>1.min</code> 60 segundos <code>'1 hour 25 sec'</code> - 1 hora e 25 segundos <p>A sintaxe para definir as diretivas de processo no arquivo de configura\u00e7\u00e3o requer <code>=</code> (ou seja, operador de atribui\u00e7\u00e3o). Esta nota\u00e7\u00e3o n\u00e3o deve ser usada para definir as diretivas do processo no script do fluxo de trabalho.</p> Example <pre><code>process FOO {\ncpus 4\nmemory 2.GB\ntime 1.hour\nmaxRetries 3\nscript:\n\"\"\"\n    seu_comando --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <p>Isso \u00e9 especialmente importante quando voc\u00ea deseja criar uma defini\u00e7\u00e3o de configura\u00e7\u00e3o usando uma express\u00e3o din\u00e2mica com uma clausura. Por exemplo, em um arquivo de fluxo de trabalho:</p> <pre><code>process FOO {\nmemory { 4.GB * task.cpus }\n}\n</code></pre> <p>E o equivalente em um arquivo de configura\u00e7\u00e3o, se voc\u00ea preferir configurar isso l\u00e1:</p> <pre><code>process {\nwithName: FOO {\nmemory = { 4.GB * task.cpus }\n}\n}\n</code></pre> <p>Diretivas que exigem mais de um valor, como a diretiva pod, precisam ser expressas como um objeto Map no arquivo de configura\u00e7\u00e3o.</p> <pre><code>process {\npod = [ambiente: 'FOO', valor: '123']\n}\n</code></pre> <p>Por fim, as diretivas que devem ser repetidas na defini\u00e7\u00e3o do processo e nos arquivos de configura\u00e7\u00e3o precisam ser definidas como um objeto List. Por exemplo:</p> <pre><code>process {\npod = [[ambiente: 'FOO', valor: '123'],\n[ambiente: 'BAR', valor: '456']]\n}\n</code></pre>"},{"location":"basic_training/config.pt/#configurar-execucao-do-docker","title":"Configurar execu\u00e7\u00e3o do Docker","text":"<p>A imagem do cont\u00eainer a ser utilizada para a execu\u00e7\u00e3o do processo pode ser especificada no arquivo <code>nextflow.config</code>:</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\ndocker.enabled = true\n</code></pre> <p>O uso de IDs \u00fanicos com \"SHA256\" para imagens Docker garante que o conte\u00fado da imagem n\u00e3o mude com o tempo, por exemplo:</p> <pre><code>process.container = 'nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266'\ndocker.enabled = true\n</code></pre>"},{"location":"basic_training/config.pt/#configurar-execucao-do-singularity","title":"Configurar execu\u00e7\u00e3o do Singularity","text":"<p>Para rodar um fluxo de trabalho com Singularity, \u00e9 necess\u00e1rio fornecer o caminho para o arquivo de imagem do cont\u00eainer usando a diretiva de cont\u00eainer (<code>container</code>):</p> <pre><code>process.container = '/alguma/imagem/singularity/imagem.sif'\nsingularity.enabled = true\n</code></pre> <p>Info</p> <p>O arquivo de imagem do cont\u00eainer deve ser um caminho absoluto: deve come\u00e7ar com <code>/</code>.</p> <p>Os seguintes protocolos s\u00e3o suportados:</p> <ul> <li><code>library://</code> baixe a imagem do cont\u00eainer do Singularity Library service.</li> <li><code>shub://</code> baixe a imagem do cont\u00eainer do Singularity Hub.</li> <li><code>docker://</code> baixe a imagem do cont\u00eainer do Docker Hub e a converta para o formato Singularity.</li> <li><code>docker-daemon://</code> extraia a imagem do cont\u00eainer de uma instala\u00e7\u00e3o local do Docker e a converta em um arquivo de imagem Singularity.</li> </ul> <p>Tip</p> <p>O hub do Singularity <code>shub://</code> n\u00e3o est\u00e1 mais dispon\u00edvel como servi\u00e7o de constru\u00e7\u00e3o de cont\u00eaineres. Entretanto, as imagens existentes anteriores a 19 de abril de 2021 ainda funcionam.</p> <p>Tip</p> <p>Ao especificar um nome simples de imagem de cont\u00eainer do Docker, o Nextflow faz o download e a converte implicitamente em uma imagem do Singularity quando a execu\u00e7\u00e3o do Singularity est\u00e1 habilitada.</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\nsingularity.enabled = true\n</code></pre> <p>A configura\u00e7\u00e3o acima instrui o Nextflow a usar o mecanismo Singularity para executar seus processos. O cont\u00eainer \u00e9 extra\u00eddo do reposit\u00f3rio de imagens do Docker e armazenado em cache no diret\u00f3rio atual para ser usado em outras execu\u00e7\u00f5es.</p> <p>Como alternativa, se voc\u00ea tiver uma imagem Singularity, seu caminho absoluto pode ser especificado por meio do nome do cont\u00eainer usando a op\u00e7\u00e3o <code>-with-singularity</code> ou a configura\u00e7\u00e3o <code>process.container</code> no arquivo de configura\u00e7\u00e3o.</p> <p>Exercise</p> <p>Tente executar o script conforme mostrado abaixo, alterando o arquivo <code>nextflow.config</code> para ser usado com o Singularity:</p> <pre><code>nextflow run script7.nf\n</code></pre> <p>Note</p> <p>O Nextflow ir\u00e1 baixar a imagem do cont\u00eainer automaticamente. Isto levar\u00e1 alguns segundos, dependendo da velocidade da sua conex\u00e3o.</p>"},{"location":"basic_training/config.pt/#configurar-execucao-com-ambientes-conda","title":"Configurar execu\u00e7\u00e3o com ambientes Conda","text":"<p>Ambientes Conda tamb\u00e9m podem ser fornecidos no arquivo de configura\u00e7\u00e3o. Basta adicionar a seguinte configura\u00e7\u00e3o no arquivo <code>nextflow.config</code>:</p> <pre><code>process.conda = \"/home/ubuntu/miniconda2/envs/nf-tutorial\"\n</code></pre> <p>Voc\u00ea pode especificar o caminho de um ambiente Conda existente como diret\u00f3rio ou o caminho do arquivo YAML do ambiente Conda.</p>"},{"location":"basic_training/containers/","title":"Manage dependencies and containers","text":"<p>Computational workflows are rarely composed of a single script or tool. More often, they depend on dozens of software components or libraries.</p> <p>Installing and maintaining such dependencies is a challenging task and the most common source of irreproducibility in scientific applications.</p> <p>To overcome these issues, we use containers that enable software dependencies, i.e. tools and libraries required by a data analysis application, to be encapsulated in one or more self-contained, ready-to-run, immutable Linux container images, that can be easily deployed in any platform that supports the container runtime.</p> <p>Containers can be executed in an isolated manner from the hosting system. Having its own copy of the file system, processing space, memory management, etc.</p> <p>Info</p> <p>Containers were first introduced with kernel 2.6 as a Linux feature known as Control Groups or Cgroups.</p>"},{"location":"basic_training/containers/#docker","title":"Docker","text":"<p>Docker is a handy management tool to build, run and share container images.</p> <p>These images can be uploaded and published in a centralized repository known as Docker Hub, or hosted by other parties like Quay.</p>"},{"location":"basic_training/containers/#run-a-container","title":"Run a container","text":"<p>A container can be run using the following command:</p> <pre><code>docker run &lt;container-name&gt;\n</code></pre> <p>Try for example the following publicly available container (if you have Docker installed):</p> <pre><code>docker run hello-world\n</code></pre>"},{"location":"basic_training/containers/#pull-a-container","title":"Pull a container","text":"<p>The pull command allows you to download a Docker image without running it. For example:</p> <pre><code>docker pull debian:bullseye-slim\n</code></pre> <p>The above command downloads a Debian Linux image. You can check it exists by using:</p> <pre><code>docker images\n</code></pre>"},{"location":"basic_training/containers/#run-a-container-in-interactive-mode","title":"Run a container in interactive mode","text":"<p>Launching a BASH shell in the container allows you to operate in an interactive mode in the containerized operating system. For example:</p> <pre><code>docker run -it debian:bullseye-slim bash\n</code></pre> <p>Once launched, you will notice that it is running as root (!). Use the usual commands to navigate the file system. This is useful to check if the expected programs are present within a container.</p> <p>To exit from the container, stop the BASH session with the <code>exit</code> command.</p>"},{"location":"basic_training/containers/#your-first-dockerfile","title":"Your first Dockerfile","text":"<p>Docker images are created by using a so-called <code>Dockerfile</code>, which is a simple text file containing a list of commands to assemble and configure the image with the software packages required.</p> <p>Here, you will create a Docker image containing cowsay and the Salmon tool.</p> <p>Warning</p> <p>The Docker build process automatically copies all files that are located in the current directory to the Docker daemon in order to create the image. This can take a lot of time when big/many files exist. For this reason, it\u2019s important to always work in a directory containing only the files you really need to include in your Docker image. Alternatively, you can use the <code>.dockerignore</code> file to select paths to exclude from the build.</p> <p>Use your favorite editor (e.g., <code>vim</code> or <code>nano</code>) to create a file named <code>Dockerfile</code> and copy the following content:</p> <pre><code>FROM debian:bullseye-slim\nLABEL image.author.name \"Your Name Here\"\nLABEL image.author.email \"your@email.here\"\nRUN apt-get update &amp;&amp; apt-get install -y curl cowsay\n\nENV PATH=$PATH:/usr/games/\n</code></pre>"},{"location":"basic_training/containers/#build-the-image","title":"Build the image","text":"<p>Build the Docker image based on the Dockerfile by using the following command:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>Where \"my-image\" is the user-specified name for the container image you plan to build .</p> <p>Tip</p> <p>Don\u2019t miss the dot in the above command.</p> <p>When it completes, verify that the image has been created by listing all available images:</p> <pre><code>docker images\n</code></pre> <p>You can try your new container by running this command:</p> <pre><code>docker run my-image cowsay Hello Docker!\n</code></pre>"},{"location":"basic_training/containers/#add-a-software-package-to-the-image","title":"Add a software package to the image","text":"<p>Add the Salmon package to the Docker image by adding the following snippet to the <code>Dockerfile</code>:</p> <pre><code>RUN curl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Save the file and build the image again with the same command as before:</p> <pre><code>docker build -t my-image .\n</code></pre> <p>You will notice that it creates a new Docker image with the same name but with a different image ID.</p>"},{"location":"basic_training/containers/#run-salmon-in-the-container","title":"Run Salmon in the container","text":"<p>Check that Salmon is running correctly in the container as shown below:</p> <pre><code>docker run my-image salmon --version\n</code></pre> <p>You can even launch a container in an interactive mode by using the following command:</p> <pre><code>docker run -it my-image bash\n</code></pre> <p>Use the <code>exit</code> command to terminate the interactive session.</p>"},{"location":"basic_training/containers/#file-system-mounts","title":"File system mounts","text":"<p>Create a genome index file by running Salmon in the container.</p> <p>Try to run Salmon in the container with the following command:</p> <pre><code>docker run my-image \\\nsalmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>The above command fails because Salmon cannot access the input file.</p> <p>This happens because the container runs in a completely separate file system and it cannot access the hosting file system by default.</p> <p>You will need to use the <code>--volume</code> command-line option to mount the input file(s) e.g.</p> <pre><code>docker run --volume $PWD/data/ggal/transcriptome.fa:/transcriptome.fa my-image \\\nsalmon index -t /transcriptome.fa -i transcript-index\n</code></pre> <p>Warning</p> <p>The generated <code>transcript-index</code> directory is still not accessible in the host file system.</p> <p>An easier way is to mount a parent directory to an identical one in the container, this allows you to use the same path when running it in the container e.g.</p> <pre><code>docker run --volume $PWD:$PWD --workdir $PWD my-image \\\nsalmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Or set a folder you want to mount as an environmental variable, called <code>DATA</code>:</p> <pre><code>DATA=/workspace/gitpod/nf-training/data\ndocker run --volume $DATA:$DATA --workdir $PWD my-image \\\nsalmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Now check the content of the <code>transcript-index</code> folder by entering the command:</p> <pre><code>ls -la transcript-index\n</code></pre> <p>Note</p> <p>Note that the permissions for files created by the Docker execution is <code>root</code>.</p>"},{"location":"basic_training/containers/#upload-the-container-in-the-docker-hub-bonus","title":"Upload the container in the Docker Hub (bonus)","text":"<p>Publish your container in the Docker Hub to share it with other people.</p> <p>Create an account on the https://hub.docker.com website. Then from your shell terminal run the following command, entering the user name and password you specified when registering in the Hub:</p> <pre><code>docker login\n</code></pre> <p>Rename the image to include your Docker user name account:</p> <pre><code>docker tag my-image &lt;user-name&gt;/my-image\n</code></pre> <p>Finally push it to the Docker Hub:</p> <pre><code>docker push &lt;user-name&gt;/my-image\n</code></pre> <p>After that anyone will be able to download it by using the command:</p> <pre><code>docker pull &lt;user-name&gt;/my-image\n</code></pre> <p>Note how after a pull and push operation, Docker prints the container digest number e.g.</p> <pre><code>Digest: sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\nStatus: Downloaded newer image for nextflow/rnaseq-nf:latest\n</code></pre> <p>This is a unique and immutable identifier that can be used to reference a container image in a univocally manner. For example:</p> <pre><code>docker pull nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\n</code></pre>"},{"location":"basic_training/containers/#run-a-nextflow-script-using-a-docker-container","title":"Run a Nextflow script using a Docker container","text":"<p>The simplest way to run a Nextflow script with a Docker image is using the <code>-with-docker</code> command-line option:</p> <pre><code>nextflow run script2.nf -with-docker my-image\n</code></pre> <p>As seen in the last section, you can also configure the Nextflow config file (<code>nextflow.config</code>) to select which container to use instead of having to specify it as a command-line argument every time.</p>"},{"location":"basic_training/containers/#singularity","title":"Singularity","text":"<p>Singularity is a container runtime designed to work in high-performance computing data centers, where the usage of Docker is generally not allowed due to security constraints.</p> <p>Singularity implements a container execution model similar to Docker. However, it uses a completely different implementation design.</p> <p>A Singularity container image is archived as a plain file that can be stored in a shared file system and accessed by many computing nodes managed using a batch scheduler.</p> <p>Warning</p> <p>Singularity will not work with Gitpod. If you wish to try this section, please do it locally, or on an HPC.</p>"},{"location":"basic_training/containers/#create-a-singularity-images","title":"Create a Singularity images","text":"<p>Singularity images are created using a <code>Singularity</code> file in a similar manner to Docker but using a different syntax.</p> <pre><code>Bootstrap: docker\nFrom: debian:bullseye-slim\n\n%environment\nexport PATH=$PATH:/usr/games/\n\n%labels\nAUTHOR &lt;your name&gt;\n\n%post\napt-get update &amp;&amp; apt-get install -y locales-all curl cowsay\ncurl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Once you have saved the <code>Singularity</code> file, you can create the image with these commands:</p> <pre><code>sudo singularity build my-image.sif Singularity\n</code></pre> <p>Note: the <code>build</code> command requires <code>sudo</code> permissions. A common workaround consists of building the image on a local workstation and then deploying it in the cluster by copying the image file.</p>"},{"location":"basic_training/containers/#running-a-container","title":"Running a container","text":"<p>Once done, you can run your container with the following command</p> <pre><code>singularity exec my-image.sif cowsay 'Hello Singularity'\n</code></pre> <p>By using the <code>shell</code> command you can enter in the container in interactive mode. For example:</p> <pre><code>singularity shell my-image.sif\n</code></pre> <p>Once in the container instance run the following commands:</p> <pre><code>touch hello.txt\nls -la\n</code></pre> <p>Info</p> <p>Note how the files on the host environment are shown. Singularity automatically mounts the host <code>$HOME</code> directory and uses the current work directory.</p>"},{"location":"basic_training/containers/#import-a-docker-image","title":"Import a Docker image","text":"<p>An easier way to create a Singularity container without requiring <code>sudo</code> permission and boosting the containers interoperability is to import a Docker container image by pulling it directly from a Docker registry. For example:</p> <pre><code>singularity pull docker://debian:bullseye-slim\n</code></pre> <p>The above command automatically downloads the Debian Docker image and converts it to a Singularity image in the current directory with the name <code>debian-jessie.simg</code>.</p>"},{"location":"basic_training/containers/#run-a-nextflow-script-using-a-singularity-container","title":"Run a Nextflow script using a Singularity container","text":"<p>Nextflow allows the transparent usage of Singularity containers as easy as with Docker.</p> <p>Simply enable the use of the Singularity engine in place of Docker in the Nextflow command line by using the <code>-with-singularity</code> command-line option:</p> <pre><code>nextflow run script7.nf -with-singularity nextflow/rnaseq-nf\n</code></pre> <p>As before, the Singularity container can also be provided in the Nextflow config file. We\u2019ll see how to do this later.</p>"},{"location":"basic_training/containers/#the-singularity-container-library","title":"The Singularity Container Library","text":"<p>The authors of Singularity, SyLabs have their own repository of Singularity containers.</p> <p>In the same way that we can push Docker images to Docker Hub, we can upload Singularity images to the Singularity Library.</p>"},{"location":"basic_training/containers/#condabioconda-packages","title":"Conda/Bioconda packages","text":"<p>Conda is a popular package and environment manager. The built-in support for Conda allows Nextflow workflows to automatically create and activate the Conda environment(s), given the dependencies specified by each process.</p> <p>In this Gitpod environment, conda is already installed.</p>"},{"location":"basic_training/containers/#using-conda","title":"Using conda","text":"<p>A Conda environment is defined using a YAML file, which lists the required software packages. The first thing you need to do is to initiate conda for shell interaction, and then open a new terminal by running bash.</p> <pre><code>conda init\nbash\n</code></pre> <p>Then write your YAML file (to <code>env.yml</code>). There is already a file named <code>env.yml</code> in the <code>nf-training</code> folder as an example. Its content is shown below.</p> <pre><code>name: nf-tutorial\nchannels:\n- conda-forge\n- defaults\n- bioconda\ndependencies:\n- bioconda::salmon=1.5.1\n- bioconda::fastqc=0.11.9\n- bioconda::multiqc=1.12\n- conda-forge::tbb=2020.2\n</code></pre> <p>Given the recipe file, the environment is created using the command shown below. The <code>conda env create</code> command may take several minutes, as conda tries to resolve dependencies of the desired packages at runtime, and then downloads everything that is required.</p> <pre><code>conda env create --file env.yml\n</code></pre> <p>You can check the environment was created successfully with the command shown below:</p> <pre><code>conda env list\n</code></pre> <p>This should look something like this:</p> <pre><code># conda environments:\n#\nbase                  *  /opt/conda\nnf-tutorial              /opt/conda/envs/nf-tutorial\n</code></pre> <p>To enable the environment, you can use the <code>activate</code> command:</p> <pre><code>conda activate nf-tutorial\n</code></pre> <p>Nextflow is able to manage the activation of a Conda environment when its directory is specified using the <code>-with-conda</code> option (using the same path shown in the <code>list</code> function. For example:</p> <pre><code>nextflow run script7.nf -with-conda /opt/conda/envs/nf-tutorial\n</code></pre> <p>Info</p> <p>When creating a Conda environment with a YAML recipe file, Nextflow automatically downloads the required dependencies, builds the environment and activates it.</p> <p>This makes easier to manage different environments for the processes in the workflow script.</p> <p>See the docs for details.</p>"},{"location":"basic_training/containers/#create-and-use-conda-like-environments-using-micromamba","title":"Create and use conda-like environments using micromamba","text":"<p>Another way to build conda-like environments is through a <code>Dockerfile</code> and <code>micromamba</code>.</p> <p><code>micromamba</code> is a fast and robust package for building small conda-based environments.</p> <p>This saves having to build a conda environment each time you want to use it (as outlined in previous sections).</p> <p>To do this, you simply require a <code>Dockerfile</code> and you use micromamba to install the packages. However, a good practice is to have a YAML recipe file like in the previous section, so we\u2019ll do it here too, using the same <code>env.yml</code> as before.</p> <pre><code>name: nf-tutorial\nchannels:\n- conda-forge\n- defaults\n- bioconda\ndependencies:\n- bioconda::salmon=1.5.1\n- bioconda::fastqc=0.11.9\n- bioconda::multiqc=1.12\n- conda-forge::tbb=2020.2\n</code></pre> <p>Then, we can write our Dockerfile with micromamba installing the packages from this recipe file.</p> <pre><code>FROM mambaorg/micromamba:0.25.1\nLABEL image.author.name \"Your Name Here\"\nLABEL image.author.email \"your@email.here\"\nCOPY --chown=$MAMBA_USER:$MAMBA_USER env.yml /tmp/env.yml\n\nRUN micromamba create -n nf-tutorial\n\nRUN micromamba install -y -n nf-tutorial -f /tmp/env.yml &amp;&amp; \\\nmicromamba clean --all --yes\n\nENV PATH /opt/conda/envs/nf-tutorial/bin:$PATH\n</code></pre> <p>The above <code>Dockerfile</code> takes the parent image mambaorg/micromamba, then installs a <code>conda</code> environment using <code>micromamba</code>, and installs <code>salmon</code>, <code>fastqc</code> and <code>multiqc</code>.</p> <p>Try executing the RNA-Seq workflow from earlier (script7.nf). Start by building your own micromamba <code>Dockerfile</code> (from above), save it to your Docker hub repo, and direct Nextflow to run from this container (changing your <code>nextflow.config</code>).</p> <p>Warning</p> <p>Building a Docker container and pushing to your personal repo can take &gt;10 minutes.</p> For an overview of steps to take, click here: <ol> <li> <p>Make a file called <code>Dockerfile</code> in the current directory (with the code above).</p> </li> <li> <p>Build the image: <code>docker build -t my-image .</code> (don\u2019t forget the .).</p> </li> <li> <p>Publish the Docker image to your online Docker account.</p> <p>Something similar to the following, with <code>&lt;myrepo&gt;</code> replaced with your own Docker ID, without &lt; and &gt; characters!</p> <p><code>my-image</code> could be replaced with any name you choose. As good practice, choose something memorable and ensure the name matches the name you used in the previous command.</p> <pre><code>docker login\ndocker tag my-image &lt;myrepo&gt;/my-image\ndocker push &lt;myrepo&gt;/my-image\n</code></pre> </li> <li> <p>Add the container image name to the <code>nextflow.config</code> file.</p> <p>e.g. remove the following from the <code>nextflow.config</code>:</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\n</code></pre> <p>and replace with:</p> <pre><code>process.container = '&lt;myrepo&gt;/my-image'\n</code></pre> </li> <li> <p>Trying running Nextflow, e.g.:</p> <pre><code>nextflow run script7.nf -with-docker\n</code></pre> </li> </ol> <p>Nextflow should now be able to find <code>salmon</code> to run the process.</p>"},{"location":"basic_training/containers/#biocontainers","title":"BioContainers","text":"<p>Another useful resource linking together Bioconda and containers is the BioContainers project. BioContainers is a community initiative that provides a registry of container images for every Bioconda recipe.</p> <p>So far, we\u2019ve seen how to install packages with conda and micromamba, both locally and within containers. With BioContainers, you don\u2019t need to create your own container image for the tools you want, and you don\u2019t need to use conda or micromamba to install the packages. It already provides you with a Docker image containing the programs you want to be installed. For example, you can get the container image of fastqc using BioContainers with:</p> <pre><code>docker pull biocontainers/fastqc:v0.11.5\n</code></pre> <p>You can check the registry for the packages you want at BioContainers official website. For finding multi-tools container images, check their Multi-package images.</p> <p>Contrary to other registries that will pull the latest image when no tag (version) is provided, you must specify a tag when pulling BioContainers (after a colon <code>:</code>, e.g. fastqc:v0.11.5). Check the tags within the registry and pick the one that better suits your needs.</p> <p>You can also install <code>galaxy-util-tools</code> and search for mulled containers in your CLI. You'll find instructions below, using conda to install the tool.</p> <pre><code>conda activate a-conda-env-you-already-have\nconda install galaxy-tool-util\nmulled-search --destination quay singularity --channel bioconda --search bowtie samtools | grep mulled\n</code></pre> <p>Tip</p> <p>You can have more complex definitions within your process block by letting the appropriate container image or conda package be used depending on if the user selected singularity, Docker or conda to be used. You can click here for more information and here for an example.</p>"},{"location":"basic_training/containers/#exercises","title":"Exercises","text":"<p>Exercise</p> <p>During the earlier RNA-Seq tutorial (script2.nf), we created an index with the salmon tool. Given we do not have salmon installed locally in the machine provided by Gitpod, we had to either run it with <code>-with-conda</code> or <code>-with-docker</code>. Your task now is to run it again <code>-with-docker</code>, but without having to create your own Docker container image. Instead, use the BioContainers image for salmon 1.7.0.</p> Solution <pre><code>nextflow run script2.nf -with-docker quay.io/biocontainers/salmon:1.7.0--h84f40af_0\n</code></pre> <p>Bonus Exercise</p> <p>Change the process directives in <code>script5.nf</code> or the <code>nextflow.config</code> file to make the workflow automatically use BioContainers when using salmon, or fastqc.</p> <p>Hint</p> <p>Temporarily comment out the line <code>process.container = 'nextflow/rnaseq-nf'</code> in the <code>nextflow.config</code> file to make sure the processes are using the BioContainers that you set, and not the container image we have been using in this training.</p> Solution <p>With these changes, you should be able to run the workflow with BioContainers by running the following in the command line:</p> <pre><code>nextflow run script5.nf\n</code></pre> <p>with the following container directives for each process:</p> <pre><code>process FASTQC {\ncontainer 'biocontainers/fastqc:v0.11.5'\ntag \"FASTQC on $sample_id\"\n...\n</code></pre> <p>and</p> <pre><code>process QUANTIFICATION {\ntag \"Salmon on $sample_id\"\ncontainer 'quay.io/biocontainers/salmon:1.7.0--h84f40af_0'\npublishDir params.outdir, mode: 'copy'\n...\n</code></pre> <p>Check the <code>.command.run</code> file in the work directory and ensure that the run line contains the correct Biocontainers.</p>"},{"location":"basic_training/containers.pt/","title":"Gerencie depend\u00eancias e cont\u00eaineres","text":"<p>Fluxos de trabalhos computacionais s\u00e3o raramente compostos de um s\u00f3 script ou ferramenta. Muitas vezes, eles dependem de d\u00fazias de componentes de softwares ou bibliotecas.</p> <p>Instalar e manter tais depend\u00eancias \u00e9 uma tarefa desafiadora e uma fonte comum de irreprodutibilidade em aplica\u00e7\u00f5es cient\u00edficas.</p> <p>Para superar esses problemas, n\u00f3s utilizamos cont\u00eaineres que habilitam essas depend\u00eancias de softwares, isto \u00e9 ferramentas e bibliotecas necess\u00e1rias para uma an\u00e1lise de dados, para estar encapsuladas em uma ou mais imagens de cont\u00eainer Linux independentes, prontas para serem executadas e imut\u00e1veis. Elas podem facilmente ser implementadas em qualquer plataforma que suporta o motor de conteineriza\u00e7\u00e3o.</p> <p>Cont\u00eaineres podem ser executados de uma forma isolada pelo sistema do hospedeiro. Elas tem sua pr\u00f3pria c\u00f3pia do sistema de arquivos, espa\u00e7o de processamento, gerenciamento de mem\u00f3ria, etc.</p> <p>Info</p> <p>Cont\u00eaineres foram introduzidos no kernel 2.6 como um recurso do Linux conhecido como Control Groups ou Cgroups.</p>"},{"location":"basic_training/containers.pt/#docker","title":"Docker","text":"<p>Docker \u00e9 uma ferramenta \u00fatil para o gerenciamento, execu\u00e7\u00e3o e compartilhamento de imagens de cont\u00eaineres.</p> <p>Essas imagens can podem ser carregadas e publicadas em um reposit\u00f3rio centralizado conhecido como Docker Hub, ou hospedadas por terceiros como o Quay.</p>"},{"location":"basic_training/containers.pt/#execute-um-conteiner","title":"Execute um cont\u00eainer","text":"<p>Um cont\u00eainer pode ser executado com o seguinte comando:</p> <pre><code>docker run &lt;nome-do-cont\u00eainer&gt;\n</code></pre> <p>Tente executar o seguinte cont\u00eainer p\u00fablico (se voc\u00ea tiver o Docker instalado), por exemplo:</p> <pre><code>docker run hello-world\n</code></pre>"},{"location":"basic_training/containers.pt/#baixe-um-conteiner","title":"Baixe um cont\u00eainer","text":"<p>O comando pull possibilita que voc\u00ea baixe uma imagem Docker sem que a execute. Por exemplo:</p> <pre><code>docker pull debian:bullseye-slim\n</code></pre> <p>O comando acima baixa uma imagem Debian Linux. Voc\u00ea pode checar se ela existe usando:</p> <pre><code>docker images\n</code></pre>"},{"location":"basic_training/containers.pt/#execute-um-conteiner-em-mode-interativo","title":"Execute um cont\u00eainer em mode interativo","text":"<p>Iniciar uma shell BASH em um cont\u00eainer permite que voc\u00ea opere em modo interativo no sistema operacional conteinerizado. Por exemplo:</p> <pre><code>docker run -it debian:bullseye-slim bash\n</code></pre> <p>Uma vez iniciado, voc\u00ea vai notar que est\u00e1 como root (!). Use os comandos usuais para navegar pelo sistema de arquivos. Isso \u00e9 \u00fatil para checar se os programas necess\u00e1rios est\u00e3o presentes no cont\u00eainer.</p> <p>Para sair do cont\u00eainer, pare a sess\u00e3o BASH com o comando <code>exit</code>.</p>"},{"location":"basic_training/containers.pt/#seu-primeiro-dockerfile","title":"Seu primeiro Dockerfile","text":"<p>Imagens Docker s\u00e3o criadas utilizando um arquivo chamado <code>Dockerfile</code>, que \u00e9 um simples arquivo de texto contendo uma lista de comandos para montar e configurar uma imagem com os pacotes de programas necess\u00e1rios.</p> <p>Aqui, voc\u00ea criar\u00e1 uma imagem Docker contendo o cowsay e a ferramenta Salmon</p> <p>Warning</p> <p>O processo de montagem do Docker automaticamente copia todos os arquivos que est\u00e3o no diret\u00f3rio atual para o Docker daemon para que ele possa criar a imagem. Isso pode custar muito tempo quando existem v\u00e1rios ou grandes arquivos. Por essa raz\u00e3o, \u00e9 importante que sempre se trabalhe em um diret\u00f3rio contendo apenas os arquivos que voc\u00ea realmente precisa incluir em sua imagem Docker. Alternativamente, voc\u00ea pode usar o arquivo <code>.dockerignore</code> para selecionar os aquivos que ser\u00e3o exclu\u00eddos da montagem.</p> <p>Use seu editor favorito (ex.: <code>vim</code> ou <code>nano</code>) para criar um arquivo chamado <code>Dockerfile</code> e copiar o seguinte conte\u00fado:</p> <pre><code>FROM debian:bullseye-slim\nLABEL image.author.name \"Seu nome aqui\"\nLABEL image.author.email \"seu@email.aqui\"\nRUN apt-get update &amp;&amp; apt-get install -y curl cowsay\n\nENV PATH=$PATH:/usr/games/\n</code></pre>"},{"location":"basic_training/containers.pt/#monte-a-imagem","title":"Monte a imagem","text":"<p>Monte a imagem do Docker com base no Dockerfile utilizando o seguinte comando:</p> <pre><code>docker build -t minha-imagem .\n</code></pre> <p>Onde \"minha-imagem\" \u00e9 o nome que o usu\u00e1rio especificou para a imagem que ser\u00e1 criada.</p> <p>Tip</p> <p>N\u00e3o esque\u00e7a do ponto no comando acima.</p> <p>Quando completo, verifique se a imagem foi criada listando todas imagens dispon\u00edveis:</p> <pre><code>docker images\n</code></pre> <p>Voc\u00ea pode testar seu novo cont\u00eainer executando esse comando:</p> <pre><code>docker run minha-imagem cowsay Ol\u00e1 Docker!\n</code></pre>"},{"location":"basic_training/containers.pt/#adicione-um-pacote-de-programa-a-imagem","title":"Adicione um pacote de programa a imagem","text":"<p>Adicione o pacote Salmon para a imagem Docker adicionando o seguinte trecho para o <code>Dockerfile</code>:</p> <pre><code>RUN curl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.5.2/salmon-1.5.2_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Salve o arquivo e monte a imagem novamente utilizando o mesmo comando anterior:</p> <pre><code>docker build -t minha-imagem .\n</code></pre> <p>Voc\u00ea perceber\u00e1 que isso cria uma nova imagem Docker mas com um ID de imagem diferente.</p>"},{"location":"basic_training/containers.pt/#execute-salmon-no-conteiner","title":"Execute Salmon no cont\u00eainer","text":"<p>Cheque se o Salmon est\u00e1 executando corretamente no cont\u00eainer como mostrado abaixo:</p> <pre><code>docker run minha-imagem salmon --version\n</code></pre> <p>Voc\u00ea pode at\u00e9 iniciar o cont\u00eainer no modo interativo utilizando o seguinte comando:</p> <pre><code>docker run -it minha-imagem bash\n</code></pre> <p>Use o comando <code>exit</code> para finalizar a sess\u00e3o interativa.</p>"},{"location":"basic_training/containers.pt/#montagem-do-sistema-de-arquivos","title":"Montagem do sistema de arquivos","text":"<p>Crie um arquivo \u00edndice de genoma utilizando o Salmon no cont\u00eainer.</p> <p>Tente executar o Salmon no cont\u00eainer com o seguinte comando:</p> <pre><code>docker run minha-imagem \\\nsalmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>O comando acima falha porque o Salmon n\u00e3o pode acessar o arquivo de entrada.</p> <p>Isso acontece porque o cont\u00eainer executa em um sistema de arquivos totalmente diferente e n\u00e3o pode acessar o arquivo no sistema de arquivo do hospedeiro por padr\u00e3o.</p> <p>Voc\u00ea precisar\u00e1 usar a op\u00e7\u00e3o de linha de comando <code>--volume</code> para montar o(s) arquivo(s) de entrada, por exemplo</p> <pre><code>docker run --volume $PWD/data/ggal/transcriptome.fa:/transcriptome.fa minha-imagem \\\nsalmon index -t /transcriptome.fa -i transcript-index\n</code></pre> <p>Warning</p> <p>O diret\u00f3rio <code>transcript-index</code> gerado ainda est\u00e1 inacess\u00edvel no sistema de arquivo do sistema operacional hospedeiro.</p> <p>Um jeito mais f\u00e1cil \u00e9 montar o diret\u00f3rio original em um id\u00eantico no cont\u00eainer. Isso permite que voc\u00ea utilize o mesmo caminho durante a execu\u00e7\u00e3o dentro do cont\u00eainer, por exemplo</p> <pre><code>docker run --volume $PWD:$PWD --workdir $PWD minha-imagem \\\nsalmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Ou definir uma pasta que voc\u00ea queira montar como uma vari\u00e1vel de ambiente, chamada <code>DATA</code>:</p> <pre><code>DATA=/workspace/gitpod/nf-training/data\ndocker run --volume $DATA:$DATA --workdir $PWD minha-imagem \\\nsalmon index -t $PWD/data/ggal/transcriptome.fa -i transcript-index\n</code></pre> <p>Agora cheque o conte\u00fado da pasta <code>transcript-index</code> utilizando o comando:</p> <pre><code>ls -la transcript-index\n</code></pre> <p>Note</p> <p>Note que as permiss\u00f5es para cria\u00e7\u00e3o dos arquivos utilizado pelo Docker s\u00e3o <code>root</code>.</p>"},{"location":"basic_training/containers.pt/#disponibilize-o-conteiner-no-docker-hub-bonus","title":"Disponibilize o cont\u00eainer no Docker Hub (b\u00f4nus)","text":"<p>Publique seu cont\u00eainer no Docker Hub para compartilh\u00e1-lo com outras pessoas.</p> <p>Crie uma conta no site https://hub.docker.com. Ent\u00e3o no seu terminal shell execute o seguinte comando, utilizando seu usu\u00e1rio e senha que criou quando se registrou no Hub:</p> <pre><code>docker login\n</code></pre> <p>Renomeie a imagem para incluir seu nome de usu\u00e1rio Docker:</p> <pre><code>docker tag minha-imagem &lt;nome-de-usuario&gt;/minha-imagem\n</code></pre> <p>Finalmente mande para o Docker Hub:</p> <pre><code>docker push &lt;nome-de-usuario&gt;/minha-imagem\n</code></pre> <p>Depois disso, qualquer um conseguir\u00e1 baixar a imagem utilizando o comando:</p> <pre><code>docker pull &lt;nome-de-usuario&gt;/minha-imagem\n</code></pre> <p>Note que depois de uma opera\u00e7\u00e3o push e pull, o Docker imprime na tela o n\u00famero de registro do cont\u00eainer, por exemplo:</p> <pre><code>Digest: sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\nStatus: Downloaded newer image for nextflow/rnaseq-nf:latest\n</code></pre> <p>Isso \u00e9 um identificador imut\u00e1vel e \u00fanico que pode ser usado para referenciar a imagem de cont\u00eainer de uma forma \u00fanica. Por exemplo:</p> <pre><code>docker pull nextflow/rnaseq-nf@sha256:aeacbd7ea1154f263cda972a96920fb228b2033544c2641476350b9317dab266\n</code></pre>"},{"location":"basic_training/containers.pt/#execute-um-script-do-nextflow-utilizando-um-conteiner-docker","title":"Execute um script do Nextflow utilizando um cont\u00eainer Docker","text":"<p>A maneira mais simples de rodar um script Nextflow \u00e9 usando a op\u00e7\u00e3o de linha de comando <code>-with-docker</code>:</p> <pre><code>nextflow run script2.nf -with-docker minha-imagem\n</code></pre> <p>Como visto na \u00faltima se\u00e7\u00e3o, voc\u00ea tamb\u00e9m pode configurar o arquivo config (<code>nextflow.config</code>) para selecionar qual cont\u00eainer utilizar inv\u00e9s de ter que especificar como um argumento de linha de comando toda vez.</p>"},{"location":"basic_training/containers.pt/#singularity","title":"Singularity","text":"<p>Singularity \u00e9 um motor de conteineriza\u00e7\u00e3o desenvolvido para trabalhar com computa\u00e7\u00e3o de alta performance em centro de dados, onde geralmente o Docker n\u00e3o \u00e9 permitido por motivos de restri\u00e7\u00f5es de seguran\u00e7a.</p> <p>O Singularity implementa um modelo de execu\u00e7\u00e3o de cont\u00eainer similar ao Docker. Entretanto, ele usa um design de implementa\u00e7\u00e3o completamente diferente.</p> <p>Uma imagem de cont\u00eainer do Singularity \u00e9 arquivada como um arquivo de texto simples que pode ser armazenado em um sistema de arquivo compartilhado e acessado por muitos n\u00f3s computacionais gerenciados usando um escalonador de lote.</p> <p>Warning</p> <p>O Singularity n\u00e3o ir\u00e1 funcionar com Gitpod. Se voc\u00ea quer testar essa se\u00e7\u00e3o, por favor fa\u00e7a localmente, ou em um cluster.</p>"},{"location":"basic_training/containers.pt/#crie-imagens-do-singularity","title":"Crie imagens do Singularity","text":"<p>Imagens do Singularity s\u00e3o criadas utilizando um arquivo <code>Singularity</code> de uma forma similar ao Docker mas utilizando uma sintaxe diferente.</p> <pre><code>Bootstrap: docker\nFrom: debian:bullseye-slim\n\n%environment\nexport PATH=$PATH:/usr/games/\n\n%labels\nAUTHOR &lt;seu nome&gt;\n\n%post\napt-get update &amp;&amp; apt-get install -y locales-all curl cowsay\ncurl -sSL https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz | tar xz \\\n&amp;&amp; mv /salmon-*/bin/* /usr/bin/ \\\n&amp;&amp; mv /salmon-*/lib/* /usr/lib/\n</code></pre> <p>Uma vez que voc\u00ea salvou o arquivo <code>Singularity</code>, voc\u00ea pode criar uma imagem utilizando esses comandos:</p> <pre><code>sudo singularity build minha-imagem.sif Singularity\n</code></pre> <p>Nota: O comando <code>build</code> requer permiss\u00f5es <code>sudo</code>. Uma forma de contornar isso consiste em criar a imagem em uma esta\u00e7\u00e3o de trabalho local e ent\u00e3o implementar no cluster copiando o arquivo imagem.</p>"},{"location":"basic_training/containers.pt/#executando-um-conteiner","title":"Executando um cont\u00eainer","text":"<p>Quando terminar, voc\u00ea pode executar o cont\u00eainer com o seguinte comando:</p> <pre><code>singularity exec minha-imagem.sif cowsay 'Ol\u00e1 Singularity'\n</code></pre> <p>Utilizando o comando <code>shell</code> voc\u00ea pode entrar no cont\u00eainer utilizando o modo interativo. Por exemplo:</p> <pre><code>singularity shell minha-imagem.sif\n</code></pre> <p>Uma vez dentro da inst\u00e2ncia do cont\u00eainer execute o comando:</p> <pre><code>touch ola.txt\nls -la\n</code></pre> <p>Info</p> <p>Note como os arquivos do sistema de arquivos hospedeiro s\u00e3o mostrados. O Singularity automaticamente monta o diret\u00f3rio do hospedeiro <code>$HOME</code> e usa como diret\u00f3rio de trabalho.</p>"},{"location":"basic_training/containers.pt/#importe-uma-imagem-do-docker","title":"Importe uma imagem do Docker","text":"<p>Uma forma mais f\u00e1cil de criar um cont\u00eainer com o Singularity n\u00e3o necessitando da permiss\u00e3o <code>sudo</code> e melhorando a interoperabilidade dos cont\u00eaineres \u00e9 importando uma imagem de cont\u00eainer do Docker puxando diretamente do reposit\u00f3rio de imagens do Docker. Por exemplo:</p> <pre><code>singularity pull docker://debian:bullseye-slim\n</code></pre> <p>O comando acima automaticamente baixa uma imagem Docker do Debian e converte para uma imagem Singularity no diret\u00f3rio atual com o nome <code>debian-jessie.simg</code>.</p>"},{"location":"basic_training/containers.pt/#execute-um-script-do-nextflow-utilizando-um-conteiner-singularity","title":"Execute um script do Nextflow utilizando um cont\u00eainer Singularity","text":"<p>O Nextflow permite o uso de cont\u00eaineres Singularity de forma t\u00e3o f\u00e1cil e transparente quanto com o Docker.</p> <p>Simplesmente ative o uso do motor Singularity no lugar do Docker na linha de comando do Nextflow utilizando a op\u00e7\u00e3o de linha de comando <code>-with-singularity</code>:</p> <pre><code>nextflow run script7.nf -with-singularity nextflow/rnaseq-nf\n</code></pre> <p>Como antes, o cont\u00eainer Singularity tamb\u00e9m pode ser disponibilizado no arquivo de configura\u00e7\u00e3o do Nextflow. N\u00f3s iremos ver como funciona isso mais tarde.</p>"},{"location":"basic_training/containers.pt/#a-biblioteca-de-conteineres-singularity","title":"A Biblioteca de Cont\u00eaineres Singularity","text":"<p>Os autores do Singularity, SyLabs tem o seu pr\u00f3prio reposit\u00f3rio de cont\u00eaineres Singularity.</p> <p>Da mesma forma que disponibilizamos as imagens Docker no Docker Hub, n\u00f3s podemos disponibilizar as imagens Singularity na Singularity Library.</p>"},{"location":"basic_training/containers.pt/#pacotes-condabioconda","title":"Pacotes Conda/Bioconda","text":"<p>O Conda \u00e9 um popular gerenciador de pacotes e ambientes. O suporte a Conda permite que fluxos de trabalho Nextflow automaticamente criem e ativem ambientes Conda, dadas as depend\u00eancias especificadas de cada processo.</p> <p>Neste ambiente Gitpod, o conda j\u00e1 est\u00e1 instalado.</p>"},{"location":"basic_training/containers.pt/#usando-conda","title":"Usando conda","text":"<p>Um ambiente Conda \u00e9 definido utilizando um arquivo YAML, que lista todos os pacotes de programas. A primeira coisa que voc\u00ea precisa fazer \u00e9 iniciar o conda para uma intera\u00e7\u00e3o shell, e da\u00ed abrir um novo terminal utilizando bash.</p> <pre><code>conda init\nbash\n</code></pre> <p>Com isso, escreva seu arquivo YAML (<code>env.yml</code>). J\u00e1 existe um arquivo <code>env.yml</code> na pasta <code>nf-training</code> como um exemplo. O seu conte\u00fado \u00e9 mostrado abaixo.</p> <pre><code>name: nf-tutorial\nchannels:\n- conda-forge\n- defaults\n- bioconda\ndependencies:\n- bioconda::salmon=1.5.1\n- bioconda::fastqc=0.11.9\n- bioconda::multiqc=1.12\n- conda-forge::tbb=2020.2\n</code></pre> <p>Dado o arquivo de receita, o ambiente \u00e9 criado utilizando o comando abaixo. O comando <code>conda env create</code> deve demorar v\u00e1rios minutos, pois o conda tenta resolver todas as depend\u00eancias dos pacotes desejados durante a execu\u00e7\u00e3o, e ent\u00e3o baixa tudo que \u00e9 requerido.</p> <pre><code>conda env create --file env.yml\n</code></pre> <p>Voc\u00ea pode checar se o ambiente foi criado com \u00eaxito com o comando abaixo:</p> <pre><code>conda env list\n</code></pre> <p>Voc\u00ea deve ver algo similar ao mostrado abaixo:</p> <pre><code># conda environments:\n#\nbase                  *  /opt/conda\nnf-tutorial              /opt/conda/envs/nf-tutorial\n</code></pre> <p>Para habilitar o ambiente, voc\u00ea pode usar o comando <code>activate</code>:</p> <pre><code>conda activate nf-tutorial\n</code></pre> <p>O Nextflow consegue gerenciar a ativa\u00e7\u00e3o de um ambiente Conda quando seu diret\u00f3rio \u00e9 especificado utilizando a op\u00e7\u00e3o <code>-with-conda</code> (usando o mesmo caminho mostrado com a fun\u00e7\u00e3o <code>list</code>). Por exemplo:</p> <pre><code>nextflow run script7.nf -with-conda /opt/conda/envs/nf-tutorial\n</code></pre> <p>Info</p> <p>Quando criar um ambiente Conda com o arquivo de receita YAML, o Nextflow automaticamente baixar\u00e1 todas depend\u00eancias necess\u00e1rias, montar\u00e1 o ambiente e o ativar\u00e1.</p> <p>Isso torna f\u00e1cil gerenciar diferentes ambientes para os processos no fluxo de trabalho do script.</p> <p>Veja a documenta\u00e7\u00e3o para mais detalhes.</p>"},{"location":"basic_training/containers.pt/#crie-e-utilize-ambientes-no-estilo-do-conda-com-o-micromamba","title":"Crie e utilize ambientes no estilo do conda com o micromamba","text":"<p>Outra forma de construir um ambiente no estilo do conda \u00e9 com o <code>Dockerfile</code> e o <code>micromamba</code>.</p> <p>O <code>micromamba</code> \u00e9 um pacote r\u00e1pido e robusto para montar pequenos ambientes baseados no conda.</p> <p>Isso economiza tempo ao montar um ambiente conda toda vez que quiser utiliz\u00e1-lo (como delineado nas se\u00e7\u00f5es anteriores).</p> <p>Para fazer isso, voc\u00ea simplesmente precisa de um <code>Dockerfile</code> e utilizar o micromamba para instalar os pacotes. Por\u00e9m, uma boa pr\u00e1tica \u00e9 ter o arquivo de receita YAML como nas se\u00e7\u00f5es anteriores, ent\u00e3o n\u00f3s iremos fazer isso aqui tamb\u00e9m, utilizando o mesmo <code>env.yml</code> utilizado anteriormente.</p> <pre><code>name: nf-tutorial\nchannels:\n- conda-forge\n- defaults\n- bioconda\ndependencies:\n- bioconda::salmon=1.5.1\n- bioconda::fastqc=0.11.9\n- bioconda::multiqc=1.12\n- conda-forge::tbb=2020.2\n</code></pre> <p>Ent\u00e3o, n\u00f3s podemos escrever nosso Dockerfile com o micromamba instalando os pacotes por esse arquivo de receita.</p> <pre><code>FROM mambaorg/micromamba:0.25.1\nLABEL image.author.name \"Seu Nome Aqui\"\nLABEL image.author.email \"seu@email.aqui\"\nCOPY --chown=$MAMBA_USER:$MAMBA_USER env.yml /tmp/env.yml\n\nRUN micromamba create -n nf-tutorial\n\nRUN micromamba install -y -n nf-tutorial -f /tmp/env.yml &amp;&amp; \\\nmicromamba clean --all --yes\n\nENV PATH /opt/conda/envs/nf-tutorial/bin:$PATH\n</code></pre> <p>O <code>Dockerfile</code> acima pega a imagem pai mambaorg/micromamba, e instala um ambiente <code>conda</code> utilizando <code>micromamba</code>, e ent\u00e3o instala o <code>salmon</code>, o <code>fastqc</code> e o <code>multiqc</code>.</p> <p>Tente executar o fluxo de trabalho RNA-seq visto anteriormente (script7.nf). Comece montando seu pr\u00f3prio <code>Dockerfile</code> micromamba (como mostrado acima), salve no seu reposit\u00f3rio no Docker Hub, e oriente o Nextflow a rodar por esse cont\u00eainer (mudando seu <code>nextflow.config</code>).</p> <p>Warning</p> <p>Montar um cont\u00eainer Docker e disponibilizar no seu reposit\u00f3rio pessoal pode levar &gt;10 minutos.</p> Para um resumo dos passos a tomar, clique aqui: <ol> <li> <p>Crie um arquivo chamado <code>Dockerfile</code> no diret\u00f3rio atual (com os c\u00f3digo acima).</p> </li> <li> <p>Monte a imagem: <code>docker build -t minha-imagem .</code> (n\u00e3o esque\u00e7a o .).</p> </li> <li> <p>Publique a imagem Docker na sua conta do Docker Hub.</p> <p>Algo parecido como o seguinte, com <code>&lt;meurepo&gt;</code> substitu\u00eddo para seu pr\u00f3prio ID do Docker, sem &lt; e &gt; caracteres!</p> <p><code>minha-imagem</code> pode ser substitu\u00eddo por qualquer nome que voc\u00ea escolher. Como boa pr\u00e1tica, escolha algo memor\u00e1vel e certifique-se de que o nome combine com o nome usado no comando anterior.</p> <pre><code>docker login\ndocker tag minha-imagem &lt;meurepo&gt;/minha-imagem\ndocker push &lt;meurepo&gt;/minha-imagem\n</code></pre> </li> <li> <p>Adicione a imagem do cont\u00eainer no arquivo <code>nextflow.config</code>.</p> <p>ex. remova o seguinte do arquivo <code>nextflow.config</code>:</p> <pre><code>process.container = 'nextflow/rnaseq-nf'\n</code></pre> <p>e adicione:</p> <pre><code>process.container = '&lt;meurepo&gt;/minha-imagem'\n</code></pre> </li> <li> <p>Tente executar o Nextflow, por exemplo:</p> <pre><code>nextflow run script7.nf -with-docker\n</code></pre> </li> </ol> <p>Agora, o Nextflow deve conseguir achar <code>salmon</code> pra rodar o processo.</p>"},{"location":"basic_training/containers.pt/#biocontainers","title":"BioContainers","text":"<p>Outro recurso \u00fatil para conectar Bioconda e cont\u00eaineres \u00e9 o projeto BioContainers. BioContainers \u00e9 uma iniciativa da comunidade para prover um reposit\u00f3rio de imagens de cont\u00eainer para cada receita do Bioconda.</p> <p>At\u00e9 agora, n\u00f3s vimos como instalar pacotes com conda e micromamba, ambos localmente e com cont\u00eainer. Com o BioContainers, voc\u00ea n\u00e3o precisa criar sua pr\u00f3pria imagem de cont\u00eainer para as ferramentas que voc\u00ea quiser, e n\u00e3o precisa utilizar conda ou micromamba para instalar pacotes. O BioContainers j\u00e1 disponibiliza uma imagem Docker contendo os programas que voc\u00ea quer instalado. Por exemplo, voc\u00ea pode adquirir a imagem de cont\u00eainer do fastqc utilizando BioContainers:</p> <pre><code>docker pull biocontainers/fastqc:v0.11.5\n</code></pre> <p>Voc\u00ea pode checar o reposit\u00f3rio dos pacotes que quer no site oficial do BioContainers. Para encontrar imagens de container com v\u00e1rias ferramentas, confira a p\u00e1gina Multi-package images.</p> <p>Diferente de outros reposit\u00f3rios que ir\u00e3o puxar a imagem mais recente quando nenhum r\u00f3tulo (vers\u00e3o) \u00e9 especificado, voc\u00ea precisa especificar um r\u00f3tulo quando for baixar do BioContainers (depois de dois pontos <code>:</code>, por exemplo fastqc:v0.11.5). Cheque os r\u00f3tulos com o registro e escolha o que melhor se ad\u00e9qua a suas necessidades.</p> <p>Voc\u00ea tamb\u00e9m pode instalar o pacote <code>galaxy-util-tools</code> e procurar por imagens de container mulled atrav\u00e9s da linha de comando. Veja as instru\u00e7\u00f5es abaixo, usando o <code>conda</code> para instalar o pacote.</p> <pre><code>conda activate um-ambiente-conda-que-voce-ja-criou\nconda install galaxy-tool-util\nmulled-search --destination quay singularity --channel bioconda --search bowtie samtools | grep mulled\n</code></pre> <p>Tip</p> <p>Voc\u00ea pode ter defini\u00e7\u00f5es mais complexas dentro de seu bloco de processo deixando a imagem de cont\u00eainer apropriada ou o pacote conda para serem usadas dependendo se o usu\u00e1rio selecionou singularity, Docker ou conda. Voc\u00ea pode clicar aqui para mais informa\u00e7\u00f5es e aqui para um exemplo.</p>"},{"location":"basic_training/containers.pt/#exercises","title":"Exercises","text":"<p>Exercise</p> <p>Durante a se\u00e7\u00e3o onde constru\u00edmos o fluxo de trabalho de RNA-Seq, n\u00f3s criamos um \u00edndice (script2.nf). Dado que n\u00f3s n\u00e3o temos Salmon instalado localmente na m\u00e1quina provida pelo Gitpod, n\u00f3s temos que ou executar com <code>-with-conda</code> ou <code>-with-docker</code>. Sua tarefa agora \u00e9 executar novamente com <code>-with-docker</code>, mas sem ter que criar sua pr\u00f3pria imagem de cont\u00eainer Docker. Inv\u00e9s disso, use a imagem do BioContainers para Salmon 1.7.0.</p> Solution <pre><code>nextflow run script2.nf -with-docker quay.io/biocontainers/salmon:1.7.0--h84f40af_0\n</code></pre> <p>Bonus Exercise</p> <p>Mude as diretivas do processo no <code>script5.nf</code> ou no arquivo <code>nextflow.config</code> para fazer o fluxo de trabalho utilizar automaticamente BioContainers quando usando salmon, ou fastqc.</p> <p>Dica</p> <p>temporariamente comente a linha <code>process.container = 'nextflow/rnaseq-nf'</code> no arquivo <code>nextflow.config</code> para ter certeza que o processo est\u00e1 utilizando o cont\u00eainer do BioContainers que voc\u00ea configurou, e n\u00e3o a imagem de cont\u00eainer que est\u00e1vamos usando durante esse treinamento.</p> Solution <p>Com essas mudan\u00e7as, voc\u00ea deve ser capaz de executar o fluxo de trabalho com BioContainers executando a seguinte linha de comando:</p> <pre><code>nextflow run script5.nf\n</code></pre> <p>com as seguintes diretivas de cont\u00eainer para cada processo:</p> <pre><code>process FASTQC {\ncontainer 'biocontainers/fastqc:v0.11.5'\ntag \"FASTQC on $sample_id\"\n...\n</code></pre> <p>e</p> <pre><code>process QUANTIFICATION {\ntag \"Salmon on $sample_id\"\ncontainer 'quay.io/biocontainers/salmon:1.7.0--h84f40af_0'\npublishDir params.outdir, mode: 'copy'\n...\n</code></pre> <p>Cheque o arquivo <code>.command.run</code> no diret\u00f3rio de trabalho e certifique-se que a linha de execu\u00e7\u00e3o cont\u00e9m o Biocontainers correto.</p>"},{"location":"basic_training/debugging.fr/","title":"Traitement des erreurs et d\u00e9pannage","text":""},{"location":"basic_training/debugging.fr/#debogage-des-erreurs-dexecution","title":"D\u00e9bogage des erreurs d'ex\u00e9cution","text":"<p>Lorsqu'une ex\u00e9cution de processus se termine avec un statut de sortie non nul, Nextflow arr\u00eate l'ex\u00e9cution du workflow et signale la t\u00e2che d\u00e9faillante :</p> <p>Cliquez sur les ic\u00f4nes  dans le code pour obtenir des explications.</p> <pre><code>ERROR ~ Error executing process &gt; 'INDEX'\nCaused by: # (1)!\nProcess `INDEX` terminated with an error exit status (127)\nCommand executed: # (2)!\nsalmon index --threads 1 -t transcriptome.fa -i index\n\nCommand exit status: # (3)!\n127\nCommand output: # (4)!\n(empty)\nCommand error: # (5)!\n.command.sh: line 2: salmon: command not found\n\nWork dir: # (6)!\n/Users/pditommaso/work/0b/b59f362980defd7376ee0a75b41f62\n</code></pre> <ol> <li>Une description de la cause de l'erreur</li> <li>La commande ex\u00e9cut\u00e9e</li> <li>L'\u00e9tat de sortie de la commande</li> <li>La sortie standard de la commande, si elle est disponible</li> <li>L'erreur standard de la commande</li> <li>Le r\u00e9pertoire de travail de la commande</li> </ol> <p>Examinez attentivement toutes les donn\u00e9es d'erreur, car elles peuvent fournir des informations pr\u00e9cieuses pour le d\u00e9bogage.</p> <p>Si cela ne suffit pas, <code>cd</code> dans le r\u00e9pertoire de travail de la t\u00e2che. Il contient tous les fichiers n\u00e9cessaires pour reproduire le probl\u00e8me de mani\u00e8re isol\u00e9e.</p> <p>Le r\u00e9pertoire d'ex\u00e9cution de la t\u00e2che contient ces fichiers :</p> <ul> <li><code>.command.sh</code> : Le script de commande.</li> <li><code>.command.run</code> : La commande envelopp\u00e9e utilis\u00e9e pour ex\u00e9cuter la t\u00e2che.</li> <li><code>.command.out</code> : La sortie standard compl\u00e8te de la t\u00e2che.</li> <li><code>.command.err</code> : L'erreur standard de la t\u00e2che compl\u00e8te.</li> <li><code>.command.log</code> : La sortie de l'ex\u00e9cution du wrapper.</li> <li><code>.command.begin</code> : Fichier sentinelle cr\u00e9\u00e9 d\u00e8s que la t\u00e2che est lanc\u00e9e.</li> <li><code>.exitcode</code> : Un fichier contenant le code de sortie de la t\u00e2che.</li> <li>Fichiers d'entr\u00e9e de la t\u00e2che (liens symboliques)</li> <li>Fichiers de sortie de la t\u00e2che</li> </ul> <p>V\u00e9rifiez que le fichier <code>.command.sh</code> contient la commande attendue et que toutes les variables sont correctement r\u00e9solues.</p> <p>V\u00e9rifiez \u00e9galement l'existence des fichiers <code>.exitcode</code> et <code>.command.begin</code>, qui, s'ils sont absents, sugg\u00e8rent que la t\u00e2che n'a jamais \u00e9t\u00e9 ex\u00e9cut\u00e9e par le sous-syst\u00e8me (par exemple, le planificateur batch). Si le fichier <code>.command.begin</code> existe, la t\u00e2che a \u00e9t\u00e9 lanc\u00e9e mais a probablement \u00e9t\u00e9 interrompue brutalement.</p> <p>Vous pouvez reproduire l'\u00e9chec de l'ex\u00e9cution en utilisant la commande <code>bash .command.run</code> pour v\u00e9rifier la cause de l'erreur.</p>"},{"location":"basic_training/debugging.fr/#ignorer-les-erreurs","title":"Ignorer les erreurs","text":"<p>Dans certains cas, une erreur de processus peut \u00eatre attendue et ne doit pas interrompre l'ex\u00e9cution globale du workflow.</p> <p>Pour g\u00e9rer ce cas d'utilisation, d\u00e9finissez le processus <code>errorStrategy</code> \u00e0 <code>ignore</code> :</p> <pre><code>process FOO {\nerrorStrategy 'ignore'\nscript:\n\"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>Si vous souhaitez ignorer toute erreur, vous pouvez d\u00e9finir la m\u00eame directive dans le fichier de configuration comme param\u00e8tre par d\u00e9faut :</p> <pre><code>process.errorStrategy = 'ignore'\n</code></pre>"},{"location":"basic_training/debugging.fr/#basculement-automatique-en-cas-derreur","title":"Basculement automatique en cas d'erreur","text":"<p>Dans de rares cas, les erreurs peuvent \u00eatre caus\u00e9es par des conditions transitoires. Dans ce cas, une strat\u00e9gie efficace consiste \u00e0 r\u00e9ex\u00e9cuter la t\u00e2che d\u00e9faillante.</p> <pre><code>process FOO {\nerrorStrategy 'retry'\nscript:\n\"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>En utilisant la strat\u00e9gie d'erreur <code>retry</code>, la t\u00e2che est r\u00e9ex\u00e9cut\u00e9e une seconde fois si elle renvoie un statut de sortie non nul avant d'arr\u00eater l'ex\u00e9cution compl\u00e8te du flux de travail.</p> <p>La directive maxRetries peut \u00eatre utilis\u00e9e pour d\u00e9finir le nombre de tentatives de r\u00e9ex\u00e9cution de la t\u00e2che avant de d\u00e9clarer qu'elle a \u00e9chou\u00e9 avec une condition d'erreur.</p>"},{"location":"basic_training/debugging.fr/#reessayer-avec-backoff","title":"R\u00e9essayer avec backoff","text":"<p>Dans certains cas, les ressources d'ex\u00e9cution requises peuvent \u00eatre temporairement indisponibles (par exemple, en cas de congestion du r\u00e9seau). Dans ce cas, la simple r\u00e9ex\u00e9cution de la m\u00eame t\u00e2che entra\u00eenera probablement une erreur identique. Une nouvelle tentative avec un d\u00e9lai exponentiel backoff permet de mieux r\u00e9cup\u00e9rer ces conditions d'erreur.</p> <pre><code>process FOO {\nerrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\nmaxRetries 5\nscript:\n'''\n    your_command --here\n    '''\n}\n</code></pre>"},{"location":"basic_training/debugging.fr/#allocation-dynamique-des-ressources","title":"Allocation dynamique des ressources","text":"<p>Il est tr\u00e8s fr\u00e9quent que diff\u00e9rentes instances d'un m\u00eame processus aient des besoins tr\u00e8s diff\u00e9rents en termes de ressources informatiques. Dans de telles situations, le fait de demander, par exemple, une quantit\u00e9 de m\u00e9moire trop faible entra\u00eenera l'\u00e9chec de certaines t\u00e2ches. Au contraire, l'utilisation d'une limite plus \u00e9lev\u00e9e qui correspond \u00e0 toutes les t\u00e2ches de votre ex\u00e9cution pourrait r\u00e9duire de mani\u00e8re significative la priorit\u00e9 d'ex\u00e9cution de votre travail dans un syst\u00e8me d'ordonnancement.</p> <p>Pour g\u00e9rer ce cas d'utilisation, vous pouvez utiliser une strat\u00e9gie d'erreur <code>retry</code> et augmenter les ressources informatiques allou\u00e9es par la t\u00e2che \u00e0 chaque attempt successif.</p> <pre><code>process FOO {\ncpus 4\nmemory { 2.GB * task.attempt } // (1)!\ntime { 1.hour * task.attempt } // (2)!\nerrorStrategy { task.exitStatus == 140 ? 'retry' : 'terminate' } // (3)!\nmaxRetries 3 // (4)!\nscript:\n\"\"\"\n    your_command --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <ol> <li>La m\u00e9moire est d\u00e9finie de mani\u00e8re dynamique, la premi\u00e8re tentative \u00e9tant de 2 Go, la seconde de 4 Go, etc.</li> <li>La hauteur du temps d'ex\u00e9cution est \u00e9galement d\u00e9fini de mani\u00e8re dynamique, la premi\u00e8re tentative d'ex\u00e9cution est fix\u00e9e \u00e0 1 heure, la seconde \u00e0 2 heures, etc.</li> <li>Si la t\u00e2che renvoie un statut de sortie \u00e9gal \u00e0 <code>140</code>, la strat\u00e9gie d'erreur sera <code>retry</code>, sinon elle mettra fin \u00e0 l'ex\u00e9cution.</li> <li>Elle r\u00e9essayera l'ex\u00e9cution du processus jusqu'\u00e0 trois fois.</li> </ol>"},{"location":"basic_training/debugging/","title":"Error handling and troubleshooting","text":""},{"location":"basic_training/debugging/#execution-error-debugging","title":"Execution error debugging","text":"<p>When a process execution exits with a non-zero exit status, Nextflow stops the workflow execution and reports the failing task:</p> <p>Click the  icons in the code for explanations.</p> <pre><code>ERROR ~ Error executing process &gt; 'INDEX'\nCaused by: # (1)!\nProcess `INDEX` terminated with an error exit status (127)\nCommand executed: # (2)!\nsalmon index --threads 1 -t transcriptome.fa -i index\n\nCommand exit status: # (3)!\n127\nCommand output: # (4)!\n(empty)\nCommand error: # (5)!\n.command.sh: line 2: salmon: command not found\n\nWork dir: # (6)!\n/Users/pditommaso/work/0b/b59f362980defd7376ee0a75b41f62\n</code></pre> <ol> <li>A description of the error cause</li> <li>The command executed</li> <li>The command exit status</li> <li>The command standard output, when available</li> <li>The command standard error</li> <li>The command work directory</li> </ol> <p>Carefully review all error data as it can provide information valuable for debugging.</p> <p>If this is not enough, <code>cd</code> into the task work directory. It contains all the files to replicate the issue in an isolated manner.</p> <p>The task execution directory contains these files:</p> <ul> <li><code>.command.sh</code>: The command script.</li> <li><code>.command.run</code>: The command wrapped used to run the task.</li> <li><code>.command.out</code>: The complete task standard output.</li> <li><code>.command.err</code>: The complete task standard error.</li> <li><code>.command.log</code>: The wrapper execution output.</li> <li><code>.command.begin</code>: Sentinel file created as soon as the task is launched.</li> <li><code>.exitcode</code>: A file containing the task exit code.</li> <li>Task input files (symlinks)</li> <li>Task output files</li> </ul> <p>Verify that the <code>.command.sh</code> file contains the expected command and all variables are correctly resolved.</p> <p>Also verify the existence of the <code>.exitcode</code> and <code>.command.begin</code> files, which if absent, suggest the task was never executed by the subsystem (e.g. the batch scheduler). If the <code>.command.begin</code> file exists, the task was launched but was likely killed abruptly.</p> <p>You can replicate the failing execution using the command <code>bash .command.run</code> to verify the cause of the error.</p>"},{"location":"basic_training/debugging/#ignore-errors","title":"Ignore errors","text":"<p>There are cases in which a process error may be expected and it should not stop the overall workflow execution.</p> <p>To handle this use case, set the process <code>errorStrategy</code> to <code>ignore</code>:</p> <pre><code>process FOO {\nerrorStrategy 'ignore'\nscript:\n\"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>If you want to ignore any error you can set the same directive in the config file as a default setting:</p> <pre><code>process.errorStrategy = 'ignore'\n</code></pre>"},{"location":"basic_training/debugging/#automatic-error-fail-over","title":"Automatic error fail-over","text":"<p>In rare cases, errors may be caused by transient conditions. In this situation, an effective strategy is re-executing the failing task.</p> <pre><code>process FOO {\nerrorStrategy 'retry'\nscript:\n\"\"\"\n    your_command --this --that\n    \"\"\"\n}\n</code></pre> <p>Using the <code>retry</code> error strategy the task is re-executed a second time if it returns a non-zero exit status before stopping the complete workflow execution.</p> <p>The directive maxRetries can be used to set the number of attempts the task can be re-executed before declaring it failed with an error condition.</p>"},{"location":"basic_training/debugging/#retry-with-backoff","title":"Retry with backoff","text":"<p>There are cases in which the required execution resources may be temporarily unavailable (e.g. network congestion). In these cases simply re-executing the same task will likely result in an identical error. A retry with an exponential backoff delay can better recover these error conditions.</p> <pre><code>process FOO {\nerrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\nmaxRetries 5\nscript:\n'''\n    your_command --here\n    '''\n}\n</code></pre>"},{"location":"basic_training/debugging/#dynamic-resources-allocation","title":"Dynamic resources allocation","text":"<p>It\u2019s a very common scenario that different instances of the same process may have very different needs in terms of computing resources. In such situations requesting, for example, an amount of memory too low will cause some tasks to fail. Instead, using a higher limit that fits all the tasks in your execution could significantly decrease the execution priority of your job in a scheduling system.</p> <p>To handle this use case, you can use a <code>retry</code> error strategy and increase the computing resources allocated by the task at each successive attempt.</p> <pre><code>process FOO {\ncpus 4\nmemory { 2.GB * task.attempt } // (1)!\ntime { 1.hour * task.attempt } // (2)!\nerrorStrategy { task.exitStatus == 140 ? 'retry' : 'terminate' } // (3)!\nmaxRetries 3 // (4)!\nscript:\n\"\"\"\n    your_command --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <ol> <li>The memory is defined in a dynamic manner, the first attempt is 2 GB, the second 4 GB, and so on.</li> <li>The wall execution time is set dynamically as well, the first execution attempt is set to 1 hour, the second 2 hours, and so on.</li> <li>If the task returns an exit status equal to <code>140</code> it will set the error strategy to <code>retry</code> otherwise it will terminate the execution.</li> <li>It will retry the process execution up to three times.</li> </ol>"},{"location":"basic_training/debugging.pt/","title":"Tratamento de erros e resolu\u00e7\u00e3o de problemas","text":""},{"location":"basic_training/debugging.pt/#depuracao-de-erros-de-execucao","title":"Depura\u00e7\u00e3o de erros de execu\u00e7\u00e3o","text":"<p>Quando a execu\u00e7\u00e3o de um processo termina com um status de sa\u00edda diferente de zero, o Nextflow encerra a execu\u00e7\u00e3o e informa sobre a tarefa com falhas:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>ERROR ~ Error executing process &gt; 'INDEX'\nCaused by: # (1)!\nProcess `INDEX` terminated with an error exit status (127)\nCommand executed: # (2)!\nsalmon index --threads 1 -t transcriptome.fa -i index\n\nCommand exit status: # (3)!\n127\nCommand output: # (4)!\n(empty)\nCommand error: # (5)!\n.command.sh: line 2: salmon: command not found\n\nWork dir: # (6)!\n/Users/pditommaso/work/0b/b59f362980defd7376ee0a75b41f62\n</code></pre> <ol> <li>Uma descri\u00e7\u00e3o da causa do erro</li> <li>O comando executado</li> <li>O status de sa\u00edda do comando</li> <li>A sa\u00edda padr\u00e3o do comando, quando dispon\u00edvel</li> <li>O erro padr\u00e3o do comando</li> <li>O diret\u00f3rio de trabalho do comando</li> </ol> <p>Analise cuidadosamente os dados do erro, j\u00e1 que eles podem fornecer informa\u00e7\u00f5es valiosas para a depura\u00e7\u00e3o.</p> <p>Se isso n\u00e3o for o suficiente, use <code>cd</code> para entrar no diret\u00f3rio de trabalho da tarefa. Ele contem todos os arquivos necess\u00e1rios para reproduzir o erro de forma isolada.</p> <p>O diret\u00f3rio de execu\u00e7\u00e3o da tarefa possui os seguintes arquivos:</p> <ul> <li><code>.command.sh</code>: O script do comando.</li> <li><code>.command.run</code>: Um wrapper do comando usado para executar a tarefa.</li> <li><code>.command.out</code>: A sa\u00edda padr\u00e3o completa da tarefa.</li> <li><code>.command.err</code>: O erro padr\u00e3o completo da tarefa.</li> <li><code>.command.log</code>: A sa\u00edda do wrapper de execu\u00e7\u00e3o.</li> <li><code>.command.begin</code>: Um arquivo sentinela criado no momento que a tarefa \u00e9 iniciada.</li> <li><code>.exitcode</code>: Um arquivo contendo o c\u00f3digo de sa\u00edda da tarefa.</li> <li>Os arquivos de entrada da tarefa (links simb\u00f3licas)</li> <li>Os arquivos de sa\u00edda da tarefa</li> </ul> <p>Certifique-se que o arquivo <code>.command.sh</code> cont\u00e9m o comando esperado e que todas as vari\u00e1veis foram substitu\u00eddas pelos valores desejados.</p> <p>Tamb\u00e9m verifique a exist\u00eancia dos arquivos <code>.exitcode</code> e <code>.command.begin</code>, que, se ausentes, sugerem que a tarefa nunca foi executada pelo subsistema (o escalonador de lotes, por exemplo). Se o arquivo <code>.command.begin</code> existir, a tarefa foi iniciada mas foi provavelmente encerrada abruptamente.</p> <p>Para verificar a causa do erro, voc\u00ea pode replicar a execu\u00e7\u00e3o com falhas usando <code>bash .command.run</code>.</p>"},{"location":"basic_training/debugging.pt/#ignorando-erros","title":"Ignorando erros","text":"<p>Existem casos em que um erro em um processo \u00e9 esperado e n\u00e3o deve encerrar a execu\u00e7\u00e3o do fluxo de trabalho.</p> <p>Para lidar com isso, forne\u00e7a o valor <code>ignore</code> a <code>errorStrategy</code>:</p> <pre><code>process FOO {\nerrorStrategy 'ignore'\nscript:\n\"\"\"\n    seu_comando --isso --aquilo\n    \"\"\"\n}\n</code></pre> <p>Se voc\u00ea deseja ignorar qualquer erro, voc\u00ea pode especificar a mesma diretiva em um arquivo de configura\u00e7\u00e3o:</p> <pre><code>process.errorStrategy = 'ignore'\n</code></pre>"},{"location":"basic_training/debugging.pt/#tolerancia-automatica-a-falhas","title":"Toler\u00e2ncia autom\u00e1tica a falhas","text":"<p>Em casos mais raros, erros podem surgir por causa de condi\u00e7\u00f5es transit\u00f3rias. Nessas situa\u00e7\u00f5es, uma estrat\u00e9gia eficaz \u00e9 re-executar a tarefa com falhas.</p> <pre><code>process FOO {\nerrorStrategy 'retry'\nscript:\n\"\"\"\n    seu_comando --isso --aquilo\n    \"\"\"\n}\n</code></pre> <p>Ao usar a estrat\u00e9gia de erro <code>retry</code> a tarefa \u00e9 re-executada uma segunda vez se ela retornar um status de sa\u00edda diferente de zero antes de encerrar a execu\u00e7\u00e3o completa do fluxo de trabalho.</p> <p>A diretiva maxRetries pode ser utilizada para configurar o n\u00famero de tentativas que uma tarefa pode ser re-executada antes de declarar que ela falhou com uma condi\u00e7\u00e3o de erro.</p>"},{"location":"basic_training/debugging.pt/#re-execucao-com-atraso","title":"Re-execu\u00e7\u00e3o com atraso","text":"<p>Existem casos em que os recursos necess\u00e1rios para a execu\u00e7\u00e3o est\u00e3o temporariamente indispon\u00edveis (por exemplo, congestionamento de rede). Nesses casos apenas re-executar a tarefa provavelmente levar\u00e1 a um erro id\u00eantico. Uma re-execu\u00e7\u00e3o com um atraso exponencial pode contribuir de uma melhor forma para a resolu\u00e7\u00e3o desses erros.</p> <pre><code>process FOO {\nerrorStrategy { sleep(Math.pow(2, task.attempt) * 200 as long); return 'retry' }\nmaxRetries 5\nscript:\n'''\n    seu_comando --aqui\n    '''\n}\n</code></pre>"},{"location":"basic_training/debugging.pt/#alocacao-dinamica-de-recursos","title":"Aloca\u00e7\u00e3o din\u00e2mica de recursos","text":"<p>Uma situa\u00e7\u00e3o bastante comum \u00e9 que diferentes inst\u00e2ncias de um mesmo processo podem ter necessidades diferentes de recursos computacionais. Nessas situa\u00e7\u00f5es, solicitar uma quantidade de mem\u00f3ria muito baixa, por exemplo, ir\u00e1 levar algumas tarefas a falharem. Por outro lado, usar um limite mais elevado que abrange todas as suas tarefas pode reduzir significativamente a prioridade de execu\u00e7\u00e3o delas em um sistema de escalonamento de tarefas.</p> <p>Para lidar com isso, voc\u00ea pode utilizar uma estrat\u00e9gia de erro <code>retry</code> e aumentar os recursos computacionais alocados pela tarefa em cada tentativa consecutiva.</p> <pre><code>process FOO {\ncpus 4\nmemory { 2.GB * task.attempt } // (1)!\ntime { 1.hour * task.attempt } // (2)!\nerrorStrategy { task.exitStatus == 140 ? 'retry' : 'terminate' } // (3)!\nmaxRetries 3 // (4)!\nscript:\n\"\"\"\n    seu_comando --cpus $task.cpus --mem $task.memory\n    \"\"\"\n}\n</code></pre> <ol> <li>A mem\u00f3ria \u00e9 definida de forma din\u00e2mica, a primeira tentativa \u00e9 com 2 GB, a segunda com 4 GB, e assim sucessivamente.</li> <li>O tempo de execu\u00e7\u00e3o da tarefa \u00e9 configurado dinamicamente tamb\u00e9m, a primeira tentativa \u00e9 com 1 hora, a segunda com 2 horas, e assim sucessivamente.</li> <li>Se a tarefa retorna um status de sa\u00edda igual a <code>140</code> a estrat\u00e9gia de erro ser\u00e1 <code>retry</code>, caso contr\u00e1rio, a execu\u00e7\u00e3o ser\u00e1 encerrada.</li> <li>O processo ser\u00e1 re-executado at\u00e9 tr\u00eas vezes.</li> </ol>"},{"location":"basic_training/executors.fr/","title":"Sc\u00e9narios de d\u00e9ploiement","text":"<p>Les applications g\u00e9nomiques du monde r\u00e9el peuvent engendrer l'ex\u00e9cution de milliers de t\u00e2ches. Dans ce sc\u00e9nario, un planificateur de lots est g\u00e9n\u00e9ralement utilis\u00e9 pour d\u00e9ployer un workflow dans un cluster informatique, permettant l'ex\u00e9cution de nombreuses t\u00e2ches en parall\u00e8le sur de nombreux n\u0153uds informatiques.</p> <p>Nextflow a un support int\u00e9gr\u00e9 pour les plannificateurs batch les plus couramment utilis\u00e9s, tels que Univa Grid Engine, SLURM et IBM LSF. Consultez la documentation de Nextflow pour obtenir la liste compl\u00e8te des prises en charge plates-formes d'ex\u00e9cution.</p>"},{"location":"basic_training/executors.fr/#deploiement-du-cluster","title":"D\u00e9ploiement du cluster","text":"<p>L'une des principales caract\u00e9ristiques de Nextflow est la capacit\u00e9 de d\u00e9coupler la mise en \u0153uvre du workflow de la plate-forme d'ex\u00e9cution proprement dite. La mise en \u0153uvre d'une couche d'abstraction permet de d\u00e9ployer le workflow r\u00e9sultant sur n'importe quelle plate-forme d'ex\u00e9cution prise en charge par le cadre.</p> <p></p> <p>Pour ex\u00e9cuter votre workflow avec un planificateur batch, modifiez le fichier <code>nextflow.config</code> en sp\u00e9cifiant l'ex\u00e9cuteur cible et les ressources informatiques requises si n\u00e9cessaire. Par exemple, le fichier <code>nextflow.config</code> peut \u00eatre modifi\u00e9 :</p> <pre><code>process.executor = 'slurm'\n</code></pre>"},{"location":"basic_training/executors.fr/#gestion-des-ressources-du-cluster","title":"Gestion des ressources du cluster","text":"<p>Lors de l'utilisation d'un planificateur batch, il est souvent n\u00e9cessaire de sp\u00e9cifier le nombre de ressources (cpus, m\u00e9moire, temps d'ex\u00e9cution, etc.) requises par chaque t\u00e2che.</p> <p>Pour ce faire, il convient d'utiliser les directives de processus suivantes :</p> queue la queue de cluster \u00e0 utiliser pour le calcul cpus le nombre de cpus \u00e0 allouer pour l'ex\u00e9cution d'une t\u00e2che memoire la quantit\u00e9 de m\u00e9moire \u00e0 allouer pour l'ex\u00e9cution d'une t\u00e2che temps la quantit\u00e9 maximale de temps \u00e0 allouer pour l'ex\u00e9cution d'une t\u00e2che disque la quantit\u00e9 de m\u00e9moire disk requise pour l'ex\u00e9cution d'une t\u00e2che"},{"location":"basic_training/executors.fr/#ressources-a-lechelle-du-workflow","title":"Ressources \u00e0 l'\u00e9chelle du workflow","text":"<p>Utilisez le champ d'application <code>process</code> pour d\u00e9finir les besoins en ressources de tous les processus de vos applications de workflow. Par exemple :</p> <pre><code>process {\nexecutor = 'slurm'\nqueue = 'short'\nmemory = '10 GB'\ntime = '30 min'\ncpus = 4\n}\n</code></pre>"},{"location":"basic_training/executors.fr/#soumettre-nextflow-en-tant-que-tache","title":"Soumettre Nextflow en tant que tache","text":"<p>Bien que la commande principale de Nextflow puisse \u00eatre lanc\u00e9e sur le n\u0153ud de connexion / t\u00eate d'un cluster, il faut savoir que le n\u0153ud doit \u00eatre configur\u00e9 pour des commandes qui s'ex\u00e9cutent pendant une longue p\u00e9riode, m\u00eame si les ressources informatiques utilis\u00e9es sont n\u00e9gligeables. Une autre option est de soumettre le processus Nextflow principal en tant que tache sur le cluster.</p> <p>Remarque</p> <p>Cela n\u00e9cessite que la configuration de votre cluster permette de lancer des t\u00e2ches \u00e0 partir des n\u0153uds de travail, car Nextflow soumettra de nouvelles t\u00e2ches et les g\u00e9rera \u00e0 partir d'ici.</p> <p>Par exemple, si votre cluster utilise Slurm comme planificateur de t\u00e2ches, vous pouvez cr\u00e9er un fichier similaire \u00e0 celui ci-dessous :</p> launch_nf.sh<pre><code>#!/bin/bash\n#SBATCH --partition WORK\n#SBATCH --mem 5G\n#SBATCH -c 1\n#SBATCH -t 12:00:00\nWORKFLOW=$1\nCONFIG=$2\n# Utiliser un environnement conda o\u00f9 vous avez install\u00e9 Nextflow\n# (peut ne pas \u00eatre n\u00e9cessaire si vous l'avez install\u00e9 d'une autre mani\u00e8re)\nconda activate nextflow\n\nnextflow -C ${CONFIG} run ${WORKFLOW}\n</code></pre> <p>Puis soumettez-le avec :</p> <pre><code>sbatch launch_nf.sh /home/my_user/path/my_workflow.nf /home/my_user/path/my_config_file.conf\n</code></pre> <p>Vous trouverez plus de d\u00e9tails sur l'exemple ci-dessus ici. Vous trouverez d'autres conseils pour l'ex\u00e9cution de Nextflow sur HPC dans les articles de blog suivants :</p> <ul> <li>5 astuces Nextflow pour les utilisateurs HPC</li> <li>Cinq astuces suppl\u00e9mentaires pour les utilisateurs de Nextflow sur le HPC</li> </ul>"},{"location":"basic_training/executors.fr/#configurer-le-processus-par-nom","title":"Configurer le processus par nom","text":"<p>Dans les applications r\u00e9elles, des t\u00e2ches diff\u00e9rentes n\u00e9cessitent des ressources informatiques diff\u00e9rentes. Il est possible de d\u00e9finir les ressources pour une t\u00e2che sp\u00e9cifique en utilisant la commande <code>withName:</code> suivie du nom du processus :</p> <pre><code>process {\nexecutor = 'slurm'\nqueue = 'short'\nmemory = '10 GB'\ntime = '30 min'\ncpus = 4\nwithName: FOO {\ncpus = 2\nmemory = '20 GB'\nqueue = 'short'\n}\nwithName: BAR {\ncpus = 4\nmemory = '32 GB'\nqueue = 'long'\n}\n}\n</code></pre> <p>Exercice</p> <p>Ex\u00e9cuter le script RNA-Seq (<code>script7.nf</code>) de tout \u00e0 l'heure, mais sp\u00e9cifier que le processus <code>QUANTIFICATION</code> n\u00e9cessite 2 CPUs et 5 GB de m\u00e9moire, dans le fichier <code>nextflow.config</code>.</p> Solution <pre><code>process {\nwithName: QUANTIFICATION {\ncpus = 2\nmemory = '5 GB'\n}\n}\n</code></pre>"},{"location":"basic_training/executors.fr/#configurer-le-processus-par-etiquettes","title":"Configurer le processus par \u00e9tiquettes","text":"<p>Lorsqu'une application de workflow est compos\u00e9e de nombreux processus, il peut \u00eatre difficile de dresser la liste de tous les noms de processus et de choisir des ressources pour chacun d'entre eux dans le fichier de configuration.</p> <p>Une meilleure strat\u00e9gie consiste \u00e0 annoter les processus avec une directive label. Sp\u00e9cifiez ensuite les ressources dans le fichier de configuration utilis\u00e9 pour tous les processus ayant le m\u00eame label.</p> <p>Le script du workflow :</p> <pre><code>process TASK1 {\nlabel 'long'\nscript:\n\"\"\"\n    first_command --here\n    \"\"\"\n}\nprocess TASK2 {\nlabel 'short'\nscript:\n\"\"\"\n    second_command --here\n    \"\"\"\n}\n</code></pre> <p>Le fichier de configuration :</p> <pre><code>process {\nexecutor = 'slurm'\nwithLabel: 'short' {\ncpus = 4\nmemory = '20 GB'\nqueue = 'alpha'\n}\nwithLabel: 'long' {\ncpus = 8\nmemory = '32 GB'\nqueue = 'omega'\n}\n}\n</code></pre>"},{"location":"basic_training/executors.fr/#configurer-plusieurs-containers","title":"Configurer plusieurs containers","text":"<p>Les containers peuvent \u00eatre d\u00e9finis pour chaque processus de votre flux de travail. Vous pouvez d\u00e9finir leurs conteneurs dans un fichier de configuration, comme indiqu\u00e9 ci-dessous :</p> <pre><code>process {\nwithName: FOO {\ncontainer = 'some/image:x'\n}\nwithName: BAR {\ncontainer = 'other/image:y'\n}\n}\ndocker.enabled = true\n</code></pre> <p>Astuce</p> <p>Dois-je utiliser un seul container fat ou plusieurs containers slim ? Les deux approches ont des avantages et des inconv\u00e9nients. Un container unique est plus simple \u00e0 construire et \u00e0 maintenir, mais lorsque vous utilisez de nombreux outils, l'image peut devenir tr\u00e8s volumineuse et les outils peuvent cr\u00e9er des conflits entre eux. L'utilisation d'un container pour chaque processus peut donner lieu \u00e0 de nombreuses images diff\u00e9rentes \u00e0 construire et \u00e0 maintenir, en particulier lorsque les processus de votre workflow utilisent des outils diff\u00e9rents pour chaque t\u00e2che.</p> <p>Pour en savoir plus sur les s\u00e9lecteurs de processus de configuration, consultez ce lien.</p>"},{"location":"basic_training/executors.fr/#configuration-des-profils","title":"Configuration des profils","text":"<p>Les fichiers de configuration peuvent contenir la d\u00e9finition d'un ou plusieurs profils. Un profil est un ensemble d'attributs de configuration qui peuvent \u00eatre activ\u00e9s/choisis lors du lancement de l'ex\u00e9cution d'un workflow en utilisant l'option de ligne de commande <code>-profile</code>.</p> <p>Les profils de configuration sont d\u00e9finis en utilisant la port\u00e9e sp\u00e9ciale <code>profiles</code> qui regroupe les attributs appartenant au m\u00eame profil en utilisant un pr\u00e9fixe commun. Par exemple :</p> <pre><code>profiles {\nstandard {\nparams.genome = '/local/path/ref.fasta'\nprocess.executor = 'local'\n}\ncluster {\nparams.genome = '/data/stared/ref.fasta'\nprocess.executor = 'sge'\nprocess.queue = 'long'\nprocess.memory = '10 GB'\nprocess.conda = '/some/path/env.yml'\n}\ncloud {\nparams.genome = '/data/stared/ref.fasta'\nprocess.executor = 'awsbatch'\nprocess.container = 'cbcrg/imagex'\ndocker.enabled = true\n}\n}\n</code></pre> <p>Cette configuration d\u00e9finit trois profils diff\u00e9rents : <code>standard</code>, <code>cluster</code> et <code>cloud</code> qui d\u00e9finissent diff\u00e9rentes strat\u00e9gies de configuration de processus en fonction de la plateforme d'ex\u00e9cution cible. Par convention, le profil <code>standard</code> est implicitement utilis\u00e9 lorsqu'aucun autre profil n'est sp\u00e9cifi\u00e9 par l'utilisateur.</p> <p>Pour activer un profil sp\u00e9cifique, utilisez l'option <code>-profile</code> suivie du nom du profil :</p> <pre><code>nextflow run &lt;your script&gt; -profile cluster\n</code></pre> <p>Astuce</p> <p>Il est possible de sp\u00e9cifier deux profils de configuration ou plus en s\u00e9parant les noms des profils par une virgule :</p> <pre><code>nextflow run &lt;your script&gt; -profile standard,cloud\n</code></pre>"},{"location":"basic_training/executors.fr/#deploiement-dans-le-cloud","title":"D\u00e9ploiement dans le cloud","text":"<p>AWS Batch est un service informatique g\u00e9r\u00e9 qui permet l'ex\u00e9cution de charges de travail contain\u00e9ris\u00e9es dans l'infrastructure cloud d'Amazon.</p> <p>Nextflow fournit un support int\u00e9gr\u00e9 pour AWS Batch qui permet le d\u00e9ploiement transparent d'un workflow Nextflow dans le cloud, en d\u00e9chargeant les ex\u00e9cutions de processus en tant que t\u00e2ches Batch.</p> <p>Une fois que l'environnement Batch est configur\u00e9, sp\u00e9cifiez les types d'instances \u00e0 utiliser et le nombre maximum de CPU \u00e0 allouer, vous devez cr\u00e9er un fichier de configuration Nextflow comme celui pr\u00e9sent\u00e9 ci-dessous :</p> <p>Cliquez sur les ic\u00f4nes :material-plus-circle : dans le code pour obtenir des explications.</p> <pre><code>process.executor = 'awsbatch' // (1)!\nprocess.queue = 'nextflow-ci' // (2)!\nprocess.container = 'nextflow/rnaseq-nf:latest' // (3)!\nworkDir = 's3://nextflow-ci/work/' // (4)!\naws.region = 'eu-west-1' // (5)!\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // (6)!\n</code></pre> <ol> <li>D\u00e9finir AWS Batch comme l'ex\u00e9cuteur pour ex\u00e9cuter les processus dans le workflow</li> <li>Le nom de la file d'attente informatique d\u00e9finie dans l'environnement Batch</li> <li>L'image du container Docker \u00e0 utiliser pour ex\u00e9cuter chaque t\u00e2che</li> <li>Le r\u00e9pertoire de travail du workflow doit \u00eatre un bucket AWS S3.</li> <li>La r\u00e9gion AWS \u00e0 utiliser</li> <li>Chemin d'acc\u00e8s \u00e0 l'outil AWS cli n\u00e9cessaire pour t\u00e9l\u00e9charger des fichiers vers/depuis le container.</li> </ol> <p>Astuce</p> <p>La meilleure pratique consiste \u00e0 conserver ce param\u00e8tre en tant que profil distinct dans le fichier de configuration de votre flux de travail. Cela permet de l'ex\u00e9cuter \u00e0 l'aide d'une simple commande.</p> <pre><code>nextflow run script7.nf -profile amazon\n</code></pre> <p>Les d\u00e9tails complets sur le d\u00e9ploiement par lots d'AWS sont disponibles sur ce lien.</p>"},{"location":"basic_training/executors.fr/#montages-des-volumes","title":"Montages des volumes","text":"<p>Les volumes Elastic Block Storage (EBS) (ou tout autre type de stockage pris en charge) peuvent \u00eatre mont\u00e9s dans le Container de t\u00e2ches \u00e0 l'aide de l'extrait de configuration suivant :</p> <pre><code>aws {\nbatch {\nvolumes = '/some/path'\n}\n}\n</code></pre> <p>Plusieurs volumes peuvent \u00eatre sp\u00e9cifi\u00e9s en utilisant des chemins d'acc\u00e8s s\u00e9par\u00e9s par des virgules. La syntaxe habituelle de montage de volume de Docker peut \u00eatre utilis\u00e9e pour d\u00e9finir des volumes complexes pour lesquels le chemin du conteneur est diff\u00e9rent du chemin de l'h\u00f4te ou pour sp\u00e9cifier une option de lecture seule :</p> <pre><code>aws {\nregion = 'eu-west-1'\nbatch {\nvolumes = ['/tmp', '/host/path:/mnt/path:ro']\n}\n}\n</code></pre> <p>Astuce</p> <p>Il s'agit d'une configuration globale qui doit \u00eatre sp\u00e9cifi\u00e9e dans un fichier de configuration Nextflow et qui sera appliqu\u00e9e \u00e0 toutes les ex\u00e9cutions de processus.</p> <p>Avertissement</p> <p>Nextflow s'attend \u00e0 ce que les chemins d'acc\u00e8s soient disponibles. Il ne g\u00e8re pas la mise \u00e0 disposition de volumes EBS ou d'un autre type de stockage.</p>"},{"location":"basic_training/executors.fr/#definition-des-taches-personnalisees","title":"D\u00e9finition des t\u00e2ches personnalis\u00e9es","text":"<p>Nextflow cr\u00e9e automatiquement les Batch definitions de taches n\u00e9cessaires \u00e0 l'ex\u00e9cution de vos processus de workflow. Il n'est donc pas n\u00e9cessaire de les d\u00e9finir avant d'ex\u00e9cuter votre workflow.</p> <p>Cependant, vous pouvez toujours avoir besoin de sp\u00e9cifier une d\u00e9finition de travail personnalis\u00e9e pour permettre un contr\u00f4le fin des param\u00e8tres de configuration d'un travail sp\u00e9cifique (par exemple, pour d\u00e9finir des chemins de montage personnalis\u00e9s ou d'autres param\u00e8tres sp\u00e9ciaux d'un batch de taches).</p> <p>Pour utiliser votre propre d\u00e9finition de travail dans un workflow Nextflow, utilisez-la \u00e0 la place du nom de l'image du conteneur, en la pr\u00e9fixant avec la cha\u00eene <code>job-definition://</code>. Par exemple :</p> <pre><code>process {\ncontainer = 'job-definition://your-job-definition-name'\n}\n</code></pre>"},{"location":"basic_training/executors.fr/#image-personnalisee","title":"Image personnalis\u00e9e","text":"<p>Comme Nextflow exige que l'outil AWS CLI soit accessible dans l'environnement informatique, une solution courante consiste \u00e0 cr\u00e9er une Amazon Machine Image (AMI) personnalis\u00e9e et \u00e0 l'installer de mani\u00e8re autonome (par exemple \u00e0 l'aide du gestionnaire de paquets Conda).</p> <p>Avertissement</p> <p>Lorsque vous cr\u00e9ez votre AMI personnalis\u00e9e pour AWS Batch, assurez-vous d'utiliser l'AMI Amazon ECS-Optimized Amazon Linux comme image de base.</p> <p>L'extrait suivant montre comment installer AWS CLI avec Miniconda :</p> <pre><code>sudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Remarque</p> <p>L'outil <code>aws</code> sera plac\u00e9 dans un r\u00e9pertoire nomm\u00e9 <code>bin</code> dans le dossier d'installation principal. Les outils ne fonctionneront pas correctement si vous modifiez la structure de ce r\u00e9pertoire apr\u00e8s l'installation.</p> <p>Enfin, sp\u00e9cifiez le chemin complet <code>aws</code> dans le fichier de configuration de Nextflow comme indiqu\u00e9 ci-dessous :</p> <pre><code>aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n</code></pre>"},{"location":"basic_training/executors.fr/#lancer-le-modele","title":"Lancer le mod\u00e8le","text":"<p>Une autre approche consiste \u00e0 cr\u00e9er une AMI personnalis\u00e9e \u00e0 l'aide d'un mod\u00e8le de lancement qui installe l'outil AWS CLI lors du d\u00e9marrage de l'instance via des donn\u00e9es utilisateur personnalis\u00e9es.</p> <p>Dans le tableau de bord EC2, cr\u00e9ez un mod\u00e8le de lancement en sp\u00e9cifiant le champ de donn\u00e9es de l'utilisateur :</p> <pre><code>MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n##!/bin/sh\n### install required deps\nset -x\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\n\n### install awscli\nUSER=/home/ec2-user\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n\n--//--\n</code></pre> <p>Cr\u00e9ez ensuite un nouvel environnement informatique dans le tableau de bord Batch et indiquez le mod\u00e8le de lancement nouvellement cr\u00e9\u00e9 dans le champ correspondant.</p>"},{"location":"basic_training/executors.fr/#deploiements-hybrides","title":"D\u00e9ploiements hybrides","text":"<p>Nextflow permet l'utilisation de plusieurs ex\u00e9cuteurs dans la m\u00eame application de workflow. Cette fonctionnalit\u00e9 permet de d\u00e9ployer des charges de travail hybrides dans lesquelles certains travaux sont ex\u00e9cut\u00e9s sur l'ordinateur local ou le cluster de calcul local, et d'autres travaux sont d\u00e9charg\u00e9s sur le service AWS Batch.</p> <p>Pour activer cette fonctionnalit\u00e9, utilisez un ou plusieurs selecteur de processes dans votre fichier de configuration Nextflow.</p> <p>Par exemple, appliquez la configuration AWS Batch uniquement \u00e0 un sous-ensemble de processus dans votre flux de travail. Vous pouvez essayer ce qui suit :</p> <pre><code>process {\nexecutor = 'slurm' // (1)!\nqueue = 'short' // (2)!\nwithLabel: bigTask {  // (3)!\nexecutor = 'awsbatch' // (4)!\nqueue = 'my-batch-queue' // (5)!\ncontainer = 'my/image:tag' // (6)!\n}\n}\naws {\nregion = 'eu-west-1' // (7)!\n}\n</code></pre> <ol> <li>D\u00e9finir <code>slurm</code> comme ex\u00e9cuteur par d\u00e9faut</li> <li>D\u00e9finir la file d'attente pour le cluster SLURM</li> <li>Mise en place de processus avec l'\u00e9tiquette <code>bigTask</code></li> <li>D\u00e9finir <code>awsbatch</code> comme l'ex\u00e9cuteur pour le(s) processus avec le label <code>bigTask</code>.</li> <li>D\u00e9finir la file d'attente pour le(s) processus avec le label <code>bigTask</code>.</li> <li>D\u00e9finir l'image du container \u00e0 d\u00e9ployer pour le(s) processus avec le label <code>bigTask</code>.</li> <li>D\u00e9finir la r\u00e9gion pour l'ex\u00e9cution par batch</li> </ol>"},{"location":"basic_training/executors/","title":"Deployment scenarios","text":"<p>Real-world genomic applications can spawn the execution of thousands of tasks. In this scenario a batch scheduler is commonly used to deploy a workflow in a computing cluster, allowing the execution of many jobs in parallel across many compute nodes.</p> <p>Nextflow has built-in support for the most commonly used batch schedulers, such as Univa Grid Engine, SLURM and IBM LSF. Check the Nextflow documentation for the complete list of supported execution platforms.</p>"},{"location":"basic_training/executors/#cluster-deployment","title":"Cluster deployment","text":"<p>A key Nextflow feature is the ability to decouple the workflow implementation from the actual execution platform. The implementation of an abstraction layer allows the deployment of the resulting workflow on any executing platform supported by the framework.</p> <p></p> <p>To run your workflow with a batch scheduler, modify the <code>nextflow.config</code> file specifying the target executor and the required computing resources if needed. For example:</p> <pre><code>process.executor = 'slurm'\n</code></pre>"},{"location":"basic_training/executors/#managing-cluster-resources","title":"Managing cluster resources","text":"<p>When using a batch scheduler, it is often needed to specify the number of resources (i.e. cpus, memory, execution time, etc.) required by each task.</p> <p>This can be done using the following process directives:</p> queue the cluster queue to be used for the computation cpus the number of cpus to be allocated for a task execution memory the amount of memory to be allocated for a task execution time the max amount of time to be allocated for a task execution disk the amount of disk storage required for a task execution"},{"location":"basic_training/executors/#workflow-wide-resources","title":"Workflow wide resources","text":"<p>Use the scope <code>process</code> to define the resource requirements for all processes in your workflow applications. For example:</p> <pre><code>process {\nexecutor = 'slurm'\nqueue = 'short'\nmemory = '10 GB'\ntime = '30 min'\ncpus = 4\n}\n</code></pre>"},{"location":"basic_training/executors/#submit-nextflow-as-a-job","title":"Submit Nextflow as a job","text":"<p>Whilst the main Nextflow command can be launched on the login / head node of a cluster, be aware that the node must be set up for commands that run for a long time, even if the compute resources used are negligible. Another option is to submit the main Nextflow process as a job on the cluster instead.</p> <p>Note</p> <p>This requires your cluster configuration to allow jobs be launched from worker nodes, as Nextflow will submit new tasks and manage them from here.</p> <p>For example, if your cluster uses Slurm as a job scheduler, you could create a file similar to the one below:</p> launch_nf.sh<pre><code>#!/bin/bash\n#SBATCH --partition WORK\n#SBATCH --mem 5G\n#SBATCH -c 1\n#SBATCH -t 12:00:00\nWORKFLOW=$1\nCONFIG=$2\n# Use a conda environment where you have installed Nextflow\n# (may not be needed if you have installed it in a different way)\nconda activate nextflow\n\nnextflow -C ${CONFIG} run ${WORKFLOW}\n</code></pre> <p>And then submit it with:</p> <pre><code>sbatch launch_nf.sh /home/my_user/path/my_workflow.nf /home/my_user/path/my_config_file.conf\n</code></pre> <p>You can find more details about the example above here. You can find more tips for running Nextflow on HPC in the following blog posts:</p> <ul> <li>5 Nextflow Tips for HPC Users</li> <li>Five more tips for Nextflow user on HPC</li> </ul>"},{"location":"basic_training/executors/#configure-process-by-name","title":"Configure process by name","text":"<p>In real-world applications, different tasks need different amounts of computing resources. It is possible to define the resources for a specific task using the select <code>withName:</code> followed by the process name:</p> <pre><code>process {\nexecutor = 'slurm'\nqueue = 'short'\nmemory = '10 GB'\ntime = '30 min'\ncpus = 4\nwithName: FOO {\ncpus = 2\nmemory = '20 GB'\nqueue = 'short'\n}\nwithName: BAR {\ncpus = 4\nmemory = '32 GB'\nqueue = 'long'\n}\n}\n</code></pre> <p>Exercise</p> <p>Run the RNA-Seq script (<code>script7.nf</code>) from earlier, but specify that the <code>QUANTIFICATION</code> process requires 2 CPUs and 5 GB of memory, within the <code>nextflow.config</code> file.</p> Solution <pre><code>process {\nwithName: QUANTIFICATION {\ncpus = 2\nmemory = '5 GB'\n}\n}\n</code></pre>"},{"location":"basic_training/executors/#configure-process-by-labels","title":"Configure process by labels","text":"<p>When a workflow application is composed of many processes, listing all of the process names and choosing resources for each of them in the configuration file can be difficult.</p> <p>A better strategy consists of annotating the processes with a label directive. Then specify the resources in the configuration file used for all processes having the same label.</p> <p>The workflow script:</p> <pre><code>process TASK1 {\nlabel 'long'\nscript:\n\"\"\"\n    first_command --here\n    \"\"\"\n}\nprocess TASK2 {\nlabel 'short'\nscript:\n\"\"\"\n    second_command --here\n    \"\"\"\n}\n</code></pre> <p>The configuration file:</p> <pre><code>process {\nexecutor = 'slurm'\nwithLabel: 'short' {\ncpus = 4\nmemory = '20 GB'\nqueue = 'alpha'\n}\nwithLabel: 'long' {\ncpus = 8\nmemory = '32 GB'\nqueue = 'omega'\n}\n}\n</code></pre>"},{"location":"basic_training/executors/#configure-multiple-containers","title":"Configure multiple containers","text":"<p>Containers can be set for each process in your workflow. You can define their containers in a config file as shown below:</p> <pre><code>process {\nwithName: FOO {\ncontainer = 'some/image:x'\n}\nwithName: BAR {\ncontainer = 'other/image:y'\n}\n}\ndocker.enabled = true\n</code></pre> <p>Tip</p> <p>Should I use a single fat container or many slim containers? Both approaches have pros &amp; cons. A single container is simpler to build and maintain, however when using many tools the image can become very big and tools can create conflicts with each other. Using a container for each process can result in many different images to build and maintain, especially when processes in your workflow use different tools for each task.</p> <p>Read more about config process selectors at this link.</p>"},{"location":"basic_training/executors/#configuration-profiles","title":"Configuration profiles","text":"<p>Configuration files can contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated/chosen when launching a workflow execution by using the <code>-profile</code> command- line option.</p> <p>Configuration profiles are defined by using the special scope <code>profiles</code> which group the attributes that belong to the same profile using a common prefix. For example:</p> <pre><code>profiles {\nstandard {\nparams.genome = '/local/path/ref.fasta'\nprocess.executor = 'local'\n}\ncluster {\nparams.genome = '/data/stared/ref.fasta'\nprocess.executor = 'sge'\nprocess.queue = 'long'\nprocess.memory = '10 GB'\nprocess.conda = '/some/path/env.yml'\n}\ncloud {\nparams.genome = '/data/stared/ref.fasta'\nprocess.executor = 'awsbatch'\nprocess.container = 'cbcrg/imagex'\ndocker.enabled = true\n}\n}\n</code></pre> <p>This configuration defines three different profiles: <code>standard</code>, <code>cluster</code> and <code>cloud</code> that set different process configuration strategies depending on the target runtime platform. By convention, the <code>standard</code> profile is implicitly used when no other profile is specified by the user.</p> <p>To enable a specific profile use <code>-profile</code> option followed by the profile name:</p> <pre><code>nextflow run &lt;your script&gt; -profile cluster\n</code></pre> <p>Tip</p> <p>Two or more configuration profiles can be specified by separating the profile names with a comma character:</p> <pre><code>nextflow run &lt;your script&gt; -profile standard,cloud\n</code></pre>"},{"location":"basic_training/executors/#cloud-deployment","title":"Cloud deployment","text":"<p>AWS Batch is a managed computing service that allows the execution of containerized workloads in the Amazon cloud infrastructure.</p> <p>Nextflow provides built-in support for AWS Batch which allows the seamless deployment of a Nextflow workflow in the cloud, offloading the process executions as Batch jobs.</p> <p>Once the Batch environment is configured, specify the instance types to be used and the max number of CPUs to be allocated, you need to create a Nextflow configuration file like the one shown below:</p> <p>Click the  icons in the code for explanations.</p> <pre><code>process.executor = 'awsbatch' // (1)!\nprocess.queue = 'nextflow-ci' // (2)!\nprocess.container = 'nextflow/rnaseq-nf:latest' // (3)!\nworkDir = 's3://nextflow-ci/work/' // (4)!\naws.region = 'eu-west-1' // (5)!\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // (6)!\n</code></pre> <ol> <li>Set AWS Batch as the executor to run the processes in the workflow</li> <li>The name of the computing queue defined in the Batch environment</li> <li>The Docker container image to be used to run each job</li> <li>The workflow work directory must be a AWS S3 bucket</li> <li>The AWS region to be used</li> <li>The path of the AWS cli tool required to download/upload files to/from the container</li> </ol> <p>Tip</p> <p>The best practice is to keep this setting as a separate profile in your workflow config file. This allows the execution with a simple command.</p> <pre><code>nextflow run script7.nf -profile amazon\n</code></pre> <p>The complete details about AWS Batch deployment are available at this link.</p>"},{"location":"basic_training/executors/#volume-mounts","title":"Volume mounts","text":"<p>Elastic Block Storage (EBS) volumes (or other supported storage) can be mounted in the job container using the following configuration snippet:</p> <pre><code>aws {\nbatch {\nvolumes = '/some/path'\n}\n}\n</code></pre> <p>Multiple volumes can be specified using comma-separated paths. The usual Docker volume mount syntax can be used to define complex volumes for which the container path is different from the host path or to specify a read-only option:</p> <pre><code>aws {\nregion = 'eu-west-1'\nbatch {\nvolumes = ['/tmp', '/host/path:/mnt/path:ro']\n}\n}\n</code></pre> <p>Tip</p> <p>This is a global configuration that has to be specified in a Nextflow config file and will be applied to all process executions.</p> <p>Warning</p> <p>Nextflow expects paths to be available. It does not handle the provision of EBS volumes or another kind of storage.</p>"},{"location":"basic_training/executors/#custom-job-definition","title":"Custom job definition","text":"<p>Nextflow automatically creates the Batch Job definitions needed to execute your workflow processes. Therefore it\u2019s not required to define them before you run your workflow.</p> <p>However, you may still need to specify a custom Job Definition to provide fine-grained control of the configuration settings of a specific job (e.g. to define custom mount paths or other special settings of a Batch Job).</p> <p>To use your own job definition in a Nextflow workflow, use it in place of the container image name, prefixing it with the <code>job-definition://</code> string. For example:</p> <pre><code>process {\ncontainer = 'job-definition://your-job-definition-name'\n}\n</code></pre>"},{"location":"basic_training/executors/#custom-image","title":"Custom image","text":"<p>Since Nextflow requires the AWS CLI tool to be accessible in the computing environment, a common solution consists of creating a custom Amazon Machine Image (AMI) and installing it in a self-contained manner (e.g. using Conda package manager).</p> <p>Warning</p> <p>When creating your custom AMI for AWS Batch, make sure to use the Amazon ECS-Optimized Amazon Linux AMI as the base image.</p> <p>The following snippet shows how to install AWS CLI with Miniconda:</p> <pre><code>sudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Note</p> <p>The <code>aws</code> tool will be placed in a directory named <code>bin</code> in the main installation folder. The tools will not work properly if you modify this directory     structure after the installation.</p> <p>Finally, specify the <code>aws</code> full path in the Nextflow config file as shown below:</p> <pre><code>aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n</code></pre>"},{"location":"basic_training/executors/#launch-template","title":"Launch template","text":"<p>An alternative approach to is to create a custom AMI using a Launch template that installs the AWS CLI tool during the instance boot via custom user data.</p> <p>In the EC2 dashboard, create a Launch template specifying the user data field:</p> <pre><code>MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n##!/bin/sh\n### install required deps\nset -x\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\n\n### install awscli\nUSER=/home/ec2-user\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n\n--//--\n</code></pre> <p>Then create a new compute environment in the Batch dashboard and specify the newly created launch template in the corresponding field.</p>"},{"location":"basic_training/executors/#hybrid-deployments","title":"Hybrid deployments","text":"<p>Nextflow allows the use of multiple executors in the same workflow application. This feature enables the deployment of hybrid workloads in which some jobs are executed on the local computer or local computing cluster, and some jobs are offloaded to the AWS Batch service.</p> <p>To enable this feature use one or more process selectors in your Nextflow configuration file.</p> <p>For example, apply the AWS Batch configuration only to a subset of processes in your workflow. You can try the following:</p> <pre><code>process {\nexecutor = 'slurm' // (1)!\nqueue = 'short' // (2)!\nwithLabel: bigTask {  // (3)!\nexecutor = 'awsbatch' // (4)!\nqueue = 'my-batch-queue' // (5)!\ncontainer = 'my/image:tag' // (6)!\n}\n}\naws {\nregion = 'eu-west-1' // (7)!\n}\n</code></pre> <ol> <li>Set <code>slurm</code> as the default executor</li> <li>Set the queue for the SLURM cluster</li> <li>Setting of process(es) with the label <code>bigTask</code></li> <li>Set <code>awsbatch</code> as the executor for the process(es) with the <code>bigTask</code> label</li> <li>Set the queue for the process(es) with the <code>bigTask</code> label</li> <li>Set the container image to deploy for the process(es) with the <code>bigTask</code> label</li> <li>Define the region for Batch execution</li> </ol>"},{"location":"basic_training/executors.pt/","title":"Cen\u00e1rios de implanta\u00e7\u00e3o","text":"<p>Aplica\u00e7\u00f5es gen\u00f4micas do mundo real podem gerar milhares de tarefas sendo executadas. Nesse cen\u00e1rio, um escalonador de lote (batch scheduler) \u00e9 comumente usado para implantar um fluxo de trabalho em um cluster de computa\u00e7\u00e3o, permitindo a execu\u00e7\u00e3o de muitos trabalhos em paralelo em muitos n\u00f3s de computa\u00e7\u00e3o.</p> <p>O Nextflow possui suporte embutido para os escalonadores de lote mais usados, como o Univa Grid Engine, o SLURM e o IBM LSF. Verifique a documenta\u00e7\u00e3o do Nextflow para obter a lista completa dos ambientes de computa\u00e7\u00e3o.</p>"},{"location":"basic_training/executors.pt/#implantacao-em-cluster","title":"Implanta\u00e7\u00e3o em cluster","text":"<p>Um recurso importante do Nextflow \u00e9 a capacidade de desacoplar a implementa\u00e7\u00e3o do fluxo de trabalho da plataforma de execu\u00e7\u00e3o de fato. A implementa\u00e7\u00e3o de uma camada de abstra\u00e7\u00e3o permite a implanta\u00e7\u00e3o do fluxo de trabalho resultante em qualquer plataforma de execu\u00e7\u00e3o suportada pelo framework.</p> <p></p> <p>Para executar seu fluxo de trabalho com um escalonador de lote, modifique o arquivo <code>nextflow.config</code> especificando o executor de destino e os recursos de computa\u00e7\u00e3o necess\u00e1rios, se necess\u00e1rio. Por exemplo:</p> <pre><code>process.executor = 'slurm'\n</code></pre>"},{"location":"basic_training/executors.pt/#gerenciando-recursos-do-cluster","title":"Gerenciando recursos do cluster","text":"<p>Ao usar um escalonador de lote, geralmente \u00e9 necess\u00e1rio especificar o n\u00famero de recursos (ou seja, CPU, mem\u00f3ria, tempo de execu\u00e7\u00e3o etc.) necess\u00e1rios para cada tarefa.</p> <p>Isso pode ser feito utilizando as seguintes diretivas de processo:</p> queue a fila a ser utilizada no cluster para computa\u00e7\u00e3o cpus o n\u00famero de cpus a serem alocadas para execu\u00e7\u00e3o da tarefa memory a quantidade de mem\u00f3ria a ser alocada para execu\u00e7\u00e3o da tarefa time a quantidade de tempo m\u00e1xima a ser alocada para execu\u00e7\u00e3o da tarefa disk a quantidade de espa\u00e7o de armazenamento necess\u00e1ria para a execu\u00e7\u00e3o da tarefa"},{"location":"basic_training/executors.pt/#recursos-do-fluxo-de-trabalho-de-modo-amplo","title":"Recursos do fluxo de trabalho de modo amplo","text":"<p>Use o escopo <code>process</code> para definir os requisitos de recursos para todos os processos em suas aplica\u00e7\u00f5es de fluxo de trabalho. Por exemplo:</p> <pre><code>process {\nexecutor = 'slurm'\nqueue = 'curta'\nmemory = '10 GB'\ntime = '30 min'\ncpus = 4\n}\n</code></pre>"},{"location":"basic_training/executors.pt/#submeta-o-nextflow-como-uma-tarefa","title":"Submeta o Nextflow como uma tarefa","text":"<p>Embora o comando principal do Nextflow possa ser iniciado no n\u00f3 de login/head de um cluster, esteja ciente de que o n\u00f3 deve ser configurado para comandos que s\u00e3o executados por um longo per\u00edodo de tempo, mesmo que os recursos computacionais usados sejam insignificantes. Outra op\u00e7\u00e3o \u00e9 enviar o processo principal do Nextflow como uma tarefa no cluster.</p> <p>Note</p> <p>Isso requer a configura\u00e7\u00e3o do seu cluster para permitir que as tarefas sejam iniciadas a partir dos n\u00f3s de trabalho, pois o Nextflow enviar\u00e1 novas tarefas e as gerenciar\u00e1 a partir daqui.</p> <p>Por exemplo, se seu cluster usa Slurm como escalonador de tarefas, voc\u00ea pode criar um arquivo semelhante ao abaixo:</p> launch_nf.sh<pre><code>#!/bin/bash\n#SBATCH --partition TRABALHO\n#SBATCH --mem 5G\n#SBATCH -c 1\n#SBATCH -t 12:00:00\nFLUXO_DE_TRABALHO=$1\nCONFIG=$2\n# Use um ambiente conda onde voc\u00ea instalou o Nextflow\n# (pode n\u00e3o ser necess\u00e1rio se voc\u00ea o tiver instalado de uma maneira diferente)\nconda activate nextflow\n\nnextflow -C ${CONFIG} run ${FLUXO_DE_TRABALHO}\n</code></pre> <p>E, em seguida, submeta-o com:</p> <pre><code>sbatch launch_nf.sh /home/meu_usuario/caminho/meu_fluxo_de_trabalho.nf /home/meu_usuario/caminho/meu_arquivo_configuracao.conf\n</code></pre> <p>Voc\u00ea pode encontrar mais detalhes sobre o exemplo acima aqui. Voc\u00ea tamb\u00e9m poder\u00e1 encontrar mais dicas de como executar o Nextflow em HPC nos seguintes posts de blog:</p> <ul> <li>5 Nextflow Tips for HPC Users</li> <li>Five more tips for Nextflow user on HPC</li> </ul>"},{"location":"basic_training/executors.pt/#configure-processos-por-nome","title":"Configure processos por nome","text":"<p>Em aplica\u00e7\u00f5es do mundo real, diferentes tarefas precisam de diferentes quantidades de recursos de computa\u00e7\u00e3o. \u00c9 poss\u00edvel definir os recursos para uma tarefa espec\u00edfica usando o seletor <code>withName:</code> seguido do nome do processo:</p> <pre><code>process {\nexecutor = 'slurm'\nqueue = 'curta'\nmemory = '10 GB'\ntime = '30 min'\ncpus = 4\nwithName: FOO {\ncpus = 2\nmemory = '20 GB'\nqueue = 'curta'\n}\nwithName: BAR {\ncpus = 4\nmemory = '32 GB'\nqueue = 'longa'\n}\n}\n</code></pre> <p>Exercise</p> <p>Execute o script RNA-Seq (<code>script7.nf</code>) visto na se\u00e7\u00e3o de RNAseq, mas especifique dentro do arquivo <code>nextflow.config</code> que o processo de quantifica\u00e7\u00e3o (<code>QUANTIFICATION</code>) requer 2 CPUs e 5 GB de mem\u00f3ria.</p> Solution <pre><code>process {\nwithName: QUANTIFICATION {\ncpus = 2\nmemory = '5 GB'\n}\n}\n</code></pre>"},{"location":"basic_training/executors.pt/#configure-processos-por-rotulos","title":"Configure processos por r\u00f3tulos","text":"<p>Quando uma aplica\u00e7\u00e3o de fluxo de trabalho \u00e9 composta por muitos processos, pode ser dif\u00edcil listar todos os nomes de processos e escolher recursos para cada um deles no arquivo de configura\u00e7\u00e3o.</p> <p>Uma melhor estrat\u00e9gia consiste em anotar os processos com uma diretiva de r\u00f3tulo (<code>label</code>). Em seguida, especifique os recursos no arquivo de configura\u00e7\u00e3o usados para todos os processos com o mesmo r\u00f3tulo.</p> <p>O script do fluxo de trabalho:</p> <pre><code>process TAREFA1 {\nlabel 'longo'\nscript:\n\"\"\"\n    primeiro_comando --aqui\n    \"\"\"\n}\nprocess TAREFA2 {\nlabel 'curto'\nscript:\n\"\"\"\n    segundo_comando --aqui\n    \"\"\"\n}\n</code></pre> <p>O arquivo de configura\u00e7\u00e3o:</p> <pre><code>process {\nexecutor = 'slurm'\nwithLabel: 'curto' {\ncpus = 4\nmemory = '20 GB'\nqueue = 'alfa'\n}\nwithLabel: 'longo' {\ncpus = 8\nmemory = '32 GB'\nqueue = 'omega'\n}\n}\n</code></pre>"},{"location":"basic_training/executors.pt/#configure-varios-conteineres","title":"Configure v\u00e1rios cont\u00eaineres","text":"<p>Os cont\u00eaineres podem ser definidos para cada processo em seu fluxo de trabalho. Voc\u00ea pode definir seus cont\u00eaineres em um arquivo de configura\u00e7\u00e3o conforme mostrado abaixo:</p> <pre><code>process {\nwithName: FOO {\ncontainer = 'uma/imagem:x'\n}\nwithName: BAR {\ncontainer = 'outra/imagem:y'\n}\n}\ndocker.enabled = true\n</code></pre> <p>Tip</p> <p>Devo usar um \u00fanico cont\u00eainer pesado ou muitos cont\u00eaineres leves? Ambas as abordagens t\u00eam pr\u00f3s e contras. Um \u00fanico cont\u00eainer \u00e9 mais simples de construir e manter, por\u00e9m ao usar muitas ferramentas a imagem pode ficar muito grande e as ferramentas podem criar conflitos umas com as outras. O uso de um cont\u00eainer para cada processo pode resultar em muitas imagens diferentes para criar e manter, especialmente quando os processos em seu fluxo de trabalho usam ferramentas diferentes para cada tarefa.</p> <p>Leia mais sobre seletores de processo de configura\u00e7\u00e3o neste link.</p>"},{"location":"basic_training/executors.pt/#perfis-de-configuracao","title":"Perfis de configura\u00e7\u00e3o","text":"<p>Os arquivos de configura\u00e7\u00e3o podem conter a defini\u00e7\u00e3o de um ou mais perfis. Um perfil \u00e9 um conjunto de atributos de configura\u00e7\u00e3o que podem ser ativados/escolhidos ao lan\u00e7ar a execu\u00e7\u00e3o de um fluxo de trabalho usando a op\u00e7\u00e3o de linha de comando <code>-profile</code>.</p> <p>Os perfis de configura\u00e7\u00e3o s\u00e3o definidos usando o escopo especial <code>profiles</code> que agrupa os atributos que pertencem ao mesmo perfil usando um prefixo comum. Por exemplo:</p> <pre><code>profiles {\nstandard {\nparams.genoma = '/local/caminho/ref.fasta'\nprocess.executor = 'local'\n}\ncluster {\nparams.genoma = '/data/stared/ref.fasta'\nprocess.executor = 'sge'\nprocess.queue = 'longa'\nprocess.memory = '10 GB'\nprocess.conda = '/um/caminho/ambiente.yml'\n}\nnuvem {\nparams.genoma = '/data/stared/ref.fasta'\nprocess.executor = 'awsbatch'\nprocess.container = 'cbcrg/imagex'\ndocker.enabled = true\n}\n}\n</code></pre> <p>Essa configura\u00e7\u00e3o define tr\u00eas perfis diferentes: <code>standard</code>, <code>cluster</code> e <code>nuvem</code> que definem diferentes estrat\u00e9gias de configura\u00e7\u00e3o de processo dependendo da plataforma de tempo de execu\u00e7\u00e3o de destino. Por conven\u00e7\u00e3o, o perfil <code>standard</code> \u00e9 usado implicitamente quando nenhum outro perfil \u00e9 especificado pelo usu\u00e1rio.</p> <p>Para ativar um perfil espec\u00edfico, use a op\u00e7\u00e3o <code>-profile</code> seguida do nome do perfil:</p> <pre><code>nextflow run &lt;seu script&gt; -profile cluster\n</code></pre> <p>Tip</p> <p>Dois ou mais perfis de configura\u00e7\u00e3o podem ser especificados separando os nomes dos perfis com uma v\u00edrgula:</p> <pre><code>nextflow run &lt;seu script&gt; -profile standard,nuvem\n</code></pre>"},{"location":"basic_training/executors.pt/#implantacao-na-nuvem","title":"Implanta\u00e7\u00e3o na nuvem","text":"<p>AWS Batch \u00e9 um servi\u00e7o de computa\u00e7\u00e3o gerenciada que permite a execu\u00e7\u00e3o de cargas de trabalho em cont\u00eaineres na infraestrutura de nuvem da Amazon.</p> <p>O Nextflow fornece suporte embutido para o AWS Batch, que permite uma implanta\u00e7\u00e3o simples de um fluxo de trabalho do Nextflow na nuvem, descarregando as execu\u00e7\u00f5es de processo como trabalhos em lote.</p> <p>Uma vez que o ambiente Batch esteja configurado, especifique os tipos de inst\u00e2ncia a serem usadas e o n\u00famero m\u00e1ximo de CPUs a serem alocadas. Voc\u00ea precisa criar um arquivo de configura\u00e7\u00e3o do Nextflow como o mostrado abaixo:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>process.executor = 'awsbatch' // (1)!\nprocess.queue = 'nextflow-ci' // (2)!\nprocess.container = 'nextflow/rnaseq-nf:latest' // (3)!\nworkDir = 's3://nextflow-ci/work/' // (4)!\naws.region = 'eu-west-1' // (5)!\naws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws' // (6)!\n</code></pre> <ol> <li>Defina o AWS Batch como o executor para executar os processos no fluxo de trabalho</li> <li>O nome da fila de computa\u00e7\u00e3o definida no ambiente Batch</li> <li>A imagem do cont\u00eainer do Docker a ser usada para executar cada trabalho</li> <li>O diret\u00f3rio de trabalho do fluxo de trabalho deve ser um bucket AWS S3</li> <li>A regi\u00e3o da AWS a ser usada</li> <li>O caminho da ferramenta de linha de comando da AWS necess\u00e1ria para fazer download/upload de arquivos de/para o cont\u00eainer</li> </ol> <p>Tip</p> <p>A pr\u00e1tica recomendada \u00e9 manter essa configura\u00e7\u00e3o como um perfil separado no arquivo de configura\u00e7\u00e3o do fluxo de trabalho. Isso permite a execu\u00e7\u00e3o com um comando simples.</p> <pre><code>nextflow run script7.nf -profile amazon\n</code></pre> <p>Os detalhes completos sobre a implanta\u00e7\u00e3o no AWS Batch est\u00e3o dispon\u00edveis nesse link.</p>"},{"location":"basic_training/executors.pt/#montagens-de-volume","title":"Montagens de volume","text":"<p>Elastic Block Storage (EBS) (ou outras formas de armazenamento suportadas) podem ser montados no cont\u00eainer da tarefa usando a seguinte configura\u00e7\u00e3o:</p> <pre><code>aws {\nbatch {\nvolumes = '/algum/caminho'\n}\n}\n</code></pre> <p>V\u00e1rios volumes podem ser especificados usando caminhos separados por v\u00edrgulas. A sintaxe usual de montagem de volume do Docker pode ser usada para definir volumes complexos para os quais o caminho do cont\u00eainer \u00e9 diferente do caminho do hospedeiro ou para especificar uma op\u00e7\u00e3o somente leitura:</p> <pre><code>aws {\nregion = 'eu-west-1'\nbatch {\nvolumes = ['/tmp', '/caminho/no/hospedeiro:/mnt/caminho:ro']\n}\n}\n</code></pre> <p>Tip</p> <p>Esta \u00e9 uma configura\u00e7\u00e3o global que deve ser especificada em um arquivo de configura\u00e7\u00e3o do Nextflow e ser\u00e1 aplicada a todas as execu\u00e7\u00f5es do processo.</p> <p>Warning</p> <p>O Nextflow espera que os caminhos estejam dispon\u00edveis. Ele n\u00e3o lida com o fornecimento de volumes EBS ou outro tipo de armazenamento.</p>"},{"location":"basic_training/executors.pt/#definicao-de-tarefa-personalizada","title":"Defini\u00e7\u00e3o de tarefa personalizada","text":"<p>O Nextflow cria automaticamente as defini\u00e7\u00f5es de trabalho do Batch necess\u00e1rias para executar seus processos de fluxo de trabalho. Portanto, n\u00e3o \u00e9 necess\u00e1rio defini-las antes de executar seu fluxo de trabalho.</p> <p>No entanto, voc\u00ea ainda pode precisar especificar uma defini\u00e7\u00e3o de tarefa personalizada para fornecer controle refinado das defini\u00e7\u00f5es de configura\u00e7\u00e3o de uma tarefa espec\u00edfica (por exemplo, para definir caminhos de montagem personalizados ou outras configura\u00e7\u00f5es especiais de uma tarefa no Batch).</p> <p>Para usar sua pr\u00f3pria defini\u00e7\u00e3o de tarefa em um fluxo de trabalho do Nextflow, use-a no lugar do nome da imagem do cont\u00eainer, prefixando-a com a string <code>job-definition://</code>. Por exemplo:</p> <pre><code>process {\ncontainer = 'job-definition://o-nome-da-sua-definicao-de-tarefa'\n}\n</code></pre>"},{"location":"basic_training/executors.pt/#imagem-personalizada","title":"Imagem personalizada","text":"<p>Como o Nextflow exige que a ferramenta de linha de comando da AWS esteja acess\u00edvel no ambiente de computa\u00e7\u00e3o, uma solu\u00e7\u00e3o comum consiste em criar uma Amazon Machine Image (AMI) personalizada e instal\u00e1-la de maneira independente (por exemplo, usando o gerenciador de pacotes Conda).</p> <p>Warning</p> <p>Ao criar sua AMI personalizada para o AWS Batch, certifique-se de usar a Amazon ECS-Optimized Amazon Linux AMI como imagem base.</p> <p>O trecho de c\u00f3digo a seguir mostra como instalar a ferramenta de linha de comando da AWS com o Miniconda:</p> <pre><code>sudo yum install -y bzip2 wget\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $HOME/miniconda\n$HOME/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>Note</p> <p>A ferramenta <code>aws</code> ser\u00e1 colocada em um diret\u00f3rio chamado <code>bin</code> na pasta principal de instala\u00e7\u00e3o. As ferramentas n\u00e3o funcionar\u00e3o corretamente se voc\u00ea modificar essa estrutura de diret\u00f3rios ap\u00f3s a instala\u00e7\u00e3o.</p> <p>Por fim, especifique o caminho completo para a <code>aws</code> no arquivo de configura\u00e7\u00e3o do Nextflow, conforme mostrado abaixo:</p> <pre><code>aws.batch.cliPath = '/home/ec2-user/miniconda/bin/aws'\n</code></pre>"},{"location":"basic_training/executors.pt/#modelo-de-lancamento","title":"Modelo de lan\u00e7amento","text":"<p>Uma abordagem alternativa \u00e9 criar uma AMI personalizada usando um modelo de lan\u00e7amento que instala a ferramenta de linha de comando da AWS durante o lan\u00e7amento da inst\u00e2ncia por meio de dados de usu\u00e1rio personalizados.</p> <p>No painel do EC2, crie um modelo de lan\u00e7amento especificando o campo de dados do usu\u00e1rio:</p> <pre><code>MIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"//\"\n--//\nContent-Type: text/x-shellscript; charset=\"us-ascii\"\n##!/bin/sh\n### install required deps\nset -x\nexport PATH=/usr/local/bin:$PATH\nyum install -y jq python27-pip sed wget bzip2\npip install -U boto3\n\n### install awscli\nUSER=/home/ec2-user\nwget -q https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -f -p $USER/miniconda\n$USER/miniconda/bin/conda install -c conda-forge -y awscli\nrm Miniconda3-latest-Linux-x86_64.sh\nchown -R ec2-user:ec2-user $USER/miniconda\n\n--//--\n</code></pre> <p>Em seguida, crie um novo ambiente de computa\u00e7\u00e3o no painel Batch e especifique o modelo de lan\u00e7amento rec\u00e9m-criado no campo correspondente.</p>"},{"location":"basic_training/executors.pt/#implantacao-hibrida","title":"Implanta\u00e7\u00e3o h\u00edbrida","text":"<p>O Nextflow permite o uso de v\u00e1rios executores na mesma aplica\u00e7\u00e3o do fluxo de trabalho. Esse recurso permite a implanta\u00e7\u00e3o de cargas de trabalho h\u00edbridas nas quais algumas tarefas s\u00e3o executados no computador local ou cluster de computa\u00e7\u00e3o local e algumas tarefas s\u00e3o transferidas para o servi\u00e7o AWS Batch.</p> <p>Para ativar esse recurso, use um ou mais seletores de processo em seu arquivo de configura\u00e7\u00e3o do Nextflow.</p> <p>Por exemplo, aplique a configura\u00e7\u00e3o do AWS Batch apenas a um subconjunto de processos em seu fluxo de trabalho. Voc\u00ea pode tentar o seguinte:</p> <pre><code>process {\nexecutor = 'slurm' // (1)!\nqueue = 'curta' // (2)!\nwithLabel: tarefaGrande {  // (3)!\nexecutor = 'awsbatch' // (4)!\nqueue = 'minha-fila-do-batch' // (5)!\ncontainer = 'minha/imagem:tag' // (6)!\n}\n}\naws {\nregion = 'eu-west-1' // (7)!\n}\n</code></pre> <ol> <li>Defina o <code>slurm</code> como o executor padr\u00e3o</li> <li>Defina a fila para o cluster SLURM</li> <li>Configura\u00e7\u00e3o de processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina <code>awsbatch</code> como o executor para o(s) processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina a fila para o(s) processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina a imagem do cont\u00eainer para implantar para o(s) processo(s) com o r\u00f3tulo <code>tarefaGrande</code></li> <li>Defina a regi\u00e3o para execu\u00e7\u00e3o do Batch</li> </ol>"},{"location":"basic_training/groovy/","title":"Groovy basic structures and idioms","text":"<p>Nextflow is a domain specific language (DSL) implemented on top of the Groovy programming language, which in turn is a super-set of the Java programming language. This means that Nextflow can run any Groovy or Java code.</p> <p>Here are some important Groovy syntax that are commonly used in Nextflow.</p>"},{"location":"basic_training/groovy/#printing-values","title":"Printing values","text":"<p>To print something is as easy as using one of the <code>print</code> or <code>println</code> methods.</p> <pre><code>println(\"Hello, World!\")\n</code></pre> <p>The only difference between the two is that the <code>println</code> method implicitly appends a new line character to the printed string.</p> <p>Tip</p> <p>Parentheses for function invocations are optional. Therefore, the following syntax is also valid:</p> <pre><code>println \"Hello, World!\"\n</code></pre>"},{"location":"basic_training/groovy/#comments","title":"Comments","text":"<p>Comments use the same syntax as C-family programming languages:</p> <pre><code>// comment a single line\n/*\n    a comment spanning\n    multiple lines\n*/\n</code></pre>"},{"location":"basic_training/groovy/#variables","title":"Variables","text":"<p>To define a variable, simply assign a value to it:</p> <pre><code>x = 1\nprintln x\nx = new java.util.Date()\nprintln x\nx = -3.1499392\nprintln x\nx = false\nprintln x\nx = \"Hi\"\nprintln x\n</code></pre> <p>Local variables are defined using the <code>def</code> keyword:</p> <pre><code>def x = 'foo'\n</code></pre> <p>The <code>def</code> should be always used when defining variables local to a function or a closure.</p>"},{"location":"basic_training/groovy/#lists","title":"Lists","text":"<p>A List object can be defined by placing the list items in square brackets:</p> <pre><code>list = [10, 20, 30, 40]\n</code></pre> <p>You can access a given item in the list with square-bracket notation (indexes start at <code>0</code>) or using the <code>get</code> method:</p> <pre><code>println list[0]\nprintln list.get(0)\n</code></pre> <p>In order to get the length of a list you can use the <code>size</code> method:</p> <pre><code>println list.size()\n</code></pre> <p>We use the <code>assert</code> keyword to test if a condition is true (similar to an <code>if</code> function). Here, Groovy will print nothing if it is correct, else it will raise an AssertionError message.</p> <pre><code>assert list[0] == 10\n</code></pre> <p>Note</p> <p>This assertion should be correct, try changing it to an incorrect one.</p> <p>Lists can also be indexed with negative indexes and reversed ranges.</p> <pre><code>list = [0, 1, 2]\nassert list[-1] == 2\nassert list[-1..0] == list.reverse()\n</code></pre> <p>Info</p> <p>In the last assert line we are referencing the initial list and converting this with a \"shorthand\" range (<code>..</code>), to run from the -1th element (2) to the 0th element (0).</p> <p>List objects implement all methods provided by the java.util.List interface, plus the extension methods provided by Groovy.</p> <pre><code>assert [1, 2, 3] &lt;&lt; 1 == [1, 2, 3, 1]\nassert [1, 2, 3] + [1] == [1, 2, 3, 1]\nassert [1, 2, 3, 1] - [1] == [2, 3]\nassert [1, 2, 3] * 2 == [1, 2, 3, 1, 2, 3]\nassert [1, [2, 3]].flatten() == [1, 2, 3]\nassert [1, 2, 3].reverse() == [3, 2, 1]\nassert [1, 2, 3].collect { it + 3 } == [4, 5, 6]\nassert [1, 2, 3, 1].unique().size() == 3\nassert [1, 2, 3, 1].count(1) == 2\nassert [1, 2, 3, 4].min() == 1\nassert [1, 2, 3, 4].max() == 4\nassert [1, 2, 3, 4].sum() == 10\nassert [4, 2, 1, 3].sort() == [1, 2, 3, 4]\nassert [4, 2, 1, 3].find { it % 2 == 0 } == 4\nassert [4, 2, 1, 3].findAll { it % 2 == 0 } == [4, 2]\n</code></pre>"},{"location":"basic_training/groovy/#maps","title":"Maps","text":"<p>Maps are like lists that have an arbitrary key instead of an integer. Therefore, the syntax is very much aligned.</p> <pre><code>map = [a: 0, b: 1, c: 2]\n</code></pre> <p>Maps can be accessed in a conventional square-bracket syntax or as if the key was a property of the map.</p> <p>Click the  icons in the code for explanations.</p> <pre><code>assert map['a'] == 0 // (1)!\nassert map.b == 1 // (2)!\nassert map.get('c') == 2 // (3)!\n</code></pre> <ol> <li>Using square brackets.</li> <li>Using dot notation.</li> <li>Using the <code>get</code> method.</li> </ol> <p>To add data or to modify a map, the syntax is similar to adding values to a list:</p> <pre><code>map['a'] = 'x' // (1)!\nmap.b = 'y' // (2)!\nmap.put('c', 'z') // (3)!\nassert map == [a: 'x', b: 'y', c: 'z']\n</code></pre> <ol> <li>Using square brackets.</li> <li>Using dot notation.</li> <li>Using the put method.</li> </ol> <p>Map objects implement all methods provided by the java.util.Map interface, plus the extension methods provided by Groovy.</p>"},{"location":"basic_training/groovy/#string-interpolation","title":"String interpolation","text":"<p>String literals can be defined by enclosing them with either single- ('') or double- (\"\") quotation marks.</p> <pre><code>foxtype = 'quick'\nfoxcolor = ['b', 'r', 'o', 'w', 'n']\nprintln \"The $foxtype ${foxcolor.join()} fox\"\nx = 'Hello'\nprintln '$x + $y'\n</code></pre> Output<pre><code>The quick brown fox\n$x + $y\n</code></pre> <p>Info</p> <p>Note the different use of <code>$</code> and <code>${..}</code> syntax to interpolate value expressions in a string literal. The <code>$x</code> variable was not expanded, as it was enclosed by single quotes.</p> <p>Finally, string literals can also be defined using the <code>/</code> character as a delimiter. They are known as slashy strings and are useful for defining regular expressions and patterns, as there is no need to escape backslashes. As with double-quote strings they allow to interpolate variables prefixed with a <code>$</code> character.</p> <p>Try the following to see the difference:</p> <pre><code>x = /tic\\tac\\toe/\ny = 'tic\\tac\\toe'\nprintln x\nprintln y\n</code></pre> Output<pre><code>tic\\tac\\toe\ntic    ac    oe\n</code></pre>"},{"location":"basic_training/groovy/#multi-line-strings","title":"Multi-line strings","text":"<p>A block of text that spans multiple lines can be defined by delimiting it with triple single or double quotes:</p> <pre><code>text = \"\"\"\n    Hello there James.\n    How are you today?\n    \"\"\"\nprintln text\n</code></pre> <p>Finally, multi-line strings can also be defined with slashy strings. For example:</p> <pre><code>text = /\n    This is a multi-line\n    slashy string!\n    It's cool, isn't it?!\n    /\nprintln text\n</code></pre> <p>Info</p> <p>Like before, multi-line strings inside double quotes and slash characters support variable interpolation, while single-quoted multi-line strings do not.</p>"},{"location":"basic_training/groovy/#if-statement","title":"If statement","text":"<p>The <code>if</code> statement uses the same syntax common in other programming languages, such as Java, C, JavaScript, etc.</p> <pre><code>if (&lt; boolean expression &gt;) {\n// true branch\n}\nelse {\n// false branch\n}\n</code></pre> <p>The <code>else</code> branch is optional. Also, the curly brackets are optional when the branch defines just a single statement.</p> <pre><code>x = 1\nif (x &gt; 10)\nprintln 'Hello'\n</code></pre> <p>Tip</p> <p><code>null</code>, empty strings, and empty collections are evaluated to <code>false</code>.</p> <p>Therefore a statement like:</p> <pre><code>list = [1, 2, 3]\nif (list != null &amp;&amp; list.size() &gt; 0) {\nprintln list\n}\nelse {\nprintln 'The list is empty'\n}\n</code></pre> <p>Can be written as:</p> <pre><code>list = [1, 2, 3]\nif (list)\nprintln list\nelse\nprintln 'The list is empty'\n</code></pre> <p>See the Groovy-Truth for further details.</p> <p>Tip</p> <p>In some cases it can be useful to replace the <code>if</code> statement with a ternary expression (aka conditional expression). For example:</p> <pre><code>println list ? list : 'The list is empty'\n</code></pre> <p>The previous statement can be further simplified using the Elvis operator, as shown below:</p> <pre><code>println list ?: 'The list is empty'\n</code></pre>"},{"location":"basic_training/groovy/#for-statement","title":"For statement","text":"<p>The classical <code>for</code> loop syntax is supported as shown here:</p> <pre><code>for (int i = 0; i &lt; 3; i++) {\nprintln(\"Hello World $i\")\n}\n</code></pre> <p>Iteration over list objects is also possible using the syntax below:</p> <pre><code>list = ['a', 'b', 'c']\nfor (String elem : list) {\nprintln elem\n}\n</code></pre>"},{"location":"basic_training/groovy/#functions","title":"Functions","text":"<p>It is possible to define a custom function into a script, as shown here:</p> <pre><code>int fib(int n) {\nreturn n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2)\n}\nassert fib(10)==89\n</code></pre> <p>A function can take multiple arguments separating them with a comma. The <code>return</code> keyword can be omitted and the function implicitly returns the value of the last evaluated expression. Also, explicit types can be omitted, though not recommended:</p> <pre><code>def fact(n) {\nn &gt; 1 ? n * fact(n - 1) : 1\n}\nassert fact(5) == 120\n</code></pre>"},{"location":"basic_training/groovy/#closures","title":"Closures","text":"<p>Closures are the Swiss army knife of Nextflow/Groovy programming. In a nutshell, a closure is a block of code that can be passed as an argument to a function. A closure can also be used to define an anonymous function.</p> <p>More formally, a closure allows the definition of functions as first-class objects.</p> <pre><code>square = { it * it }\n</code></pre> <p>The curly brackets around the expression <code>it * it</code> tells the script interpreter to treat this expression as code. The <code>it</code> identifier is an implicit variable that represents the value that is passed to the function when it is invoked.</p> <p>Once compiled, the function object is assigned to the variable <code>square</code> as any other variable assignment shown previously. To invoke the closure execution use the special method <code>call</code> or just use the round parentheses to specify the closure parameter(s). For example:</p> <pre><code>assert square.call(5) == 25\nassert square(9) == 81\n</code></pre> <p>As is, this may not seem interesting, but we can now pass the <code>square</code> function as an argument to other functions or methods. Some built-in functions take a function like this as an argument. One example is the <code>collect</code> method on lists:</p> <pre><code>x = [1, 2, 3, 4].collect(square)\nprintln x\n</code></pre> Output<pre><code>[1, 4, 9, 16]\n</code></pre> <p>By default, closures take a single parameter called <code>it</code>. To give it a different name use the <code>-&gt;</code> syntax. For example:</p> <pre><code>square = { num -&gt; num * num }\n</code></pre> <p>It\u2019s also possible to define closures with multiple, custom-named parameters.</p> <p>For example, when the method <code>each()</code> is applied to a map it can take a closure with two arguments, to which it passes the key-value pair for each entry in the <code>map</code> object. For example:</p> <pre><code>printMap = { a, b -&gt; println \"$a with value $b\" }\nvalues = [\"Yue\": \"Wu\", \"Mark\": \"Williams\", \"Sudha\": \"Kumari\"]\nvalues.each(printMap)\n</code></pre> Output<pre><code>Yue with value Wu\nMark with value Williams\nSudha with value Kumari\n</code></pre> <p>A closure has two other important features.</p> <p>First, it can access and modify variables in the scope where it is defined.</p> <p>Second, a closure can be defined in an anonymous manner, meaning that it is not given a name, and is defined in the place where it needs to be used.</p> <p>As an example showing both these features, see the following code fragment:</p> <pre><code>result = 0 // (1)!\nvalues = [\"China\": 1, \"India\": 2, \"USA\": 3] // (2)!\nvalues.keySet().each { result += values[it] } // (3)!\nprintln result\n</code></pre> <ol> <li>Defines a global variable.</li> <li>Defines a map object.</li> <li>Invokes the <code>each</code> method passing the closure object which modifies the <code>result</code> variable.</li> </ol> <p>Learn more about closures in the Groovy documentation.</p>"},{"location":"basic_training/groovy/#more-resources","title":"More resources","text":"<p>The complete Groovy language documentation is available at this link.</p> <p>A great resource to master Apache Groovy syntax is the book: Groovy in Action.</p>"},{"location":"basic_training/groovy.pt/","title":"Estruturas b\u00e1sicas e express\u00f5es do Groovy","text":"<p>Nextflow \u00e9 uma linguagem espec\u00edfica de dom\u00ednio (DSL) implementada sobre a linguagem de programa\u00e7\u00e3o Groovy, que por sua vez \u00e9 um superconjunto da linguagem de programa\u00e7\u00e3o Java. Isso significa que o Nextflow pode executar qualquer c\u00f3digo Groovy ou Java.</p> <p>Aqui est\u00e3o algumas sintaxes Groovy importantes que s\u00e3o comumente usadas no Nextflow.</p>"},{"location":"basic_training/groovy.pt/#imprimindo-valores","title":"Imprimindo valores","text":"<p>Imprimir algo \u00e9 t\u00e3o f\u00e1cil quanto usar um dos m\u00e9todos <code>print</code> ou <code>println</code>.</p> <pre><code>println(\"Ol\u00e1, mundo!\")\n</code></pre> <p>A \u00fanica diferen\u00e7a entre os dois \u00e9 que o m\u00e9todo <code>println</code> anexa implicitamente um caractere de nova linha \u00e0 string impressa.</p> <p>Tip</p> <p>Par\u00eanteses para invoca\u00e7\u00f5es de fun\u00e7\u00e3o s\u00e3o opcionais. Portanto, a seguinte sintaxe tamb\u00e9m \u00e9 v\u00e1lida:</p> <pre><code>println \"Ol\u00e1, mundo!\"\n</code></pre>"},{"location":"basic_training/groovy.pt/#comentarios","title":"Coment\u00e1rios","text":"<p>Os coment\u00e1rios usam a mesma sintaxe das linguagens de programa\u00e7\u00e3o da fam\u00edlia C:</p> <pre><code>// comente uma \u00fanica linha\n/*\n    um coment\u00e1rio abrangendo\n    v\u00e1rias linhas\n*/\n</code></pre>"},{"location":"basic_training/groovy.pt/#variaveis","title":"Vari\u00e1veis","text":"<p>Para definir uma vari\u00e1vel, basta atribuir um valor a ela:</p> <pre><code>x = 1\nprintln x\nx = new java.util.Date()\nprintln x\nx = -3.1499392\nprintln x\nx = false\nprintln x\nx = \"Oi\"\nprintln x\n</code></pre> <p>As vari\u00e1veis locais s\u00e3o definidas usando a palavra-chave <code>def</code>:</p> <pre><code>def x = 'foo'\n</code></pre> <p>O <code>def</code> deve ser sempre usado ao definir vari\u00e1veis locais para uma fun\u00e7\u00e3o ou clausura.</p>"},{"location":"basic_training/groovy.pt/#listas","title":"Listas","text":"<p>Um objeto List pode ser definido colocando os itens da lista entre colchetes:</p> <pre><code>lista = [10, 20, 30, 40]\n</code></pre> <p>Voc\u00ea pode acessar um determinado item na lista com a nota\u00e7\u00e3o de colchetes (\u00edndices come\u00e7am em <code>0</code>) ou usando o m\u00e9todo <code>get</code>:</p> <pre><code>println lista[0]\nprintln lista.get(0)\n</code></pre> <p>Para obter o comprimento de uma lista, voc\u00ea pode usar o m\u00e9todo <code>size</code>:</p> <pre><code>println lista.size()\n</code></pre> <p>Usamos a palavra-chave <code>assert</code> para testar se uma condi\u00e7\u00e3o \u00e9 verdadeira (semelhante a uma fun\u00e7\u00e3o <code>if</code>). Aqui, o Groovy n\u00e3o imprimir\u00e1 nada se estiver correto, caso contr\u00e1rio, gerar\u00e1 uma mensagem AssertionError.</p> <pre><code>assert lista[0] == 10\n</code></pre> <p>Note</p> <p>Esta afirma\u00e7\u00e3o deve estar correta, tente alter\u00e1-la para uma incorreta.</p> <p>As listas tamb\u00e9m podem ser indexadas com \u00edndices negativos e intervalos invertidos.</p> <pre><code>lista = [0, 1, 2]\nassert lista[-1] == 2\nassert lista[-1..0] == lista.reverse()\n</code></pre> <p>Info</p> <p>Na afirma\u00e7\u00e3o da \u00faltima linha, estamos referenciando a lista inicial e convertendo-a com um intervalo \"abreviado\" (<code>..</code>), para executar do -1\u00ba elemento (2), o \u00faltimo, ao 0\u00ba elemento (0), o primeiro.</p> <p>Objetos List implementam todos os m\u00e9todos fornecidos pela interface java.util.List, mais os m\u00e9todos de extens\u00e3o fornecidos pelo Groovy.</p> <pre><code>assert [1, 2, 3] &lt;&lt; 1 == [1, 2, 3, 1]\nassert [1, 2, 3] + [1] == [1, 2, 3, 1]\nassert [1, 2, 3, 1] - [1] == [2, 3]\nassert [1, 2, 3] * 2 == [1, 2, 3, 1, 2, 3]\nassert [1, [2, 3]].flatten() == [1, 2, 3]\nassert [1, 2, 3].reverse() == [3, 2, 1]\nassert [1, 2, 3].collect { it + 3 } == [4, 5, 6]\nassert [1, 2, 3, 1].unique().size() == 3\nassert [1, 2, 3, 1].count(1) == 2\nassert [1, 2, 3, 4].min() == 1\nassert [1, 2, 3, 4].max() == 4\nassert [1, 2, 3, 4].sum() == 10\nassert [4, 2, 1, 3].sort() == [1, 2, 3, 4]\nassert [4, 2, 1, 3].find { it % 2 == 0 } == 4\nassert [4, 2, 1, 3].findAll { it % 2 == 0 } == [4, 2]\n</code></pre>"},{"location":"basic_training/groovy.pt/#mapas","title":"Mapas","text":"<p>Os mapas s\u00e3o como listas que possuem uma chave arbitr\u00e1ria em vez de um n\u00famero inteiro. Portanto, a sintaxe \u00e9 bem parecida.</p> <pre><code>mapa = [a: 0, b: 1, c: 2]\n</code></pre> <p>Os mapas podem ser acessados em uma sintaxe convencional de colchetes ou como se a chave fosse uma propriedade do mapa.</p> <p>Clique no \u00edcone  para ver explica\u00e7\u00f5es no c\u00f3digo.</p> <pre><code>assert mapa['a'] == 0 // (1)!\nassert mapa.b == 1 // (2)!\nassert mapa.get('c') == 2 // (3)!\n</code></pre> <ol> <li>Usando colchetes.</li> <li>Usando a nota\u00e7\u00e3o de ponto.</li> <li>Usando o m\u00e9todo <code>get</code>.</li> </ol> <p>Para adicionar dados ou modificar um mapa, a sintaxe \u00e9 semelhante \u00e0 adi\u00e7\u00e3o de valores a uma lista:</p> <pre><code>mapa['a'] = 'x' // (1)!\nmapa.b = 'y' // (2)!\nmapa.put('c', 'z') // (3)!\nassert mapa == [a: 'x', b: 'y', c: 'z']\n</code></pre> <ol> <li>Usando colchetes.</li> <li>Usando a nota\u00e7\u00e3o de ponto.</li> <li>Usando o m\u00e9todo put.</li> </ol> <p>Objetos Map implementam todos os m\u00e9todos fornecidos pela interface java.util.Map, mais os m\u00e9todos de extens\u00e3o fornecidos pelo Groovy.</p>"},{"location":"basic_training/groovy.pt/#interpolacao-de-strings","title":"Interpola\u00e7\u00e3o de Strings","text":"<p>Strings podem ser definidas colocando-as entre aspas simples ('') ou duplas (\"\").</p> <pre><code>tipoderaposa = 'r\u00e1pida'\ncordaraposa = ['m', 'a', 'r', 'r', 'o', 'm']\nprintln \"A $tipoderaposa raposa ${cordaraposa.join()}\"\nx = 'Ol\u00e1'\nprintln '$x + $y'\n</code></pre> Output<pre><code>A r\u00e1pida raposa marrom\n$x + $y\n</code></pre> <p>Info</p> <p>Observe o uso diferente das sintaxes <code>$</code> e <code>${..}</code> para interpolar express\u00f5es de valor em uma string. A vari\u00e1vel <code>$x</code> n\u00e3o foi expandida, pois estava entre aspas simples.</p> <p>Por fim, strings tamb\u00e9m podem ser definidas usando o caractere <code>/</code> como delimitador. Elas s\u00e3o conhecidas como strings com barras e s\u00e3o \u00fateis para definir express\u00f5es regulares e padr\u00f5es, pois n\u00e3o h\u00e1 necessidade de escapar as barras invertidas. Assim como as strings de aspas duplas, elas permitem interpolar vari\u00e1veis prefixadas com um caractere <code>$</code>.</p> <p>Tente o seguinte para ver a diferen\u00e7a:</p> <pre><code>x = /tic\\tac\\toe/\ny = 'tic\\tac\\toe'\nprintln x\nprintln y\n</code></pre> Output<pre><code>tic\\tac\\toe\ntic    ac    oe\n</code></pre>"},{"location":"basic_training/groovy.pt/#strings-de-varias-linhas","title":"Strings de v\u00e1rias linhas","text":"<p>Um bloco de texto que abrange v\u00e1rias linhas pode ser definido delimitando-o com aspas simples ou duplas triplas:</p> <pre><code>texto = \"\"\"\n    E a\u00ed, James.\n    Como voc\u00ea est\u00e1 hoje?\n    \"\"\"\nprintln texto\n</code></pre> <p>Por fim, strings de v\u00e1rias linhas tamb\u00e9m podem ser definidas com strings com barras. Por exemplo:</p> <pre><code>texto = /\n    Esta \u00e9 uma string abrangendo\n    v\u00e1rias linhas com barras!\n    Super legal, n\u00e9?!\n    /\nprintln texto\n</code></pre> <p>Info</p> <p>Como antes, strings de v\u00e1rias linhas dentro de aspas duplas e caracteres de barra suportam interpola\u00e7\u00e3o de vari\u00e1vel, enquanto strings de v\u00e1rias linhas com aspas simples n\u00e3o.</p>"},{"location":"basic_training/groovy.pt/#declaracoes-condicionais-com-if","title":"Declara\u00e7\u00f5es condicionais com if","text":"<p>A instru\u00e7\u00e3o <code>if</code> usa a mesma sintaxe comum em outras linguagens de programa\u00e7\u00e3o, como Java, C, JavaScript, etc.</p> <pre><code>if (&lt; express\u00e3o booleana &gt;) {\n// ramo verdadeiro\n}\nelse {\n// ramo falso\n}\n</code></pre> <p>O ramo <code>else</code> \u00e9 opcional. Al\u00e9m disso, as chaves s\u00e3o opcionais quando a ramifica\u00e7\u00e3o define apenas uma \u00fanica instru\u00e7\u00e3o.</p> <pre><code>x = 1\nif (x &gt; 10)\nprintln 'Ol\u00e1'\n</code></pre> <p>Tip</p> <p><code>null</code>, strings vazias e cole\u00e7\u00f5es (mapas e listas) vazias s\u00e3o avaliadas como <code>false</code>.</p> <p>Portanto, uma declara\u00e7\u00e3o como:</p> <pre><code>lista = [1, 2, 3]\nif (lista != null &amp;&amp; lista.size() &gt; 0) {\nprintln lista\n}\nelse {\nprintln 'A lista est\u00e1 vazia'\n}\n</code></pre> <p>Pode ser escrita como:</p> <pre><code>lista = [1, 2, 3]\nif (lista)\nprintln lista\nelse\nprintln 'A lista est\u00e1 vazia'\n</code></pre> <p>Veja o Groovy-Truth para mais detalhes.</p> <p>Tip</p> <p>Em alguns casos, pode ser \u00fatil substituir a instru\u00e7\u00e3o <code>if</code> por uma express\u00e3o tern\u00e1ria (tamb\u00e9m conhecida como express\u00e3o condicional). Por exemplo:</p> <pre><code>println lista ? lista : 'A lista est\u00e1 vazia'\n</code></pre> <p>A declara\u00e7\u00e3o anterior pode ser ainda mais simplificada usando o operador Elvis, como mostrado abaixo:</p> <pre><code>println lista ?: 'A lista est\u00e1 vazia'\n</code></pre>"},{"location":"basic_training/groovy.pt/#declaracoes-de-loop-com-for","title":"Declara\u00e7\u00f5es de loop com for","text":"<p>A sintaxe cl\u00e1ssica do loop <code>for</code> \u00e9 suportada como mostrado aqui:</p> <pre><code>for (int i = 0; i &lt; 3; i++) {\nprintln(\"Ol\u00e1 mundo $i\")\n}\n</code></pre> <p>A itera\u00e7\u00e3o sobre objetos de lista tamb\u00e9m \u00e9 poss\u00edvel usando a sintaxe abaixo:</p> <pre><code>list = ['a', 'b', 'c']\nfor (String elem : lista) {\nprintln elem\n}\n</code></pre>"},{"location":"basic_training/groovy.pt/#funcoes","title":"Fun\u00e7\u00f5es","text":"<p>\u00c9 poss\u00edvel definir uma fun\u00e7\u00e3o personalizada em um script, conforme mostrado aqui:</p> <pre><code>int fib(int n) {\nreturn n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2)\n}\nassert fib(10)==89\n</code></pre> <p>Uma fun\u00e7\u00e3o pode receber v\u00e1rios argumentos, separando-os com uma v\u00edrgula. A palavra-chave <code>return</code> pode ser omitida e a fun\u00e7\u00e3o retorna implicitamente o valor da \u00faltima express\u00e3o avaliada. Al\u00e9m disso, tipos expl\u00edcitos podem ser omitidos, embora n\u00e3o sejam recomendados:</p> <pre><code>def fact(n) {\nn &gt; 1 ? n * fact(n - 1) : 1\n}\nassert fact(5) == 120\n</code></pre>"},{"location":"basic_training/groovy.pt/#clausuras","title":"Clausuras","text":"<p>Clausuras s\u00e3o o canivete su\u00ed\u00e7o da programa\u00e7\u00e3o com Nextflow/Groovy. Resumindo, uma clausura \u00e9 um bloco de c\u00f3digo que pode ser passado como um argumento para uma fun\u00e7\u00e3o. Clausuras tamb\u00e9m podem ser usadas para definir uma fun\u00e7\u00e3o an\u00f4nima.</p> <p>Mais formalmente, uma clausura permite a defini\u00e7\u00e3o de fun\u00e7\u00f5es como objetos de primeira classe.</p> <pre><code>quadrado = { it * it }\n</code></pre> <p>As chaves ao redor da express\u00e3o <code>it * it</code> informam ao interpretador de scripts para tratar essa express\u00e3o como c\u00f3digo. O identificador <code>it</code> \u00e9 uma vari\u00e1vel impl\u00edcita que representa o valor que \u00e9 passado para a fun\u00e7\u00e3o quando ela \u00e9 invocada.</p> <p>Depois de compilado, o objeto de fun\u00e7\u00e3o \u00e9 atribu\u00eddo \u00e0 vari\u00e1vel <code>quadrado</code> como qualquer outra atribui\u00e7\u00e3o de vari\u00e1vel mostrada anteriormente. Para invocar a execu\u00e7\u00e3o da clausura, use o m\u00e9todo especial <code>call</code> ou simplesmente use os par\u00eanteses para especificar o(s) par\u00e2metro(s) da clausura. Por exemplo:</p> <pre><code>assert quadrado.call(5) == 25\nassert quadrado(9) == 81\n</code></pre> <p>Da forma como foi mostrado, isso pode n\u00e3o parecer interessante, mas agora podemos passar a fun\u00e7\u00e3o <code>quadrado</code> como um argumento para outras fun\u00e7\u00f5es ou m\u00e9todos. Algumas fun\u00e7\u00f5es embutidas aceitam uma fun\u00e7\u00e3o como esta como um argumento. Um exemplo \u00e9 o m\u00e9todo <code>collect</code> em listas:</p> <pre><code>x = [1, 2, 3, 4].collect(quadrado)\nprintln x\n</code></pre> Output<pre><code>[1, 4, 9, 16]\n</code></pre> <p>Por padr\u00e3o, as clausuras recebem um \u00fanico par\u00e2metro chamado <code>it</code>. Para dar a ele um nome diferente, use a sintaxe <code>-&gt;</code>. Por exemplo:</p> <pre><code>quadrado = { num -&gt; num * num }\n</code></pre> <p>Tamb\u00e9m \u00e9 poss\u00edvel definir clausuras com v\u00e1rios par\u00e2metros com nomes personalizados.</p> <p>Por exemplo, quando o m\u00e9todo <code>each()</code> \u00e9 aplicado a um mapa, ele pode receber uma clausura com dois argumentos, para os quais passa o par chave-valor para cada entrada no objeto <code>Map</code>. Por exemplo:</p> <pre><code>imprimirMapa = { a, b -&gt; println \"$a com o valor $b\" }\nvalores = [\"Yue\": \"Wu\", \"Mark\": \"Williams\", \"Sudha\": \"Kumari\"]\nvalores.each(imprimirMapa)\n</code></pre> Output<pre><code>Yue com o valor Wu\nMark com o valor Williams\nSudha com o valor Kumari\n</code></pre> <p>Uma clausura tem duas outras caracter\u00edsticas importantes.</p> <p>Primeiro, ela pode acessar e modificar vari\u00e1veis no escopo em que est\u00e1 definida.</p> <p>Em segundo lugar, uma clausura pode ser definida de maneira an\u00f4nima, o que significa que n\u00e3o recebe um nome e \u00e9 definida no local em que precisa ser usada.</p> <p>Para um exemplo mostrando esses dois recursos, consulte o seguinte trecho de c\u00f3digo:</p> <pre><code>resultado = 0 // (1)!\nvalores = [\"China\": 1, \"India\": 2, \"USA\": 3] // (2)!\nvalores.keySet().each { resultado += valores[it] } // (3)!\nprintln resultado\n</code></pre> <ol> <li>Define uma vari\u00e1vel global.</li> <li>Define um objeto de mapa.</li> <li>Chama o m\u00e9todo <code>each</code> passando o objeto de clausura que modifica a vari\u00e1vel <code>resultado</code>.</li> </ol> <p>Saiba mais sobre clausuras na documenta\u00e7\u00e3o do Groovy.</p>"},{"location":"basic_training/groovy.pt/#mais-recursos","title":"Mais recursos","text":"<p>A documenta\u00e7\u00e3o completa da linguagem Groovy est\u00e1 dispon\u00edvel nesse link.</p> <p>Um \u00f3timo recurso para dominar a sintaxe do Apache Groovy \u00e9 o livro: Groovy in Action.</p>"},{"location":"basic_training/index.es/","title":"Bienvenido","text":""},{"location":"basic_training/index.es/#bienvenido-al-taller-de-capacitacion-de-nextflow","title":"Bienvenido al taller de capacitaci\u00f3n de Nextflow","text":"<p>Nos complace acompa\u00f1arlo en el camino para escribir flujos de trabajo cient\u00edficos reproducibles y escalables usando Nextflow. Esta gu\u00eda complementa la documentaci\u00f3n completa de Nextflow; si alguna vez tienes alguna duda, dir\u00edjete a los documentos que se encuentran en el siguiente enlace.</p> <p>Al final de este curso deber\u00eda:</p> <ol> <li>Ser competente en la escritura de scripts de Nextflow</li> <li>Conocer los conceptos b\u00e1sicos de Nextflow de Canales, Procesos y Operadores</li> <li>Comprender los flujos de trabajo en contenedores</li> <li>Comprender las diferentes plataformas de ejecuci\u00f3n compatibles con Nextflow</li> <li>Conocer la comunidad y el ecosistema de Nextflow</li> </ol>"},{"location":"basic_training/index.es/#sigue-los-videos-de-entrenamiento","title":"Sigue los videos de entrenamiento","text":"<p>En nuestra \u00faltima capacitaci\u00f3n de nf-core en marzo de 2023, utilizamos este material de capacitaci\u00f3n para ense\u00f1ar Nextflow y la grabaci\u00f3n est\u00e1 disponible para que la vea a su propio ritmo en varios idiomas.</p> <p>Consulte los siguientes links para ver los videos de YouTube en los idiomas disponibles:</p> <ul> <li> En Ingl\u00e9s</li> <li> En Hind\u00fa</li> <li> En Espa\u00f1ol</li> <li> En Portugues</li> <li> En Franc\u00e9s</li> </ul>"},{"location":"basic_training/index.es/#descripcion-general","title":"Descripci\u00f3n general","text":"<p>Para que comience con Nextflow lo m\u00e1s r\u00e1pido posible, seguiremos los siguientes pasos:</p> <ol> <li>Configurar un entorno de desarrollo para ejecutar Nextflow</li> <li>Explorar los conceptos de Nextflow utilizando algunos flujos de trabajo b\u00e1sicos, incluido un an\u00e1lisis de RNA-Seq en varios pasos</li> <li>Crear y use contenedores Docker para encapsular todas las dependencias del flujo de trabajo</li> <li>Profundizar en la sintaxis central de Nextflow, incluidos canales, procesos y operadores</li> <li>Probar escenarios de implementaci\u00f3n en la nube y explore las capacidades de Nextflow Tower</li> </ol> <p>Esto te dar\u00e1 una amplia comprensi\u00f3n de Nextflow para comenzar a escribir tus propios flujos de trabajo. \u00a1Esperamos que disfrutes del curso! Este es un documento en constante evoluci\u00f3n: los comentarios siempre son bienvenidos.</p>"},{"location":"basic_training/index.fr/","title":"Index.fr","text":""},{"location":"basic_training/index.fr/#bienvenue","title":"Bienvenue","text":""},{"location":"basic_training/index.fr/#bienvenue-a-latelier-de-formation-nextflow","title":"Bienvenue \u00e0 l'atelier de formation Nextflow","text":"<p>Nous sommes heureux de vous accompagner sur le chemin de r\u00e9daction de workflows scientifiques reproductibles et \u00e9volutifs en utilisant Nextflow. Ce guide compl\u00e8te toute la documentation de Nextflow - si vous avez des doutes, consultez la documentation situ\u00e9e ici.</p> <p>\u00c0 la fin de ce cours, vous devriez:</p> <ol> <li>Ma\u00eetriser la redaction de workflows Nextflow</li> <li>Conna\u00eetre les concepts de base de Nextflow : canaux, processus et op\u00e9rateurs.</li> <li>Avoir une compr\u00e9hension des workflows conteneuris\u00e9s</li> <li>Comprendre les diff\u00e9rentes plateformes d'ex\u00e9cution support\u00e9es par Nextflow</li> <li>Conna\u00eetre la communaut\u00e9 et l'\u00e9cosyst\u00e8me Nextflow</li> </ol>"},{"location":"basic_training/index.fr/#suivez-les-videos-de-formation","title":"Suivez les vid\u00e9os de formation","text":"<p>Dans notre derni\u00e8re formation nf-core en mars 2023, nous avons utilis\u00e9 ce mat\u00e9riel de formation pour enseigner Nextflow et l'enregistrement est disponible pour que vous puissiez le regarder \u00e0 votre rythme dans plusieurs langues.</p> <p>Consultez les liens ci-dessous pour les vid\u00e9os YouTube dans les langues disponibles\u00a0:</p> <ul> <li> En Anglais</li> <li> En Hindi</li> <li> En Espagnol</li> <li> En Portugais</li> <li> En Fran\u00e7ais</li> </ul>"},{"location":"basic_training/index.fr/#apercu","title":"Aper\u00e7u","text":"<p>Pour que vous puissiez commencer \u00e0 utiliser Nextflow le plus rapidement possible, nous allons suivre les \u00e9tapes suivantes :</p> <ol> <li>Mettre en place un environnement de d\u00e9veloppement pour ex\u00e9cuter Nextflow</li> <li>Explorer les concepts de Nextflow en utilisant quelques workflows de base, y compris une analyse RNA-Seq en plusieurs \u00e9tapes.</li> <li>Construire et utiliser des conteneurs Docker pour encapsuler toutes les d\u00e9pendances des workflows.</li> <li>Approfondir la syntaxe de base de Nextflow, y compris les canaux, les processus et les op\u00e9rateurs.</li> <li>Couvrir les sc\u00e9narios de d\u00e9ploiement en cluster et en cloud et explorer les capacit\u00e9s de Nextflow Tower.</li> </ol> <p>Cela vous donnera une large compr\u00e9hension de Nextflow, pour commencer \u00e0 \u00e9crire vos propres workflows. Nous esp\u00e9rons que vous appr\u00e9cierez ce cours ! Il s'agit d'un document en constante \u00e9volution - les commentaires sont toujours les bienvenus.</p>"},{"location":"basic_training/index.pt/","title":"Seja bem vindo","text":""},{"location":"basic_training/index.pt/#bem-vindo-ao-treinamento-basico-do-nextflow","title":"Bem vindo ao treinamento b\u00e1sico do Nextflow","text":"<p>Estamos entusiasmados em t\u00ea-lo no caminho para escrever fluxos de trabalho cient\u00edficos reprodut\u00edveis e escal\u00e1veis usando o Nextflow. Este guia complementa a documenta\u00e7\u00e3o oficial do Nextflow - se voc\u00ea tiver alguma d\u00favida, acesse a documenta\u00e7\u00e3o oficial localizada aqui.</p> <p>Ao final deste curso voc\u00ea dever\u00e1:</p> <ol> <li>Ser proficiente em escrever fluxos de trabalho com o Nextflow</li> <li>Conhecer os conceitos b\u00e1sicos de Canais, Processos e Operadores no Nextflow</li> <li>Ter uma compreens\u00e3o dos fluxos de trabalho usando cont\u00eaineres</li> <li>Entender as diferentes plataformas de execu\u00e7\u00e3o suportadas pelo Nextflow</li> <li>Sentir-se apresentado \u00e0 comunidade e ecossistema do Nextflow</li> </ol>"},{"location":"basic_training/index.pt/#acompanhe-os-videos-de-treinamento","title":"Acompanhe os v\u00eddeos de treinamento","text":"<p>Em nosso \u00faltimo treinamento do nf-core em Mar\u00e7o de 2023, utilizamos esse material de treinamento para ensinar Nextflow e as grava\u00e7\u00f5es est\u00e3o dispon\u00edveis para voc\u00ea assistir no seu ritmo em v\u00e1rios idiomas diferentes.</p> <p>Confira os links abaixo para os v\u00eddeos no YouTube nas linguagens dispon\u00edveis:</p> <ul> <li> Em Ingl\u00eas</li> <li> Em Hindu</li> <li> Em Espanhol</li> <li> Em Portugu\u00eas</li> <li> Em Franc\u00eas</li> </ul>"},{"location":"basic_training/index.pt/#visao-geral","title":"Vis\u00e3o geral","text":"<p>Para come\u00e7ar a usar o Nextflow o mais r\u00e1pido poss\u00edvel, seguiremos as seguintes etapas:</p> <ol> <li>Configurar um ambiente de desenvolvimento para executar o Nextflow</li> <li>Explorar os conceitos do Nextflow usando alguns fluxos de trabalho b\u00e1sicos, incluindo uma an\u00e1lise de RNA-Seq com v\u00e1rias etapas</li> <li>Criar e usar cont\u00eaineres do Docker para encapsular todas as depend\u00eancias do fluxo de trabalho</li> <li>Mergulhar mais fundo na sintaxe principal do Nextflow, incluindo Canais, Processos e Operadores</li> <li>Cobrir cen\u00e1rios de implanta\u00e7\u00e3o na nuvem e em clusters e explorar os recursos do Nextflow Tower</li> </ol> <p>Isso lhe dar\u00e1 uma ampla compreens\u00e3o do Nextflow para come\u00e7ar a escrever seus pr\u00f3prios fluxos de trabalho. Esperamos que goste do curso! Este \u00e9 um documento em constante evolu\u00e7\u00e3o - feedback \u00e9 sempre bem-vindo.</p>"},{"location":"basic_training/intro.fr/","title":"Introduction","text":""},{"location":"basic_training/intro.fr/#concepts-de-base","title":"Concepts de base","text":"<p>Nextflow est un moteur d'orchestration de workflow et un langage de domaine sp\u00e9cifique (DSL) qui facilite la redaction dun workflow informatiques \u00e0 forte intensit\u00e9 de donn\u00e9es.</p> <p>Il est con\u00e7u autour de l'id\u00e9e que la plateforme Linux est la lingua franca de la science des donn\u00e9es. Linux fournit de nombreux outils de ligne de commande et de script simples mais puissants qui, lorsqu'ils sont combin\u00e9s, facilitent les manipulations de donn\u00e9es complexes.</p> <p>Nextflow \u00e9tend cette approche en ajoutant la possibilit\u00e9 de d\u00e9finir des interactions complexes entre les programmes et un environnement de calcul parall\u00e8le de haut niveau, bas\u00e9 sur le mod\u00e8le de programmation par flux de donn\u00e9es. Les principales caract\u00e9ristiques de Nextflow sont les suivantes:</p> <ul> <li>Portabilit\u00e9 et reproductibilit\u00e9 du workflow</li> <li>la scalabilit\u00e9 de la parall\u00e9lisation et du d\u00e9ploiement</li> <li>Int\u00e9gration des outils, syst\u00e8mes et normes industrielles existants</li> </ul>"},{"location":"basic_training/intro.fr/#processus-et-canaux","title":"Processus et canaux","text":"<p>En pratique, un workflow Nextflow est constitu\u00e9 par l'assemblage de diff\u00e9rents processus. Chaque <code>processus</code> peut \u00eatre \u00e9crit dans n'importe quel langage de script qui peut \u00eatre ex\u00e9cut\u00e9 par la plateforme Linux (Bash, Perl, Ruby, Python, etc.).</p> <p>Les processus sont ex\u00e9cut\u00e9s ind\u00e9pendamment et sont isol\u00e9s les uns des autres, c'est-\u00e0-dire qu'ils ne partagent pas d'\u00e9tat commun (redigeable). Ils ne peuvent communiquer que par l'interm\u00e9diaire de files d'attente asynchrones FIFO (first-in, first-out), appel\u00e9es <code>canaux</code>.</p> <p>Tout <code>processus</code> peut d\u00e9finir un ou plusieurs <code>canaux</code> comme 'input' et 'output'. L'interaction entre ces processus, et finalement le flux d'ex\u00e9cution du workflow lui-m\u00eame, est implicitement d\u00e9finie par ces d\u00e9clarations <code>input</code> et <code>output</code>.</p> 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 data zdata ydata xChannelProcessdata xoutput xdata youtput ydata zoutput ztask 1task 2task 3"},{"location":"basic_training/intro.fr/#abstraction-dexecution","title":"Abstraction d'ex\u00e9cution","text":"<p>Alors qu'un <code>processus</code> d\u00e9finit quelle commande ou <code>script</code> doit \u00eatre ex\u00e9cut\u00e9, l'ex\u00e9cuteur d\u00e9termine comment ce <code>script</code> est ex\u00e9cut\u00e9 sur la plateforme cible.</p> <p>Sauf indication contraire, les processus sont ex\u00e9cut\u00e9s sur l'ordinateur local. L'ex\u00e9cuteur local est tr\u00e8s utile pour le d\u00e9veloppement et les tests de workflows, mais pour les workflows de calcul r\u00e9els, une plateforme de calcul haute performance (HPC) ou une plateforme cloud est souvent n\u00e9cessaire.</p> <p>En d'autres termes, Nextflow fournit une abstraction entre la logique fonctionnelle du workflow et le syst\u00e8me d'ex\u00e9cution sous-jacent (ou runtime). Ainsi, il est possible d'\u00e9crire un workflow qui s'ex\u00e9cute de mani\u00e8re transparente sur votre ordinateur, un cluster ou le cloud, sans \u00eatre modifi\u00e9. Il vous suffit de d\u00e9finir la plateforme d'ex\u00e9cution cible dans le fichier de configuration.</p> <p></p>"},{"location":"basic_training/intro.fr/#langage-de-script","title":"Langage de script","text":"<p>Nextflow met en \u0153uvre un DSL d\u00e9claratif qui simplifie l'\u00e9criture de workflows d'analyse de donn\u00e9es complexes en tant qu'extension d'un langage de programmation g\u00e9n\u00e9ral.</p> <p>Cette approche rend Nextflow flexible - il offre les avantages d'un DSL concis pour le traitement des cas d'utilisation r\u00e9currents avec facilit\u00e9 et la flexibilit\u00e9 et la puissance d'un langage de programmation polyvalent pour traiter les cas d'angle dans le m\u00eame environnement informatique. Cela serait difficile \u00e0 mettre en \u0153uvre en utilisant une approche purement d\u00e9clarative.</p> <p>En termes pratiques, le script Nextflow est une extension du [langage de programmation Groovy] (https://groovy-lang.org/) qui, \u00e0 son tour, est un super-ensemble du langage de programmation Java. Groovy peut \u00eatre consid\u00e9r\u00e9 comme \"Python pour Java\", en ce sens qu'il simplifie l'\u00e9criture du code et qu'il est plus accessible.</p>"},{"location":"basic_training/intro.fr/#votre-premier-script","title":"Votre premier script","text":"<p>Ici vous allez ex\u00e9cuter votre premier script Nextflow (<code>hello.nf</code>), que nous allons parcourir ligne par ligne.</p> <p>Dans cet exemple, le script prend une cha\u00eene d'entr\u00e9e (un param\u00e8tre appel\u00e9 <code>params.greeting</code>) et la divise en morceaux de six caract\u00e8res dans le premier processus. Le deuxi\u00e8me processus convertit ensuite les caract\u00e8res en majuscules. Le r\u00e9sultat est finalement affich\u00e9 \u00e0 l'\u00e9cran.</p>"},{"location":"basic_training/intro.fr/#code-nextflow","title":"Code Nextflow","text":"<p>Info</p> <p>Cliquez sur les ic\u00f4nes  dans le code pour obtenir des explications.</p> nf-training/hello.nf<pre><code>#!/usr/bin/env nextflow\n// (1)!\nparams.greeting = 'Hello world!' // (2)!\ngreeting_ch = Channel.of(params.greeting) // (3)!\nprocess SPLITLETTERS { // (4)!\ninput: // (5)!\nval x // (6)!\noutput: // (7)!\npath 'chunk_*' // (8)!\nscript: // (9)!\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} // (10)!\nprocess CONVERTTOUPPER { // (11)!\ninput: // (12)!\npath y // (13)!\noutput: // (14)!\nstdout // (15)!\nscript: // (16)!\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} // (17)!\nworkflow { // (18)!\nletters_ch = SPLITLETTERS(greeting_ch) // (19)!\nresults_ch = CONVERTTOUPPER(letters_ch.flatten()) // (20)!\nresults_ch.view { it } // (21)!\n} // (22)!\n</code></pre> <ol> <li>Le code commence par un shebang, qui d\u00e9clare Nextflow comme interpr\u00e8te.</li> <li>D\u00e9clare un param\u00e8tre <code>greeting</code> qui est initialis\u00e9 avec la valeur 'Hello world!'.</li> <li>Initialise un <code>canal</code> nomm\u00e9 <code>greeting_ch</code>, qui contient la valeur de <code>params.greeting</code>. Les canaux sont le type d'entr\u00e9e pour les processus dans Nextflow.</li> <li>Commence le premier bloc de processus, d\u00e9fini comme <code>SPLITLETTERS</code>.</li> <li>D\u00e9claration d'entr\u00e9e pour le processus <code>SPLITLETTERS</code>. Les entr\u00e9es peuvent \u00eatre des valeurs (<code>val</code>), des fichiers ou des chemins (<code>path</code>), ou d'autres qualificatifs (voir ici).</li> <li>Indique au processus d'attendre une valeur d'entr\u00e9e (<code>val</code>), que nous assignons \u00e0 la variable 'x'.</li> <li>D\u00e9claration de sortie pour le processus <code>SPLITLETTERS</code>.</li> <li>Indique au processus d'attendre un ou plusieurs fichiers de sortie (<code>path</code>), avec un nom de fichier commen\u00e7ant par 'chunk_', en tant que sortie du script. Le processus envoie la sortie sous forme de canal.</li> <li>Trois guillemets doubles commencent et terminent le bloc de code pour ex\u00e9cuter ce <code>processus</code>.    Le code \u00e0 ex\u00e9cuter se trouve \u00e0 l'int\u00e9rieur : il s'agit d'imprimer la valeur d<code>input</code> x (appel\u00e9e \u00e0 l'aide du pr\u00e9fixe dollar [$]), de diviser la cha\u00eene en morceaux de 6 caract\u00e8res (\"Hello\" et \"world !\") et de sauvegarder chacun d'eux dans un fichier (chunk_aa et chunk_ab).</li> <li>Fin du premier bloc de processus.</li> <li>Commence le deuxi\u00e8me bloc de processus, d\u00e9fini comme <code>CONVERTTOUPPER</code>.</li> <li>D\u00e9claration d'entr\u00e9e pour le <code>processus</code> <code>CONVERTTOUPPER</code>.</li> <li>Indique au <code>processus</code> d'attendre un ou plusieurs fichiers <code>input</code> (<code>path</code> ; i.e. chunk_aa et chunk_ab), que nous assignons \u00e0 la variable 'y'.</li> <li>D\u00e9claration de sortie pour le processus <code>CONVERTTOUPPER</code>.</li> <li>Indique au processus de s'attendre \u00e0 une sortie standard (stdout) et envoie cette sortie en tant que canal.</li> <li>Trois guillemets doubles commencent et terminent le bloc de code pour ex\u00e9cuter ce <code>process</code>.     Le bloc contient un script qui lit les fichiers (cat) en utilisant la variable d'entr\u00e9e \"$y\", puis convertit les caract\u00e8res en majuscules et les envoie vers la sortie standard.</li> <li>Fin du deuxi\u00e8me bloc <code>process</code>.</li> <li>D\u00e9but de la port\u00e9e du workflow o\u00f9 chaque processus peut \u00eatre appel\u00e9.</li> <li>Ex\u00e9cutez le <code>processus</code> <code>SPLITLETTERS</code> sur le <code>greeting_ch</code> (aka greeting channel), et stockez la sortie dans le canal <code>letters_ch</code>.</li> <li>Ex\u00e9cutez le <code>processus</code> <code>CONVERTTOUPPER</code> sur le canal de lettres <code>letters_ch</code>, qui est aplati en utilisant l'op\u00e9rateur <code>.flatten()</code>. Cela transforme le canal d'entr\u00e9e de telle sorte que chaque \u00e9l\u00e9ment est un \u00e9l\u00e9ment s\u00e9par\u00e9. Nous stockons la sortie dans le canal <code>results_ch</code>.</li> <li>La sortie finale (dans le canal <code>results_ch</code>) est imprim\u00e9e \u00e0 l'\u00e9cran en utilisant l'op\u00e9rateur <code>view</code> (ajout\u00e9 au nom du canal).</li> <li>Fin de l'\u00e9tendue du workflow.</li> </ol> <p>L'utilisation de l'op\u00e9rateur <code>.flatten()</code> ici permet de diviser les deux fichiers en deux \u00e9l\u00e9ments distincts qui seront trait\u00e9s par le processus suivant (sinon, ils seraient trait\u00e9s comme un seul \u00e9l\u00e9ment).</p>"},{"location":"basic_training/intro.fr/#dans-la-pratique","title":"Dans la pratique","text":"<p>Copiez maintenant l'exemple ci-dessus dans votre \u00e9diteur de texte favori et enregistrez-le dans un fichier nomm\u00e9 <code>hello.nf</code>.</p> <p>Warning</p> <p>Pour le tutoriel Gitpod, assurez-vous d'\u00eatre dans le dossier appel\u00e9 <code>nf-training</code></p> <p>Ex\u00e9cutez le script en entrant la commande suivante dans votre terminal :</p> <pre><code>nextflow run hello.nf\n</code></pre> <p>Le r\u00e9sultat ressemblera au texte ci-dessous :</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [cheeky_keller] DSL2 - revision: 197a0e289a\nexecutor &gt;  local (3)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[37/b9332f] process &gt; CONVERTTOUPPER (2) [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n</code></pre> <p>La sortie standard affiche (ligne par ligne) :</p> <ol> <li>La version de Nextflow qui a \u00e9t\u00e9 ex\u00e9cut\u00e9e.</li> <li>Les noms du script et de la version.</li> <li>L'ex\u00e9cuteur utilis\u00e9 (dans le cas ci-dessus : local).</li> <li>Le premier <code>processus</code> est ex\u00e9cut\u00e9 une fois, ce qui signifie qu'il y a une t\u00e2che. La ligne commence par une valeur hexad\u00e9cimale unique (voir TIP ci-dessous), et se termine par le pourcentage et d'autres informations sur l'ach\u00e8vement de la t\u00e2che.</li> <li>Le deuxi\u00e8me processus est ex\u00e9cut\u00e9 deux fois (une fois pour chunk_aa et une fois pour chunk_ab), ce qui signifie qu'il y a deux t\u00e2ches.</li> <li>La cha\u00eene de r\u00e9sultats de stdout est imprim\u00e9e.</li> </ol> <p>Info</p> <p>Les nombres hexad\u00e9cimaux, comme <code>31/52c31e</code>, identifient l'ex\u00e9cution unique du processus, que nous appelons une t\u00e2che. Ces nombres sont \u00e9galement le pr\u00e9fixe des r\u00e9pertoires o\u00f9 chaque t\u00e2che est ex\u00e9cut\u00e9e. Vous pouvez inspecter les fichiers produits en allant dans le r\u00e9pertoire <code>$PWD/work</code> et en utilisant ces num\u00e9ros pour trouver le chemin d'ex\u00e9cution sp\u00e9cifique \u00e0 la t\u00e2che.</p> <p>Tip</p> <p>Le second processus s'ex\u00e9cute deux fois, dans deux r\u00e9pertoires de travail diff\u00e9rents pour chaque fichier d'entr\u00e9e. La sortie du journal ANSI de Nextflow se rafra\u00eechit dynamiquement au fur et \u00e0 mesure que le workflow s'ex\u00e9cute ; dans l'exemple pr\u00e9c\u00e9dent, le r\u00e9pertoire de travail <code>[37/b9332f]</code> est le second des deux r\u00e9pertoires qui ont \u00e9t\u00e9 trait\u00e9s (en \u00e9crasant le journal avec le premier). Pour imprimer tous les chemins pertinents \u00e0 l'\u00e9cran, d\u00e9sactivez la sortie du journal ANSI en utilisant l'option <code>-ansi-log</code> (par exemple, <code>nextflow run hello.nf -ansi-log false</code>).</p> <p>Il faut noter que le processus <code>CONVERTTOUPPER</code> est ex\u00e9cut\u00e9 en parall\u00e8le, donc il n'y a aucune garantie que l'instance qui traite le premier split (le chunk Hello ') sera ex\u00e9cut\u00e9e avant celle qui traite le second split (le chunk 'world!).</p> <p>Il se peut donc que le r\u00e9sultat final soit imprim\u00e9 dans un ordre diff\u00e9rent :</p> <pre><code>WORLD!\nHELLO\n</code></pre>"},{"location":"basic_training/intro.fr/#modifier-et-reprendre","title":"Modifier et reprendre","text":"<p>Nextflow garde la trace de tous les processus ex\u00e9cut\u00e9s dans votre workflow. Si vous modifiez certaines parties de votre script, seuls les processus modifi\u00e9s seront r\u00e9-ex\u00e9cut\u00e9s. L'ex\u00e9cution des processus qui n'ont pas \u00e9t\u00e9 modifi\u00e9s sera ignor\u00e9e et le r\u00e9sultat mis en cache sera utilis\u00e9 \u00e0 la place.</p> <p>Cela permet de tester ou de modifier une partie de votre flux de travail sans avoir \u00e0 le r\u00e9ex\u00e9cuter depuis le d\u00e9but.</p> <p>Pour les besoins de ce tutoriel, modifiez le processus <code>CONVERTTOUPPER</code> de l'exemple pr\u00e9c\u00e9dent, en rempla\u00e7ant le script du processus par la cha\u00eene <code>rev $y</code>, de fa\u00e7on \u00e0 ce que le processus ressemble \u00e0 ceci :</p> <pre><code>process CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout\nscript:\n\"\"\"\n    rev $y\n    \"\"\"\n}\n</code></pre> <p>Enregistrez ensuite le fichier sous le m\u00eame nom et ex\u00e9cutez-le en ajoutant l'option <code>-resume</code> \u00e0 la ligne de commande :</p> <pre><code>$ nextflow run hello.nf -resume\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [zen_colden] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (2)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 \u2714\n[0f/8175a7] process &gt; CONVERTTOUPPER (1) [100%] 2 of 2 \u2714\n!dlrow\n olleH\n</code></pre> <p>Vous verrez que l'ex\u00e9cution du processus <code>SPLITLETTERS</code> est ignor\u00e9e (l'ID de la t\u00e2che est le m\u00eame que dans la premi\u00e8re sortie) \u2014 ses r\u00e9sultats sont r\u00e9cup\u00e9r\u00e9s dans le cache. Le second processus est ex\u00e9cut\u00e9 comme pr\u00e9vu, imprimant les cha\u00eenes invers\u00e9es.</p> <p>Info</p> <p>Les r\u00e9sultats du workflow sont mis en cache par d\u00e9faut dans le r\u00e9pertoire <code>$PWD/work</code>. En fonction de votre script, ce dossier peut prendre beaucoup d'espace disque. Si vous \u00eates s\u00fbr de ne pas avoir besoin de reprendre l'ex\u00e9cution de votre workflow, nettoyez ce dossier p\u00e9riodiquement.</p>"},{"location":"basic_training/intro.fr/#parametres-du-workflow","title":"Param\u00e8tres du workflow","text":"<p>Les param\u00e8tres de workflow sont simplement d\u00e9clar\u00e9s en ajoutant le pr\u00e9fixe <code>params</code> au nom d'une variable, s\u00e9par\u00e9 par un caract\u00e8re point. Leur valeur peut \u00eatre sp\u00e9cifi\u00e9e sur la ligne de commande en faisant pr\u00e9c\u00e9der le nom du param\u00e8tre d'un double tiret, c'est-\u00e0-dire <code>--paramName</code>.</p> <p>Essayons maintenant d'ex\u00e9cuter l'exemple pr\u00e9c\u00e9dent en sp\u00e9cifiant un param\u00e8tre de cha\u00eene d'entr\u00e9e diff\u00e9rent, comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>La cha\u00eene sp\u00e9cifi\u00e9e sur la ligne de commande remplacera la valeur par d\u00e9faut du param\u00e8tre. La sortie ressemblera \u00e0 ceci :</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [goofy_kare] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (4)\n[8b/7c7d13] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[58/3b2df0] process &gt; CONVERTTOUPPER (3) [100%] 3 of 3 \u2714\nuojnoB\nm el r\n!edno\n</code></pre>"},{"location":"basic_training/intro.fr/#au-format-dag","title":"Au format DAG","text":"<p>Pour mieux comprendre comment Nextflow traite les donn\u00e9es dans ce workflow, voici une figure de type DAG pour visualiser toutes les <code>inputs</code>, <code>outputs</code>, <code>canaux</code> et <code>processes</code> :</p> <p></p>"},{"location":"basic_training/intro/","title":"Introduction","text":""},{"location":"basic_training/intro/#basic-concepts","title":"Basic concepts","text":"<p>Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational workflows.</p> <p>It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations.</p> <p>Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment, based on the dataflow programming model. Nextflow\u2019s core features are:</p> <ul> <li>Workflow portability and reproducibility</li> <li>Scalability of parallelization and deployment</li> <li>Integration of existing tools, systems, and industry standards</li> </ul>"},{"location":"basic_training/intro/#processes-and-channels","title":"Processes and Channels","text":"<p>In practice, a Nextflow workflow is made by joining together different processes. Each <code>process</code> can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.).</p> <p>Processes are executed independently and are isolated from each other, i.e., they do not share a common (writable) state. The only way they can communicate is via asynchronous first-in, first-out (FIFO) queues, called <code>channels</code>.</p> <p>Any <code>process</code> can define one or more <code>channels</code> as an <code>input</code> and <code>output</code>. The interaction between these processes, and ultimately the workflow execution flow itself, is implicitly defined by these <code>input</code> and <code>output</code> declarations.</p> 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 data zdata ydata xChannelProcessdata xoutput xdata youtput ydata zoutput ztask 1task 2task 3"},{"location":"basic_training/intro/#execution-abstraction","title":"Execution abstraction","text":"<p>While a <code>process</code> defines what command or <code>script</code> has to be executed, the executor determines how that <code>script</code> is run in the target platform.</p> <p>If not otherwise specified, processes are executed on the local computer. The local executor is very useful for workflow development and testing purposes, however, for real-world computational workflows a high-performance computing (HPC) or cloud platform is often required.</p> <p>In other words, Nextflow provides an abstraction between the workflow\u2019s functional logic and the underlying execution system (or runtime). Thus, it is possible to write a workflow that runs seamlessly on your computer, a cluster, or the cloud, without being modified. You simply define the target execution platform in the configuration file.</p> <p></p>"},{"location":"basic_training/intro/#scripting-language","title":"Scripting language","text":"<p>Nextflow implements a declarative DSL that simplifies the writing of complex data analysis workflows as an extension of a general-purpose programming language.</p> <p>This approach makes Nextflow flexible \u2014 it provides the benefits of a concise DSL for the handling of recurrent use cases with ease and the flexibility and power of a general-purpose programming language to handle corner cases in the same computing environment. This would be difficult to implement using a purely declarative approach.</p> <p>In practical terms, Nextflow scripting is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as \"Python for Java\", in that it simplifies the writing of code and is more approachable.</p>"},{"location":"basic_training/intro/#your-first-script","title":"Your first script","text":"<p>Here you will execute your first Nextflow script (<code>hello.nf</code>), which we will go through line-by-line.</p> <p>In this toy example, the script takes an input string (a parameter called <code>params.greeting</code>) and splits it into chunks of six characters in the first process. The second process then converts the characters to upper case. The result is finally displayed on-screen.</p>"},{"location":"basic_training/intro/#nextflow-code","title":"Nextflow code","text":"<p>Info</p> <p>Click the  icons in the code for explanations.</p> nf-training/hello.nf<pre><code>#!/usr/bin/env nextflow\n// (1)!\nparams.greeting = 'Hello world!' // (2)!\ngreeting_ch = Channel.of(params.greeting) // (3)!\nprocess SPLITLETTERS { // (4)!\ninput: // (5)!\nval x // (6)!\noutput: // (7)!\npath 'chunk_*' // (8)!\nscript: // (9)!\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} // (10)!\nprocess CONVERTTOUPPER { // (11)!\ninput: // (12)!\npath y // (13)!\noutput: // (14)!\nstdout // (15)!\nscript: // (16)!\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} // (17)!\nworkflow { // (18)!\nletters_ch = SPLITLETTERS(greeting_ch) // (19)!\nresults_ch = CONVERTTOUPPER(letters_ch.flatten()) // (20)!\nresults_ch.view { it } // (21)!\n} // (22)!\n</code></pre> <ol> <li>The code begins with a shebang, which declares Nextflow as the interpreter.</li> <li>Declares a parameter <code>greeting</code> that is initialized with the value 'Hello world!'.</li> <li>Initializes a <code>channel</code> labeled <code>greeting_ch</code>, which contains the value from <code>params.greeting</code>. Channels are the input type for processes in Nextflow.</li> <li>Begins the first process block, defined as <code>SPLITLETTERS</code>.</li> <li>Input declaration for the <code>SPLITLETTERS</code> process. Inputs can be values (<code>val</code>), files or paths (<code>path</code>), or other qualifiers (see here).</li> <li>Tells the <code>process</code> to expect an input value (<code>val</code>), that we assign to the variable 'x'.</li> <li>Output declaration for the <code>SPLITLETTERS</code> process.</li> <li>Tells the process to expect an output file(s) (<code>path</code>), with a filename starting with 'chunk_', as output from the script. The process sends the output as a channel.</li> <li>Three double quotes start and end the code block to execute this <code>process</code>.    Inside is the code to execute \u2014 printing the <code>input</code> value x (called using the dollar symbol [$] prefix), splitting the string into chunks with a length of 6 characters (\"Hello \" and \"world!\"), and saving each to a file (chunk_aa and chunk_ab).</li> <li>End of the first process block.</li> <li>Begins the second process block, defined as <code>CONVERTTOUPPER</code>.</li> <li>Input declaration for the <code>CONVERTTOUPPER</code> <code>process</code>.</li> <li>Tells the <code>process</code> to expect an <code>input</code> file(s) (<code>path</code>; i.e. chunk_aa and chunk_ab), that we assign to the variable 'y'.</li> <li>Output declaration for the <code>CONVERTTOUPPER</code> process.</li> <li>Tells the process to expect output as standard output (stdout) and sends this output as a channel.</li> <li>Three double quotes start and end the code block to execute this <code>process</code>.     Within the block there is a script to read files (cat) using the '$y' input variable, then pipe to uppercase conversion, outputting to standard output.</li> <li>End of second <code>process</code> block.</li> <li>Start of the workflow scope where each process can be called.</li> <li>Execute the <code>process</code> <code>SPLITLETTERS</code> on the <code>greeting_ch</code> (aka greeting channel), and store the output in the channel <code>letters_ch</code>.</li> <li>Execute the <code>process</code> <code>CONVERTTOUPPER</code> on the letters channel <code>letters_ch</code>, which is flattened using the operator <code>.flatten()</code>. This transforms the input channel in such a way that every item is a separate element. We store the output in the channel <code>results_ch</code>.</li> <li>The final output (in the <code>results_ch</code> channel) is printed to screen using the <code>view</code> operator (appended onto the channel name).</li> <li>End of the workflow scope.</li> </ol> <p>The use of the operator <code>.flatten()</code> here is to split the two files into two separate items to be put through the next process (else they would be treated as a single element).</p>"},{"location":"basic_training/intro/#in-practice","title":"In practice","text":"<p>Now copy the above example into your favorite text editor and save it to a file named <code>hello.nf</code>.</p> <p>Warning</p> <p>For the Gitpod tutorial, make sure you are in the folder called <code>nf-training</code></p> <p>Execute the script by entering the following command in your terminal:</p> <pre><code>nextflow run hello.nf\n</code></pre> <p>The output will look similar to the text shown below:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [cheeky_keller] DSL2 - revision: 197a0e289a\nexecutor &gt;  local (3)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[37/b9332f] process &gt; CONVERTTOUPPER (2) [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n</code></pre> <p>The standard output shows (line by line):</p> <ol> <li>The version of Nextflow that was executed.</li> <li>The script and version names.</li> <li>The executor used (in the above case: local).</li> <li>The first <code>process</code> is executed once, which means there is one task. The line starts with a unique hexadecimal value (see TIP below), and ends with the percentage and other task completion information.</li> <li>The second process is executed twice (once for chunk_aa and once for chunk_ab), which means two tasks.</li> <li>The result string from stdout is printed.</li> </ol> <p>Info</p> <p>The hexadecimal numbers, like <code>31/52c31e</code>, identify the unique process execution, that we call a task. These numbers are also the prefix of the directories where each task is executed. You can inspect the files produced by changing to the directory <code>$PWD/work</code> and using these numbers to find the task-specific execution path.</p> <p>Tip</p> <p>The second process runs twice, executing in two different work directories for each input file. The ANSI log output from Nextflow dynamically refreshes as the workflow runs; in the previous example the work directory <code>[37/b9332f]</code> is the second of the two directories that were processed (overwriting the log with the first). To print all the relevant paths to the screen, disable the ANSI log output using the <code>-ansi-log</code> flag (e.g., <code>nextflow run hello.nf -ansi-log false</code>).</p> <p>It\u2019s worth noting that the process <code>CONVERTTOUPPER</code> is executed in parallel, so there\u2019s no guarantee that the instance processing the first split (the chunk Hello ') will be executed before the one processing the second split (the chunk 'world!).</p> <p>Thus, it could be that your final result will be printed out in a different order:</p> <pre><code>WORLD!\nHELLO\n</code></pre>"},{"location":"basic_training/intro/#modify-and-resume","title":"Modify and resume","text":"<p>Nextflow keeps track of all the processes executed in your workflow. If you modify some parts of your script, only the processes that are changed will be re-executed. The execution of the processes that are not changed will be skipped and the cached result will be used instead.</p> <p>This allows for testing or modifying part of your workflow without having to re-execute it from scratch.</p> <p>For the sake of this tutorial, modify the <code>CONVERTTOUPPER</code> process in the previous example, replacing the process script with the string <code>rev $y</code>, so that the process looks like this:</p> <pre><code>process CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout\nscript:\n\"\"\"\n    rev $y\n    \"\"\"\n}\n</code></pre> <p>Then save the file with the same name, and execute it by adding the <code>-resume</code> option to the command line:</p> <pre><code>$ nextflow run hello.nf -resume\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [zen_colden] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (2)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 \u2714\n[0f/8175a7] process &gt; CONVERTTOUPPER (1) [100%] 2 of 2 \u2714\n!dlrow\n olleH\n</code></pre> <p>You will see that the execution of the process <code>SPLITLETTERS</code> is skipped (the task ID is the same as in the first output) \u2014 its results are retrieved from the cache. The second process is executed as expected, printing the reversed strings.</p> <p>Info</p> <p>The workflow results are cached by default in the directory <code>$PWD/work</code>. Depending on your script, this folder can take up a lot of disk space. If you are sure you won\u2019t need to resume your workflow execution, clean this folder periodically.</p>"},{"location":"basic_training/intro/#workflow-parameters","title":"Workflow parameters","text":"<p>Workflow parameters are simply declared by prepending the prefix <code>params</code> to a variable name, separated by a dot character. Their value can be specified on the command line by prefixing the parameter name with a double dash character, i.e. <code>--paramName</code>.</p> <p>Now, let\u2019s try to execute the previous example specifying a different input string parameter, as shown below:</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>The string specified on the command line will override the default value of the parameter. The output will look like this:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [goofy_kare] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (4)\n[8b/7c7d13] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[58/3b2df0] process &gt; CONVERTTOUPPER (3) [100%] 3 of 3 \u2714\nuojnoB\nm el r\n!edno\n</code></pre>"},{"location":"basic_training/intro/#in-dag-like-format","title":"In DAG-like format","text":"<p>To better understand how Nextflow is dealing with the data in this workflow, below is a DAG-like figure to visualize all the <code>inputs</code>, <code>outputs</code>, <code>channels</code> and <code>processes</code>:</p> <p></p>"},{"location":"basic_training/intro.pt/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"basic_training/intro.pt/#conceitos-basicos","title":"Conceitos b\u00e1sicos","text":"<p>O Nextflow \u00e9 tanto um motor de orquestra\u00e7\u00e3o de fluxo de trabalho quanto uma linguagem de dom\u00ednio espec\u00edfico (Domain-Specific Language - DSL) que facilita a escrita de fluxos de trabalho computacionais que fazem uso intensivo de dados.</p> <p>Ele foi projetado com base na ideia de que a plataforma Linux \u00e9 a l\u00edngua franca da ci\u00eancia de dados. O Linux fornece muitas ferramentas de linha de comando que, ainda que simples, s\u00e3o poderosas ferramentas de script que, quando encadeadas, facilitam manipula\u00e7\u00f5es complexas de dados.</p> <p>O Nextflow estende essa abordagem, adicionando a capacidade de definir intera\u00e7\u00f5es complexas entre programas e um ambiente de computa\u00e7\u00e3o paralela de alto n\u00edvel, baseado no modelo de programa\u00e7\u00e3o Dataflow. Os principais recursos do Nextflow s\u00e3o:</p> <ul> <li>Portabilidade e reprodutibilidade de fluxos de trabalho</li> <li>Escalabilidade na paraleliza\u00e7\u00e3o e na implanta\u00e7\u00e3o</li> <li>Integra\u00e7\u00e3o de ferramentas j\u00e1 existentes, sistemas e padr\u00f5es da ind\u00fastria</li> </ul>"},{"location":"basic_training/intro.pt/#processos-e-canais","title":"Processos e Canais","text":"<p>Na pr\u00e1tica, um fluxo de trabalho Nextflow \u00e9 feito juntando diferentes processos. Cada processo pode ser escrito em qualquer linguagem de script que possa ser executada pela plataforma Linux (Bash, Perl, Ruby, Python, etc.).</p> <p>Os processos s\u00e3o executados de forma independente e isolados uns dos outros, ou seja, n\u00e3o compartilham um estado (grav\u00e1vel) comum. A \u00fanica maneira de eles se comunicarem \u00e9 por meio de filas ass\u00edncronas, chamadas de <code>canais</code>, onde o primeiro elemento a entrar, \u00e9 o primeiro a sair (FIFO - First-in-First-out).</p> <p>Qualquer processo pode definir um ou mais <code>canais</code> como uma <code>entrada</code> e <code>sa\u00edda</code>. A intera\u00e7\u00e3o entre esses processos e, em \u00faltima an\u00e1lise, o pr\u00f3prio fluxo de execu\u00e7\u00e3o do fluxo de trabalho, \u00e9 definido implicitamente por essas declara\u00e7\u00f5es de <code>entrada</code> e <code>sa\u00edda</code>.</p> dado zdado ydado xCanalProcessodado xsa\u00edda xdado ysa\u00edda ydado zsa\u00edda ztarefa 1tarefa 2tarefa 3"},{"location":"basic_training/intro.pt/#abstracao-de-execucao","title":"Abstra\u00e7\u00e3o de execu\u00e7\u00e3o","text":"<p>Enquanto um processo define qual comando ou <code>script</code> deve ser executado, o executor determina como esse <code>script</code> \u00e9 executado na plataforma alvo.</p> <p>Se n\u00e3o for especificado de outra forma, os processos s\u00e3o executados no computador local. O executor local \u00e9 muito \u00fatil para fins de desenvolvimento e teste de fluxos de trabalho, no entanto, para fluxos de trabalho computacionais do mundo real, uma plataforma de computa\u00e7\u00e3o de alto desempenho (High-Performance Computing - HPC) ou de computa\u00e7\u00e3o em nuvem geralmente \u00e9 necess\u00e1ria.</p> <p>Em outras palavras, o Nextflow fornece uma abstra\u00e7\u00e3o entre a l\u00f3gica funcional do fluxo de trabalho e o sistema de execu\u00e7\u00e3o subjacente (ou sistema de tempo de execu\u00e7\u00e3o). Assim, \u00e9 poss\u00edvel escrever um fluxo de trabalho que seja executado perfeitamente em seu computador, em um cluster ou na nuvem, sem ser modificado. Voc\u00ea simplesmente define a plataforma de execu\u00e7\u00e3o alvo no arquivo de configura\u00e7\u00e3o.</p> <p></p>"},{"location":"basic_training/intro.pt/#linguagem-de-script","title":"Linguagem de script","text":"<p>O Nextflow implementa uma DSL declarativa que simplifica a escrita de fluxos de trabalho complexos de an\u00e1lise de dados como uma extens\u00e3o de uma linguagem de programa\u00e7\u00e3o de uso geral.</p> <p>Essa abordagem torna o Nextflow flex\u00edvel \u2014 ele fornece os benef\u00edcios de uma DSL concisa para lidar com casos de uso recorrentes com facilidade e a flexibilidade e o poder de uma linguagem de programa\u00e7\u00e3o de prop\u00f3sito geral para lidar com casos extremos no mesmo ambiente de computa\u00e7\u00e3o. Isso seria dif\u00edcil de implementar usando uma abordagem puramente declarativa.</p> <p>Em termos pr\u00e1ticos, a linguagem de script Nextflow \u00e9 uma extens\u00e3o da linguagem de programa\u00e7\u00e3o Groovy a qual, por sua vez, \u00e9 um superconjunto da linguagem de programa\u00e7\u00e3o Java. Groovy pode ser pensado como \"Python para Java\", pois simplifica a escrita do c\u00f3digo e \u00e9 mais acess\u00edvel.</p>"},{"location":"basic_training/intro.pt/#seu-primeiro-script","title":"Seu primeiro script","text":"<p>Aqui voc\u00ea executar\u00e1 seu primeiro script Nextflow (<code>hello.nf</code>), que veremos linha por linha.</p> <p>Neste exemplo ilustrativo, o script recebe no primeiro processo uma string de entrada (um par\u00e2metro chamado <code>params.greeting</code>) e a divide em blocos de seis caracteres. O segundo processo converte os caracteres em mai\u00fasculas. O resultado \u00e9 ent\u00e3o finalmente exibido na tela.</p>"},{"location":"basic_training/intro.pt/#codigo-em-nextflow","title":"C\u00f3digo em Nextflow","text":"<p>Info</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> nf-training/hello.nf<pre><code>#!/usr/bin/env nextflow\n// (1)!\nparams.greeting = 'Hello world!' // (2)!\ngreeting_ch = Channel.of(params.greeting) // (3)!\nprocess SPLITLETTERS { // (4)!\ninput: // (5)!\nval x // (6)!\noutput: // (7)!\npath 'chunk_*' // (8)!\nscript: // (9)!\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n} // (10)!\nprocess CONVERTTOUPPER { // (11)!\ninput: // (12)!\npath y // (13)!\noutput: // (14)!\nstdout // (15)!\nscript: // (16)!\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n} // (17)!\nworkflow { // (18)!\nletters_ch = SPLITLETTERS(greeting_ch) // (19)!\nresults_ch = CONVERTTOUPPER(letters_ch.flatten()) // (20)!\nresults_ch.view { it } // (21)!\n} // (22)!\n</code></pre> <ol> <li>O c\u00f3digo come\u00e7a com um shebang, que declara o Nextflow como o interpretador.</li> <li>Declara um par\u00e2metro <code>greeting</code> que \u00e9 inicializado com o valor 'Hello world!'.</li> <li>Inicializa um canal chamado <code>greeting_ch</code>, que cont\u00e9m o valor de <code>params.greeting</code>. Os canais s\u00e3o o tipo de entrada para processos no Nextflow.</li> <li>Inicia o primeiro bloco do processo, definido como <code>SPLITLETTERS</code>.</li> <li>Declara\u00e7\u00e3o de entrada para o processo <code>SPLITLETTERS</code>. As entradas podem ser valores (<code>val</code>), arquivos ou caminhos (<code>path</code>), ou ainda outros qualificadores (veja aqui).</li> <li>Diz ao processo para esperar um valor de entrada (<code>val</code>), que atribu\u00edmos \u00e0 vari\u00e1vel 'x'.</li> <li>Declara\u00e7\u00e3o de sa\u00edda para o processo <code>SPLITLETTERS</code>.</li> <li>Diz ao processo para esperar um ou mais arquivos de sa\u00edda (<code>path</code>), com um nome de arquivo come\u00e7ando com 'chunk_', como sa\u00edda do script. O processo envia a sa\u00edda como um canal.</li> <li>Tr\u00eas aspas duplas iniciam e terminam o bloco de c\u00f3digo para executar este processo. Dentro est\u00e1 o c\u00f3digo a ser executado \u2014 imprimindo o valor de <code>entrada</code> x (chamado usando o prefixo do s\u00edmbolo de d\u00f3lar [$]), dividindo a string em peda\u00e7os com um comprimento de 6 caracteres (\"Hello \" e \"world!\") e salvando cada um para um arquivo (chunk_aa e chunk_ab).</li> <li>Fim do primeiro bloco de processo.</li> <li>Inicia o segundo bloco de processo, definido como <code>CONVERTTOUPPER</code>.</li> <li>Declara\u00e7\u00e3o de entrada para o processo <code>CONVERTTOUPPER</code>.</li> <li>Diz ao processo para esperar um ou mais arquivos de <code>entrada</code> (<code>path</code>; ou seja, chunk_aa e chunk_ab), que atribu\u00edmos \u00e0 vari\u00e1vel 'y'.</li> <li>Declara\u00e7\u00e3o de sa\u00edda para o processo <code>CONVERTTOUPPER</code>.</li> <li>Diz ao processo para esperar a sa\u00edda padr\u00e3o (stdout) como sa\u00edda e envia essa sa\u00edda como um canal.</li> <li>Tr\u00eas aspas duplas iniciam e terminam o bloco de c\u00f3digo para executar este processo. Dentro do bloco, h\u00e1 um script para ler arquivos (cat) usando a vari\u00e1vel de entrada '$y' e, em seguida, um pipe (|) para a convers\u00e3o em mai\u00fasculas, imprimindo na sa\u00edda padr\u00e3o.</li> <li>Fim do segundo bloco de processo.</li> <li>In\u00edcio do bloco de fluxo de trabalho (<code>workflow</code>) onde cada processo pode ser chamado.</li> <li>Execute o processo <code>SPLITLETTERS</code> no <code>greeting_ch</code> (tamb\u00e9m conhecido como canal de sauda\u00e7\u00e3o) e armazene a sa\u00edda no canal <code>letters_ch</code>.</li> <li>Execute o processo <code>CONVERTTOUPPER</code> no canal de letras <code>letters_ch</code>, que \u00e9 achatado usando o operador <code>.flatten()</code>. Isso transforma o canal de entrada de forma que cada item seja um elemento separado. Armazenamos a sa\u00edda no canal <code>results_ch</code>.</li> <li>A sa\u00edda final (no canal <code>results_ch</code>) \u00e9 impressa na tela usando o operador <code>view</code> (aplicado ao nome do canal).</li> <li>Fim do bloco do fluxo de trabalho (<code>workflow</code>).</li> </ol> <p>O uso do operador <code>.flatten()</code> aqui \u00e9 para dividir os dois arquivos em dois itens separados para serem colocados no pr\u00f3ximo processo (caso contr\u00e1rio, eles seriam tratados como um \u00fanico elemento).</p>"},{"location":"basic_training/intro.pt/#hora-de-praticar","title":"Hora de praticar","text":"<p>Agora copie o exemplo acima em seu editor de texto favorito e salve-o em um arquivo chamado <code>hello.nf</code>.</p> <p>Warning</p> <p>Para o tutorial do Gitpod, verifique se voc\u00ea est\u00e1 na pasta chamada <code>nf-training</code></p> <p>Execute o script digitando o seguinte comando em seu terminal:</p> <pre><code>nextflow run hello.nf\n</code></pre> <p>A sa\u00edda ser\u00e1 semelhante ao texto mostrado abaixo:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [cheeky_keller] DSL2 - revision: 197a0e289a\nexecutor &gt;  local (3)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[37/b9332f] process &gt; CONVERTTOUPPER (2) [100%] 2 of 2 \u2714\nHELLO\nWORLD!\n</code></pre> <p>A sa\u00edda padr\u00e3o mostra (linha por linha):</p> <ol> <li>A vers\u00e3o do Nextflow que foi executada.</li> <li>Os nomes do script e da vers\u00e3o.</li> <li>O executor usado (no caso acima: local).</li> <li>O primeiro processo \u00e9 executado uma vez, o que significa que houve 1 tarefa. A linha come\u00e7a com um valor hexadecimal exclusivo (consulte a dica abaixo) e termina com a porcentagem e outras informa\u00e7\u00f5es de conclus\u00e3o da tarefa.</li> <li>O segundo processo \u00e9 executado duas vezes (uma vez para chunk_aa e outra para chunk_ab), o que significa duas tarefas.</li> <li>A string de resultado de stdout \u00e9 impressa na tela.</li> </ol> <p>Info</p> <p>Os n\u00fameros hexadecimais, como <code>31/52c31e</code>, identificam de forma \u00fanica a execu\u00e7\u00e3o do processo. Esses n\u00fameros tamb\u00e9m s\u00e3o o prefixo dos diret\u00f3rios onde cada tarefa \u00e9 executado. Voc\u00ea pode inspecionar os arquivos produzidos mudando para o diret\u00f3rio <code>$PWD/work</code> e usando esses n\u00fameros para encontrar o caminho de execu\u00e7\u00e3o espec\u00edfico da tarefa.</p> <p>Tip</p> <p>O segundo processo \u00e9 executado duas vezes, em dois diret\u00f3rios de trabalho diferentes para cada arquivo de entrada. A sa\u00edda de log ANSI do Nextflow \u00e9 atualizada dinamicamente conforme o fluxo de trabalho \u00e9 executado; no exemplo anterior, o diret\u00f3rio de trabalho <code>[37/b9332f]</code> \u00e9 o segundo dos dois diret\u00f3rios que foram processados (sobrescrevendo o log com o primeiro). Para imprimir para a tela todos os caminhos relevantes, desative a sa\u00edda de log ANSI usando o sinalizador <code>-ansi-log</code> (por exemplo, <code>nextflow run hello.nf -ansi-log false</code>).</p> <p>Vale ressaltar que o processo <code>CONVERTTOUPPER</code> \u00e9 executado em paralelo, portanto n\u00e3o h\u00e1 garantia de que a inst\u00e2ncia que processa a primeira divis\u00e3o (o bloco Hello) ser\u00e1 executada antes daquela que processa a segundo divis\u00e3o (o bloco world!).</p> <p>Assim, pode ser que seu resultado final seja impresso em uma ordem diferente:</p> <pre><code>WORLD!\nHELLO\n</code></pre>"},{"location":"basic_training/intro.pt/#modifique-e-retome","title":"Modifique e retome","text":"<p>O Nextflow acompanha todos os processos executados em seu fluxo de trabalho. Se voc\u00ea modificar algumas partes do seu script, apenas os processos alterados ser\u00e3o executados novamente. A execu\u00e7\u00e3o dos processos que n\u00e3o foram alterados ser\u00e1 ignorada e o resultado armazenado em cache ser\u00e1 usado em seu lugar.</p> <p>Isso permite testar ou modificar parte do fluxo de trabalho sem precisar execut\u00e1-lo novamente do zero.</p> <p>Para este tutorial, modifique o processo <code>CONVERTTOUPPER</code> do exemplo anterior, substituindo o script do processo pela string <code>rev $y</code>, para que o processo fique assim:</p> <pre><code>process CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout\nscript:\n\"\"\"\n    rev $y\n    \"\"\"\n}\n</code></pre> <p>Em seguida, salve o arquivo com o mesmo nome e execute-o adicionando a op\u00e7\u00e3o <code>-resume</code> \u00e0 linha de comando:</p> <pre><code>$ nextflow run hello.nf -resume\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [zen_colden] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (2)\n[31/52c31e] process &gt; SPLITLETTERS (1)   [100%] 1 of 1, cached: 1 \u2714\n[0f/8175a7] process &gt; CONVERTTOUPPER (1) [100%] 2 of 2 \u2714\n!dlrow\n olleH\n</code></pre> <p>Voc\u00ea ver\u00e1 que a execu\u00e7\u00e3o do processo <code>SPLITLETTERS</code> \u00e9 ignorada (o ID da tarefa \u00e9 o mesmo da primeira sa\u00edda) \u2014 seus resultados s\u00e3o recuperados do cache. O segundo processo \u00e9 executado conforme o esperado, imprimindo as strings invertidas.</p> <p>Info</p> <p>Os resultados do fluxo de trabalho s\u00e3o armazenados em cache por padr\u00e3o no diret\u00f3rio <code>$PWD/work</code>. Dependendo do seu script, esta pasta pode ocupar muito espa\u00e7o em disco. Se tiver certeza de que n\u00e3o precisar\u00e1 retomar a execu\u00e7\u00e3o do fluxo de trabalho, limpe esta pasta periodicamente.</p>"},{"location":"basic_training/intro.pt/#parametros-do-fluxo-de-trabalho","title":"Par\u00e2metros do fluxo de trabalho","text":"<p>Os par\u00e2metros de fluxo de trabalho s\u00e3o declarados simplesmente adicionando o prefixo <code>params</code> a um nome de vari\u00e1vel, separando-os por um caractere de ponto. Seu valor pode ser especificado na linha de comando prefixando o nome do par\u00e2metro com um tra\u00e7o duplo, ou seja, <code>--nomeParametro</code>.</p> <p>Agora, vamos tentar executar o exemplo anterior especificando um par\u00e2metro de string de entrada diferente, conforme mostrado abaixo:</p> <pre><code>nextflow run hello.nf --greeting 'Bonjour le monde!'\n</code></pre> <p>A string especificada na linha de comando substituir\u00e1 o valor padr\u00e3o do par\u00e2metro. A sa\u00edda ficar\u00e1 assim:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.nf` [goofy_kare] DSL2 - revision: 0676c711e8\nexecutor &gt;  local (4)\n[8b/7c7d13] process &gt; SPLITLETTERS (1)   [100%] 1 of 1 \u2714\n[58/3b2df0] process &gt; CONVERTTOUPPER (3) [100%] 3 of 3 \u2714\nuojnoB\nm el r\n!edno\n</code></pre>"},{"location":"basic_training/intro.pt/#em-formato-de-dag","title":"Em formato de DAG","text":"<p>Para entender melhor como o Nextflow est\u00e1 lidando com os dados neste fluxo de trabalho, abaixo est\u00e1 uma figura tipo DAG para visualizar todas as entradas (<code>input</code>), sa\u00eddas (<code>output</code>), canais (<code>channel</code>) e processos (<code>process</code>):</p> <p></p>"},{"location":"basic_training/modules/","title":"Modularization","text":"<p>The definition of module libraries simplifies the writing of complex data analysis workflows and makes re-use of processes much easier.</p> <p>Using the <code>hello.nf</code> example from earlier, we will convert the workflow\u2019s processes into modules, then call them within the workflow scope in a variety of ways.</p>"},{"location":"basic_training/modules/#modules","title":"Modules","text":"<p>Nextflow DSL2 allows for the definition of stand-alone module scripts that can be included and shared across multiple workflows. Each module can contain its own <code>process</code> or <code>workflow</code> definition.</p>"},{"location":"basic_training/modules/#importing-modules","title":"Importing modules","text":"<p>Components defined in the module script can be imported into other Nextflow scripts using the <code>include</code> statement. This allows you to store these components in a separate file(s) so that they can be re-used in multiple workflows.</p> <p>Using the <code>hello.nf</code> example, we can achieve this by:</p> <ul> <li>Creating a file called <code>modules.nf</code> in the top-level directory.</li> <li>Copying and pasting the two process definitions for <code>SPLITLETTERS</code> and <code>CONVERTTOUPPER</code> into <code>modules.nf</code>.</li> <li>Removing the <code>process</code> definitions in the <code>hello.nf</code> script.</li> <li>Importing the processes from <code>modules.nf</code> within the <code>hello.nf</code> script anywhere above the <code>workflow</code> definition:</li> </ul> <pre><code>include { SPLITLETTERS   } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n</code></pre> <p>Note</p> <p>In general, you would use relative paths to define the location of the module scripts using the <code>./</code> prefix.</p> <p>Exercise</p> <p>Create a <code>modules.nf</code> file with the previously defined processes from <code>hello.nf</code>. Then remove these processes from <code>hello.nf</code> and add the <code>include</code> definitions shown above.</p> Solution <p>The <code>hello.nf</code> script should look similar like this:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting  = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\ninclude { SPLITLETTERS   } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\nworkflow {\nletters_ch = SPLITLETTERS(greeting_ch)\nresults_ch = CONVERTTOUPPER(letters_ch.flatten())\nresults_ch.view { it }\n}\n</code></pre> <p>You should have the following in the file <code>./modules.nf</code>:</p> <pre><code>process SPLITLETTERS {\ninput:\nval x\noutput:\npath 'chunk_*'\nscript:\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\nprocess CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout\nscript:\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre> <p>We now have modularized processes which makes the code reusable.</p>"},{"location":"basic_training/modules/#multiple-imports","title":"Multiple imports","text":"<p>If a Nextflow module script contains multiple <code>process</code> definitions they can also be imported using a single <code>include</code> statement as shown in the example below:</p> <pre><code>include { SPLITLETTERS; CONVERTTOUPPER } from './modules.nf'\n</code></pre>"},{"location":"basic_training/modules/#module-aliases","title":"Module aliases","text":"<p>When including a module component it is possible to specify a name alias using the <code>as</code> declaration. This allows the inclusion and the invocation of the same component multiple times using different names:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\nworkflow {\nletters_ch1 = SPLITLETTERS_one(greeting_ch)\nresults_ch1 = CONVERTTOUPPER_one(letters_ch1.flatten())\nresults_ch1.view { it }\nletters_ch2 = SPLITLETTERS_two(greeting_ch)\nresults_ch2 = CONVERTTOUPPER_two(letters_ch2.flatten())\nresults_ch2.view { it }\n}\n</code></pre> <p>Exercise</p> <p>Save the previous snippet as <code>hello.2.nf</code>, and try to guess what will be shown on the screen.</p> Solution <p>The <code>hello.2.nf</code> output should look something like this:</p> Output<pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.2.nf` [crazy_shirley] DSL2 - revision: 99f6b6e40e\nexecutor &gt;  local (6)\n[2b/ec0395] process &gt; SPLITLETTERS_one (1)   [100%] 1 of 1 \u2714\n[d7/be3b77] process &gt; CONVERTTOUPPER_one (1) [100%] 2 of 2 \u2714\n[04/9ffc05] process &gt; SPLITLETTERS_two (1)   [100%] 1 of 1 \u2714\n[d9/91b029] process &gt; CONVERTTOUPPER_two (2) [100%] 2 of 2 \u2714\nWORLD!\nHELLO\nHELLO\nWORLD!\n</code></pre> <p>Tip</p> <p>You can store each process in separate files within separate sub-folders or combined in one big file (both are valid). You can find examples of this on public repos such as the Seqera RNA-Seq tutorial or within nf-core workflows, such as nf-core/rnaseq.</p>"},{"location":"basic_training/modules/#output-definition","title":"Output definition","text":"<p>Nextflow allows the use of alternative output definitions within workflows to simplify your code.</p> <p>In the previous basic example (<code>hello.nf</code>), we defined the channel names to specify the input to the next process:</p> <pre><code>workflow  {\ngreeting_ch = Channel.of(params.greeting)\nletters_ch = SPLITLETTERS(greeting_ch)\nresults_ch = CONVERTTOUPPER(letters_ch.flatten())\nresults_ch.view { it }\n}\n</code></pre> <p>Note</p> <p>We have moved the <code>greeting_ch</code> into the workflow scope for this exercise.</p> <p>We can also explicitly define the output of one channel to another using the <code>.out</code> attribute, removing the channel definitions completely:</p> <pre><code>workflow  {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.view()\n}\n</code></pre> <p>If a process defines two or more output channels, each channel can be accessed by indexing the <code>.out</code> attribute, e.g., <code>.out[0]</code>, <code>.out[1]</code>, etc. In our example we only have the <code>[0]'th</code> output:</p> <pre><code>workflow  {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out[0].view()\n}\n</code></pre> <p>Alternatively, the process <code>output</code> definition allows the use of the <code>emit</code> statement to define a named identifier that can be used to reference the channel in the external scope.</p> <p>For example, try adding the <code>emit</code> statement on the <code>CONVERTTOUPPER</code> process in your <code>modules.nf</code> file:</p> modules.nf<pre><code>process SPLITLETTERS {\ninput:\nval x\noutput:\npath 'chunk_*'\nscript:\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\nprocess CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout emit: upper\nscript:\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre> <p>Then change the workflow scope in <code>hello.nf</code> to call this specific named output (notice the added <code>.upper</code>):</p> hello.nf<pre><code>workflow {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre>"},{"location":"basic_training/modules/#using-piped-outputs","title":"Using piped outputs","text":"<p>Another way to deal with outputs in the workflow scope is to use pipes <code>|</code>.</p> <p>Exercise</p> <p>Try changing the workflow script to the snippet below:</p> <pre><code>workflow {\nChannel.of(params.greeting) | SPLITLETTERS | flatten | CONVERTTOUPPER | view\n}\n</code></pre> <p>Here we use a pipe which passed the output as a channel to the next process without the need of applying <code>.out</code> to the process name.</p>"},{"location":"basic_training/modules/#workflow-definition","title":"Workflow definition","text":"<p>The <code>workflow</code> scope allows the definition of components that define the invocation of one or more processes or operators:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\nworkflow my_workflow {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.upper.view { it }\n}\nworkflow {\nmy_workflow()\n}\n</code></pre> <p>For example, the snippet above defines a <code>workflow</code> named <code>my_workflow</code>, that can be invoked via another <code>workflow</code> definition.</p> <p>Note</p> <p>Make sure that your <code>modules.nf</code> file is the one containing the <code>emit</code> on the <code>CONVERTTOUPPER</code> process.</p> <p>Warning</p> <p>A workflow component can access any variable or parameter defined in the outer scope. In the running example, we can also access <code>params.greeting</code> directly within the <code>workflow</code> definition.</p>"},{"location":"basic_training/modules/#workflow-inputs","title":"Workflow inputs","text":"<p>A <code>workflow</code> component can declare one or more input channels using the <code>take</code> statement. For example:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\nworkflow my_workflow {\ntake:\ngreeting\nmain:\nSPLITLETTERS(greeting)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre> <p>Note</p> <p>When the <code>take</code> statement is used, the <code>workflow</code> definition needs to be declared within the <code>main</code> block.</p> <p>The input for the <code>workflow</code> can then be specified as an argument:</p> <pre><code>workflow {\nmy_workflow(Channel.of(params.greeting))\n}\n</code></pre>"},{"location":"basic_training/modules/#workflow-outputs","title":"Workflow outputs","text":"<p>A <code>workflow</code> can declare one or more output channels using the <code>emit</code> statement. For example:</p> <pre><code>workflow my_workflow {\ntake:\ngreeting\nmain:\nSPLITLETTERS(greeting)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nemit:\nCONVERTTOUPPER.out.upper\n}\nworkflow {\nmy_workflow(Channel.of(params.greeting))\nmy_workflow.out.view()\n}\n</code></pre> <p>As a result, we can use the <code>my_workflow.out</code> notation to access the outputs of <code>my_workflow</code> in the invoking <code>workflow</code>.</p> <p>We can also declare named outputs within the <code>emit</code> block.</p> <pre><code>workflow my_workflow {\ntake:\ngreeting\nmain:\nSPLITLETTERS(greeting)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nemit:\nmy_data = CONVERTTOUPPER.out.upper\n}\nworkflow {\nmy_workflow(Channel.of(params.greeting))\nmy_workflow.out.my_data.view()\n}\n</code></pre> <p>The result of the above snippet can then be accessed using <code>my_workflow.out.my_data</code>.</p>"},{"location":"basic_training/modules/#calling-named-workflows","title":"Calling named workflows","text":"<p>Within a <code>main.nf</code> script (called <code>hello.nf</code> in our example) we also can have multiple workflows. In which case we may want to call a specific workflow when running the code. For this we use the entrypoint call <code>-entry &lt;workflow_name&gt;</code>.</p> <p>The following snippet has two named workflows (<code>my_workflow_one</code> and <code>my_workflow_two</code>):</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\nworkflow my_workflow_one {\nletters_ch1 = SPLITLETTERS_one(params.greeting)\nresults_ch1 = CONVERTTOUPPER_one(letters_ch1.flatten())\nresults_ch1.view { it }\n}\nworkflow my_workflow_two {\nletters_ch2 = SPLITLETTERS_two(params.greeting)\nresults_ch2 = CONVERTTOUPPER_two(letters_ch2.flatten())\nresults_ch2.view { it }\n}\nworkflow {\nmy_workflow_one(Channel.of(params.greeting))\nmy_workflow_two(Channel.of(params.greeting))\n}\n</code></pre> <p>You can choose which workflow to run by using the <code>entry</code> flag:</p> <pre><code>nextflow run hello.2.nf -entry my_workflow_one\n</code></pre>"},{"location":"basic_training/modules/#parameter-scopes","title":"Parameter scopes","text":"<p>A module script can define one or more parameters or custom functions using the same syntax as with any other Nextflow script. Using the minimal examples below:</p> Module script (<code>./modules.nf</code>)<pre><code>params.foo = 'Hello'\nparams.bar = 'world!'\ndef SAYHELLO() {\nprintln \"$params.foo $params.bar\"\n}\n</code></pre> Main script (<code>./hello.nf</code>)<pre><code>#!/usr/bin/env nextflow\nparams.foo = 'Hola'\nparams.bar = 'mundo!'\ninclude { SAYHELLO } from './modules.nf'\nworkflow {\nSAYHELLO()\n}\n</code></pre> <p>Running <code>hello.nf</code> should print:</p> <pre><code>Hola mundo!\n</code></pre> <p>As highlighted above, the script will print <code>Hola mundo!</code> instead of <code>Hello world!</code> because parameters inherited from the including context are overwritten by the definitions in the script file where they're being included.</p> <p>Info</p> <p>To avoid being ignored, workflow parameters should be defined at the beginning of the script before any <code>include</code> declarations.</p> <p>The <code>addParams</code> option can be used to extend the module parameters without affecting the external scope. For example:</p> <pre><code>#!/usr/bin/env nextflow\nparams.foo = 'Hola'\nparams.bar = 'mundo!'\ninclude { SAYHELLO } from './modules.nf' addParams(foo: 'Ol\u00e1')\nworkflow {\nSAYHELLO()\n}\n</code></pre> <p>Executing the main script above should print:</p> <pre><code>Ol\u00e1 mundo!\n</code></pre>"},{"location":"basic_training/modules/#dsl2-migration-notes","title":"DSL2 migration notes","text":"<p>To view a summary of the changes introduced when Nextflow migrated from DSL1 to DSL2 please refer to the DSL2 migration notes in the official Nextflow documentation.</p>"},{"location":"basic_training/modules.pt/","title":"Modulariza\u00e7\u00e3o","text":"<p>A defini\u00e7\u00e3o de bibliotecas modulares simplifica a escrita de fluxos de trabalho complexos de an\u00e1lise de dados, al\u00e9m tornar o reuso de processos mais f\u00e1cil.</p> <p>Ao usar o exemplo <code>hello.nf</code> da se\u00e7\u00e3o de introdu\u00e7\u00e3o, n\u00f3s converteremos os processos do fluxo de trabalho em m\u00f3dulos e, em seguida, executaremos estes processos dentro do escopo <code>workflow</code> de diferentes formas.</p>"},{"location":"basic_training/modules.pt/#modulos","title":"M\u00f3dulos","text":"<p>A DSL2 do Nextflow permite a defini\u00e7\u00e3o de scripts de m\u00f3dulos aut\u00f4nomos que podem ser inclu\u00eddos e compartilhados em v\u00e1rios fluxos de trabalho. Cada m\u00f3dulo pode conter sua pr\u00f3pria defini\u00e7\u00e3o de <code>process</code> ou <code>workflow</code>.</p>"},{"location":"basic_training/modules.pt/#importando-modulos","title":"Importando m\u00f3dulos","text":"<p>Os componentes definidos no script do m\u00f3dulo podem ser importados para outros scripts do Nextflow usando a instru\u00e7\u00e3o <code>include</code>. Isso permite que voc\u00ea armazene esses componentes em arquivos separados para que possam ser reutilizados em v\u00e1rios fluxos de trabalho.</p> <p>Usando o exemplo <code>hello.nf</code>, podemos fazer isso:</p> <ul> <li>Criando um arquivo chamado <code>modules.nf</code> no mesmo diret\u00f3rio do <code>hello.nf</code>.</li> <li>Copiando e colando as duas defini\u00e7\u00f5es de processo para <code>SPLITLETTERS</code> e <code>CONVERTTOUPPER</code> em <code>modules.nf</code>.</li> <li>Removendo as defini\u00e7\u00f5es <code>process</code> no script <code>hello.nf</code>.</li> <li>Importando os processos de <code>modules.nf</code> dentro do script <code>hello.nf</code> em qualquer lugar acima da defini\u00e7\u00e3o de <code>workflow</code>:</li> </ul> <pre><code>include { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\n</code></pre> <p>Note</p> <p>Em geral, voc\u00ea deve usar caminhos relativos para definir a localiza\u00e7\u00e3o dos scripts do m\u00f3dulo usando o prefixo <code>./</code>.</p> <p>Exercise</p> <p>Crie um arquivo <code>modules.nf</code> com os processos previamente definidos no script <code>hello.nf</code>. Em seguida, remova esses processos de <code>hello.nf</code> e adicione as defini\u00e7\u00f5es <code>include</code> mostradas acima.</p> Solution <p>O script <code>hello.nf</code> deve ser similar a este:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting  = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\ninclude { SPLITLETTERS   } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\nworkflow {\nletters_ch = SPLITLETTERS(greeting_ch)\nresults_ch = CONVERTTOUPPER(letters_ch.flatten())\nresults_ch.view { it }\n}\n</code></pre> <p>Voc\u00ea deve ter o seguinte c\u00f3digo em <code>./modules.nf</code>:</p> <pre><code>process SPLITLETTERS {\ninput:\nval x\noutput:\npath 'chunk_*'\nscript:\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\nprocess CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout\nscript:\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre> <p>Agora n\u00f3s modularizamos os processos, o que faz com que o c\u00f3digo seja reutiliz\u00e1vel.</p>"},{"location":"basic_training/modules.pt/#importacoes-multiplas","title":"Importa\u00e7\u00f5es m\u00faltiplas","text":"<p>Se um script de m\u00f3dulo Nextflow contiver v\u00e1rias defini\u00e7\u00f5es de <code>process</code>, elas tamb\u00e9m podem ser importadas usando uma \u00fanica instru\u00e7\u00e3o <code>include</code>, conforme mostrado no exemplo abaixo:</p> <pre><code>include { SPLITLETTERS; CONVERTTOUPPER } from './modules.nf'\n</code></pre>"},{"location":"basic_training/modules.pt/#apelidos-dos-modulos","title":"Apelidos dos m\u00f3dulos","text":"<p>Ao incluir um componente de um m\u00f3dulo, \u00e9 poss\u00edvel especificar um apelido para os processos usando a declara\u00e7\u00e3o <code>as</code>. Isso permite a inclus\u00e3o e a invoca\u00e7\u00e3o do mesmo componente v\u00e1rias vezes usando diferentes apelidos:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ngreeting_ch = Channel.of(params.greeting)\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\nworkflow {\nletters_ch1 = SPLITLETTERS_one(greeting_ch)\nresults_ch1 = CONVERTTOUPPER_one(letters_ch1.flatten())\nresults_ch1.view { it }\nletters_ch2 = SPLITLETTERS_two(greeting_ch)\nresults_ch2 = CONVERTTOUPPER_two(letters_ch2.flatten())\nresults_ch2.view { it }\n}\n</code></pre> <p>Exercise</p> <p>Salve o trecho anterior como <code>hello.2.nf</code>, e tente adivinhar qual sa\u00edda ser\u00e1 mostrada na tela.</p> Solution <p>A sa\u00edda de <code>hello.2.nf</code> deve ser semelhante a essa:</p> Output<pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `hello.2.nf` [crazy_shirley] DSL2 - revision: 99f6b6e40e\nexecutor &gt;  local (6)\n[2b/ec0395] process &gt; SPLITLETTERS_one (1)   [100%] 1 of 1 \u2714\n[d7/be3b77] process &gt; CONVERTTOUPPER_one (1) [100%] 2 of 2 \u2714\n[04/9ffc05] process &gt; SPLITLETTERS_two (1)   [100%] 1 of 1 \u2714\n[d9/91b029] process &gt; CONVERTTOUPPER_two (2) [100%] 2 of 2 \u2714\nWORLD!\nHELLO\nHELLO\nWORLD!\n</code></pre> <p>Tip</p> <p>Voc\u00ea pode armazenar cada processo em arquivos separados em subpastas separadas ou combinados em um arquivo grande (ambos s\u00e3o v\u00e1lidos). Voc\u00ea pode encontrar exemplos disso em reposit\u00f3rios p\u00fablicos, como no tutorial de RNA-Seq da Seqera ou em fluxos de trabalho do nf-core, como o nf-core/rnaseq.</p>"},{"location":"basic_training/modules.pt/#definicao-de-saida","title":"Defini\u00e7\u00e3o de sa\u00edda","text":"<p>O Nextflow permite o uso de defini\u00e7\u00f5es de sa\u00edda alternativas em fluxos de trabalho para simplificar seu c\u00f3digo.</p> <p>No exemplo b\u00e1sico anterior (<code>hello.nf</code>), definimos os nomes dos canais para especificar a entrada para o pr\u00f3ximo processo:</p> <pre><code>workflow  {\ngreeting_ch = Channel.of(params.greeting)\nletters_ch = SPLITLETTERS(greeting_ch)\nresults_ch = CONVERTTOUPPER(letters_ch.flatten())\nresults_ch.view { it }\n}\n</code></pre> <p>Note</p> <p>N\u00f3s movemos o <code>greeting_ch</code> para o escopo <code>workflow</code> para este exerc\u00edcio.</p> <p>Tamb\u00e9m podemos definir explicitamente a sa\u00edda de um canal para outro usando o atributo <code>.out</code>, removendo completamente as defini\u00e7\u00f5es de canal:</p> <pre><code>workflow  {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.view()\n}\n</code></pre> <p>Se um processo define dois ou mais canais de sa\u00edda, cada canal pode ser acessado indexando o atributo <code>.out</code>, por exemplo, <code>.out[0]</code>, <code>.out[1]</code>, etc. Em nosso exemplo, temos apenas a sa\u00edda <code>[0]'th</code>:</p> <pre><code>workflow  {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out[0].view()\n}\n</code></pre> <p>Alternativamente, a defini\u00e7\u00e3o de <code>output</code> do processo permite o uso da instru\u00e7\u00e3o <code>emit</code> para definir um identificador nomeado que pode ser usado para referenciar o canal no escopo externo.</p> <p>Por exemplo, tente adicionar a instru\u00e7\u00e3o <code>emit</code> no processo <code>CONVERTTOUPPER</code> em seu arquivo <code>modules.nf</code>:</p> modules.nf<pre><code>process SPLITLETTERS {\ninput:\nval x\noutput:\npath 'chunk_*'\nscript:\n\"\"\"\n    printf '$x' | split -b 6 - chunk_\n    \"\"\"\n}\nprocess CONVERTTOUPPER {\ninput:\npath y\noutput:\nstdout emit: upper\nscript:\n\"\"\"\n    cat $y | tr '[a-z]' '[A-Z]'\n    \"\"\"\n}\n</code></pre> <p>Em seguida, altere o escopo <code>workflow</code> em <code>hello.nf</code> para chamar essa sa\u00edda nomeada espec\u00edfica (observe o <code>.upper</code> adicionado):</p> hello.nf<pre><code>workflow {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre>"},{"location":"basic_training/modules.pt/#usando-saidas-canalizadas","title":"Usando sa\u00eddas canalizadas","text":"<p>Outra maneira de lidar com as sa\u00eddas no escopo <code>workflow</code> \u00e9 usar pipes <code>|</code>.</p> <p>Exercise</p> <p>Tente alterar o script do fluxo de trabalho para o trecho abaixo:</p> <pre><code>workflow {\nChannel.of(params.greeting) | SPLITLETTERS | flatten | CONVERTTOUPPER | view\n}\n</code></pre> <p>Aqui usamos um pipe que passa a sa\u00edda de um processo como um canal para o pr\u00f3ximo processo sem a necessidade de aplicar <code>.out</code> ao nome do processo.</p>"},{"location":"basic_training/modules.pt/#definicao-do-escopo-workflow","title":"Defini\u00e7\u00e3o do escopo workflow","text":"<p>O escopo <code>workflow</code> permite a defini\u00e7\u00e3o de componentes que definem a invoca\u00e7\u00e3o de um ou mais processos ou operadores:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\nworkflow meu_fluxo_de_trabalho {\ngreeting_ch = Channel.of(params.greeting)\nSPLITLETTERS(greeting_ch)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.upper.view { it }\n}\nworkflow {\nmeu_fluxo_de_trabalho()\n}\n</code></pre> <p>Por exemplo, o trecho acima define um <code>workflow</code> chamado <code>meu_fluxo_de_trabalho</code>, que pode ser chamado por meio de outra defini\u00e7\u00e3o de <code>workflow</code>.</p> <p>Note</p> <p>Certifique-se de que seu arquivo <code>modules.nf</code> \u00e9 o que cont\u00e9m o <code>emit</code> no processo <code>CONVERTTOUPPER</code>.</p> <p>Warning</p> <p>Um componente de um fluxo de trabalho pode acessar qualquer vari\u00e1vel ou par\u00e2metro definido no escopo externo. No exemplo em execu\u00e7\u00e3o, tamb\u00e9m podemos acessar <code>params.greeting</code> diretamente na defini\u00e7\u00e3o de <code>workflow</code>.</p>"},{"location":"basic_training/modules.pt/#entradas-no-escopo-workflow","title":"Entradas no escopo workflow","text":"<p>Um componente <code>workflow</code> pode declarar um ou mais canais de entrada usando a instru\u00e7\u00e3o <code>take</code>. Por exemplo:</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ninclude { SPLITLETTERS } from './modules.nf'\ninclude { CONVERTTOUPPER } from './modules.nf'\nworkflow meu_fluxo_de_trabalho {\ntake:\ngreeting\nmain:\nSPLITLETTERS(greeting)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nCONVERTTOUPPER.out.upper.view { it }\n}\n</code></pre> <p>Note</p> <p>Quando a instru\u00e7\u00e3o <code>take</code> \u00e9 usada, a defini\u00e7\u00e3o <code>workflow</code> precisa ser declarada dentro do bloco <code>main</code>.</p> <p>A entrada para o <code>workflow</code> pode ent\u00e3o ser especificada como um argumento:</p> <pre><code>workflow {\nmeu_fluxo_de_trabalho(Channel.of(params.greeting))\n}\n</code></pre>"},{"location":"basic_training/modules.pt/#saidas-no-escopo-workflow","title":"Sa\u00eddas no escopo workflow","text":"<p>Um bloco <code>workflow</code> pode declarar um ou mais canais de sa\u00edda usando a instru\u00e7\u00e3o <code>emit</code>. Por exemplo:</p> <pre><code>workflow meu_fluxo_de_trabalho {\ntake:\ngreeting\nmain:\nSPLITLETTERS(greeting)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nemit:\nCONVERTTOUPPER.out.upper\n}\nworkflow {\nmeu_fluxo_de_trabalho(Channel.of(params.greeting))\nmeu_fluxo_de_trabalho.out.view()\n}\n</code></pre> <p>Como resultado, podemos usar a nota\u00e7\u00e3o <code>meu_fluxo_de_trabalho.out</code> para acessar as sa\u00eddas de <code>meu_fluxo_de_trabalho</code> na chamada <code>workflow</code>.</p> <p>Tamb\u00e9m podemos declarar sa\u00eddas nomeadas dentro do bloco <code>emit</code>.</p> <pre><code>workflow meu_fluxo_de_trabalho {\ntake:\ngreeting\nmain:\nSPLITLETTERS(greeting)\nCONVERTTOUPPER(SPLITLETTERS.out.flatten())\nemit:\nmeus_dados = CONVERTTOUPPER.out.upper\n}\nworkflow {\nmeu_fluxo_de_trabalho(Channel.of(params.greeting))\nmeu_fluxo_de_trabalho.out.meus_dados.view()\n}\n</code></pre> <p>O resultado do trecho de c\u00f3digo acima pode ser acessado usando <code>meu_fluxo_de_trabalho.out.meus_dados</code>.</p>"},{"location":"basic_training/modules.pt/#chamando-escopos-workflows-nomeados","title":"Chamando escopos workflows nomeados","text":"<p>Dentro de um script <code>main.nf</code> (chamado <code>hello.nf</code> em nosso exemplo), tamb\u00e9m podemos ter v\u00e1rios fluxos de trabalho. Nesse caso, podemos chamar um fluxo de trabalho espec\u00edfico ao executar o c\u00f3digo. Para isso, usamos a chamada de ponto de entrada <code>-entry &lt;nome_do_flux_de_trabalho&gt;</code>.</p> <p>O trecho a seguir tem dois fluxos de trabalho nomeados (<code>meu_fluxo_de_trabalho_um</code> e <code>meu_fluxo_de_trabalho_dois</code>):</p> <pre><code>#!/usr/bin/env nextflow\nparams.greeting = 'Hello world!'\ninclude { SPLITLETTERS as SPLITLETTERS_one } from './modules.nf'\ninclude { SPLITLETTERS as SPLITLETTERS_two } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_one } from './modules.nf'\ninclude { CONVERTTOUPPER as CONVERTTOUPPER_two } from './modules.nf'\nworkflow meu_fluxo_de_trabalho_um {\nletras_canal1 = SPLITLETTERS_one(params.greeting)\nresultados_canal1 = CONVERTTOUPPER_one(letters_ch1.flatten())\nresultados_canal1.view { it }\n}\nworkflow meu_fluxo_de_trabalho_dois {\nletras_canal2 = SPLITLETTERS_two(params.greeting)\nresultados_canal2 = CONVERTTOUPPER_two(letters_ch2.flatten())\nresultados_canal2.view { it }\n}\nworkflow {\nmeu_fluxo_de_trabalho_um(Channel.of(params.greeting))\nmeu_fluxo_de_trabalho_dois(Channel.of(params.greeting))\n}\n</code></pre> <p>Voc\u00ea pode escolher qual fluxo de trabalho \u00e9 executado usando o sinalizador <code>entry</code>:</p> <pre><code>nextflow run hello.2.nf -entry meu_fluxo_de_trabalho_um\n</code></pre>"},{"location":"basic_training/modules.pt/#escopos-de-parametros","title":"Escopos de par\u00e2metros","text":"<p>Um script de m\u00f3dulo pode definir um ou mais par\u00e2metros ou fun\u00e7\u00f5es personalizadas usando a mesma sintaxe de qualquer outro script Nextflow. Usando os exemplos m\u00ednimos abaixo:</p> Script do m\u00f3dulo (<code>./modules.nf</code>)<pre><code>params.foo = 'Hello'\nparams.bar = 'world!'\ndef DIGAOLA() {\nprintln \"$params.foo $params.bar\"\n}\n</code></pre> Script principal (<code>./hello.nf</code>)<pre><code>#!/usr/bin/env nextflow\nparams.foo = 'Hola'\nparams.bar = 'mundo!'\ninclude { DIGAOLA } from './modules.nf'\nworkflow {\nDIGAOLA()\n}\n</code></pre> <p>A execu\u00e7\u00e3o de <code>hello.nf</code> deve imprimir:</p> <pre><code>Hola mundo!\n</code></pre> <p>Como destacado acima, o script imprimir\u00e1 <code>Hola mundo!</code> em vez de <code>Hello world!</code> porque os par\u00e2metros herdados do contexto de inclus\u00e3o s\u00e3o substitu\u00eddos pelas defini\u00e7\u00f5es no arquivo de script onde est\u00e3o sendo inclu\u00eddos.</p> <p>Info</p> <p>Para evitar que sejam ignorados, os par\u00e2metros do fluxo de trabalho devem ser definidos no in\u00edcio do script antes de qualquer declara\u00e7\u00e3o de inclus\u00e3o.</p> <p>A op\u00e7\u00e3o <code>addParams</code> pode ser usada para estender os par\u00e2metros do m\u00f3dulo sem afetar o escopo externo. Por exemplo:</p> <pre><code>#!/usr/bin/env nextflow\nparams.foo = 'Hola'\nparams.bar = 'mundo!'\ninclude { DIGAOLA } from './modules.nf' addParams(foo: 'Ol\u00e1')\nworkflow {\nDIGAOLA()\n}\n</code></pre> <p>A execu\u00e7\u00e3o do script principal acima deve imprimir:</p> <pre><code>Ol\u00e1 mundo!\n</code></pre>"},{"location":"basic_training/modules.pt/#notas-de-migracao-dsl2","title":"Notas de migra\u00e7\u00e3o DSL2","text":"<p>Para visualizar um resumo das altera\u00e7\u00f5es introduzidas quando o Nextflow migrou da DSL1 para a DSL2, consulte as notas de migra\u00e7\u00e3o da DSL2 na documenta\u00e7\u00e3o oficial do Nextflow.</p>"},{"location":"basic_training/operators/","title":"Operators","text":"<p>Operators are methods that allow you to connect channels, transform values emitted by a channel, or apply some user-provided rules.</p> <p>There are seven main groups of operators are described in greater detail within the Nextflow Reference Documentation, linked below:</p> <ol> <li>Filtering operators</li> <li>Transforming operators</li> <li>Splitting operators</li> <li>Combining operators</li> <li>Forking operators</li> <li>Maths operators</li> <li>Other operators</li> </ol>"},{"location":"basic_training/operators/#basic-example","title":"Basic example","text":"<p>Click the  icons in the code for explanations.</p> <pre><code>nums = Channel.of(1, 2, 3, 4) // (1)!\nsquare = nums.map { it -&gt; it * it } // (2)!\nsquare.view() // (3)!\n</code></pre> <ol> <li>Creates a queue channel emitting four values</li> <li>Creates a new channel, transforming each number into its square</li> <li>Prints the channel content</li> </ol> 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 432nums11694square1map <p>Operators can also be chained to implement custom behaviors, so the previous snippet can also be written as:</p> <pre><code>Channel\n.of(1, 2, 3, 4)\n.map { it -&gt; it * it }\n.view()\n</code></pre>"},{"location":"basic_training/operators/#basic-operators","title":"Basic operators","text":"<p>Here we explore some of the most commonly used operators.</p>"},{"location":"basic_training/operators/#view","title":"<code>view()</code>","text":"<p>The <code>view</code> operator prints the items emitted by a channel to the console standard output, appending a new line character to each item. For example:</p> <pre><code>Channel\n.of('foo', 'bar', 'baz')\n.view()\n</code></pre> Output<pre><code>foo\nbar\nbaz\n</code></pre> <p>An optional closure parameter can be specified to customize how items are printed. For example:</p> <pre><code>Channel\n.of('foo', 'bar', 'baz')\n.view { \"- $it\" }\n</code></pre> Output<pre><code>- foo\n- bar\n- baz\n</code></pre>"},{"location":"basic_training/operators/#map","title":"<code>map()</code>","text":"<p>The <code>map</code> operator applies a function of your choosing to every item emitted by a channel and returns the items obtained as a new channel. The function applied is called the mapping function and is expressed with a closure as shown in the example below:</p> <pre><code>Channel\n.of('hello', 'world')\n.map { it -&gt; it.reverse() }\n.view()\n</code></pre> <p>A <code>map</code> can associate a generic tuple to each element and can contain any data.</p> <pre><code>Channel\n.of('hello', 'world')\n.map { word -&gt; [word, word.size()] }\n.view { word, len -&gt; \"$word contains $len letters\" }\n</code></pre> <p>Exercise</p> <p>Use <code>fromPath</code> to create a channel emitting the fastq files matching the pattern <code>data/ggal/*.fq</code>, then use <code>map</code> to return a pair containing the file name and the path itself, and finally, use <code>view</code> to print the resulting channel.</p> Solution <pre><code>Channel\n.fromPath('data/ggal/*.fq')\n.map { file -&gt; [file.name, file] }\n.view { name, file -&gt; \"&gt; $name : $file\" }\n</code></pre>"},{"location":"basic_training/operators/#mix","title":"<code>mix()</code>","text":"<p>The <code>mix</code> operator combines the items emitted by two (or more) channels into a single channel.</p> <pre><code>my_channel_1 = Channel.of(1, 2, 3)\nmy_channel_2 = Channel.of('a', 'b')\nmy_channel_3 = Channel.of('z')\nmy_channel_1\n.mix(my_channel_2, my_channel_3)\n.view()\n</code></pre> Output<pre><code>1\n2\na\n3\nb\nz\n</code></pre> <p>Warning</p> <p>The items in the resulting channel have the same order as in the respective original channels. However, there is no guarantee that the element of the second channel are appended after the elements of the first. Indeed, in the example above, the element <code>a</code> has been printed before <code>3</code>.</p>"},{"location":"basic_training/operators/#flatten","title":"<code>flatten()</code>","text":"<p>The <code>flatten</code> operator transforms a channel in such a way that every tuple is flattened so that each entry is emitted as a sole element by the resulting channel.</p> <pre><code>foo = [1, 2, 3]\nbar = [4, 5, 6]\nChannel\n.of(foo, bar)\n.flatten()\n.view()\n</code></pre> Output<pre><code>1\n2\n3\n4\n5\n6\n</code></pre>"},{"location":"basic_training/operators/#collect","title":"<code>collect()</code>","text":"<p>The <code>collect</code> operator collects all of the items emitted by a channel in a list and returns the object as a sole emission.</p> <pre><code>Channel\n.of(1, 2, 3, 4)\n.collect()\n.view()\n</code></pre> <p>It prints a single value:</p> Output<pre><code>[1, 2, 3, 4]\n</code></pre> <p>Info</p> <p>The result of the <code>collect</code> operator is a value channel.</p>"},{"location":"basic_training/operators/#grouptuple","title":"<code>groupTuple()</code>","text":"<p>The <code>groupTuple</code> operator collects tuples (or lists) of values emitted by the source channel, grouping the elements that share the same key. Finally, it emits a new tuple object for each distinct key collected.</p> <p>Try the following example:</p> <pre><code>Channel\n.of([1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'])\n.groupTuple()\n.view()\n</code></pre> Output<pre><code>[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\n</code></pre> <p>This operator is useful to process a group together with all the elements that share a common property or grouping key.</p> <p>Exercise</p> <p>Use <code>fromPath</code> to create a channel emitting all of the files in the folder <code>data/meta/</code>, then use a <code>map</code> to associate the <code>baseName</code> prefix to each file. Finally, group all files that have the same common prefix.</p> Solution <pre><code>Channel\n.fromPath('data/meta/*')\n.map { file -&gt; tuple(file.baseName, file) }\n.groupTuple()\n.view { baseName, file -&gt; \"&gt; $baseName : $file\" }\n</code></pre>"},{"location":"basic_training/operators/#join","title":"<code>join()</code>","text":"<p>The <code>join</code> operator creates a channel that joins together the items emitted by two channels with a matching key. The key is defined, by default, as the first element in each item emitted.</p> <pre><code>left = Channel.of(['X', 1], ['Y', 2], ['Z', 3], ['P', 7])\nright = Channel.of(['Z', 6], ['Y', 5], ['X', 4])\nleft.join(right).view()\n</code></pre> Output<pre><code>[Z, 3, 6]\n[Y, 2, 5]\n[X, 1, 4]\n</code></pre> <p>Note</p> <p>Notice P is missing in the final result.</p>"},{"location":"basic_training/operators/#branch","title":"<code>branch()</code>","text":"<p>The <code>branch</code> operator allows you to forward the items emitted by a source channel to one or more output channels.</p> <p>The selection criterion is defined by specifying a closure that provides one or more boolean expressions, each of which is identified by a unique label. For the first expression that evaluates to a true value, the item is bound to a named channel as the label identifier. For example:</p> <pre><code>Channel\n.of(1, 2, 3, 40, 50)\n.branch {\nsmall: it &lt; 10\nlarge: it &gt; 10\n}\n.set { result }\nresult.small.view { \"$it is small\" }\nresult.large.view { \"$it is large\" }\n</code></pre> <p>Info</p> <p>The <code>branch</code> operator returns a multi-channel object (i.e., a variable that holds more than one channel object).</p> <p>Note</p> <p>In the above example, what would happen to a value of 10? To deal with this, you can also use <code>&gt;=</code>.</p>"},{"location":"basic_training/operators/#more-resources","title":"More resources","text":"<p>Check the operators documentation on Nextflow web site.</p>"},{"location":"basic_training/operators.pt/","title":"Operadores","text":"<p>Operadores s\u00e3o m\u00e9todos para conectar canais, transformar valores emitidos por canais ou executar regras pr\u00f3prias em canais.</p> <p>Existem sete grupos de operadores descritos em detalhe na Documenta\u00e7\u00e3o do Nextflow, estes s\u00e3o:</p> <ol> <li>Operadores de filtragem</li> <li>Operadores de transforma\u00e7\u00e3o</li> <li>Operadores de divis\u00e3o</li> <li>Operadores de combina\u00e7\u00e3o</li> <li>Operadores de bifurca\u00e7\u00e3o</li> <li>Operadores matem\u00e1ticos</li> <li>Outros operadores</li> </ol>"},{"location":"basic_training/operators.pt/#exemplo-basico","title":"Exemplo b\u00e1sico","text":"<p>Clique no \u00edcone  para ver explica\u00e7\u00f5es do c\u00f3digo.</p> <pre><code>nums = Channel.of(1, 2, 3, 4) // (1)!\nquadrados = nums.map { it -&gt; it * it } // (2)!\nquadrados.view() // (3)!\n</code></pre> <ol> <li>Cria um canal de fila que emite quatro valores</li> <li>Cria um novo canal, transformando cada n\u00famero ao quadrado</li> <li>Imprime o conte\u00fado do canal</li> </ol> 432nums11694quadrados1map <p>Para implementar funcionalidades espec\u00edficas operadores tamb\u00e9m podem ser encadeados. Ent\u00e3o, o c\u00f3digo anterior tamb\u00e9m pode ser escrito assim:</p> <pre><code>Channel\n.of(1, 2, 3, 4)\n.map { it -&gt; it * it }\n.view()\n</code></pre>"},{"location":"basic_training/operators.pt/#operadores-basicos","title":"Operadores b\u00e1sicos","text":"<p>Agora iremos explorar alguns dos operadores mais comuns.</p>"},{"location":"basic_training/operators.pt/#view","title":"<code>view()</code>","text":"<p>O operador <code>view</code> imprime os itens emitidos por um canal para o terminal, acrescentando um caractere de quebra de linha ap\u00f3s cada item. Por exemplo:</p> <pre><code>Channel\n.of('foo', 'bar', 'baz')\n.view()\n</code></pre> Output<pre><code>foo\nbar\nbaz\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode especificar uma clausura para personalizar como os itens s\u00e3o impressos. Por exemplo:</p> <pre><code>Channel\n.of('foo', 'bar', 'baz')\n.view { \"- $it\" }\n</code></pre> Output<pre><code>- foo\n- bar\n- baz\n</code></pre>"},{"location":"basic_training/operators.pt/#map","title":"<code>map()</code>","text":"<p>O operador <code>map</code> aplica uma fun\u00e7\u00e3o de sua escolha em cada item emitido por um canal e retorna os items obtidos como um novo canal. A fun\u00e7\u00e3o aplicada \u00e9 chamada de fun\u00e7\u00e3o de mapeamento e \u00e9 expressa com uma clausura, como demonstrado no exemplo abaixo:</p> <pre><code>Channel\n.of('ol\u00e1', 'mundo')\n.map { it -&gt; it.reverse() }\n.view()\n</code></pre> <p>Um <code>map</code> pode associar uma tupla gen\u00e9rica a cada elemento e pode conter qualquer tipo de dado.</p> <pre><code>Channel\n.of('ol\u00e1', 'mundo')\n.map { palavra -&gt; [palavra, palavra.size()] }\n.view { palavra, comprimento -&gt; \"$palavra cont\u00e9m $comprimento letras\" }\n</code></pre> <p>Exercise</p> <p>Use <code>fromPath</code> para criar um canal emitindo os arquivos fastq que correspondam \u00e0 express\u00e3o <code>data/ggal/*.fq</code>, ent\u00e3o use <code>map</code> para retornar um par contendo o nome e o caminho para o arquivo, e, por fim, use <code>view</code> para imprimir o canal resultante.</p> Solution <pre><code>Channel\n.fromPath('data/ggal/*.fq')\n.map { arquivo -&gt; [arquivo.name, arquivo] }\n.view { nome, arquivo -&gt; \"&gt; $nome : $arquivo\" }\n</code></pre>"},{"location":"basic_training/operators.pt/#mix","title":"<code>mix()</code>","text":"<p>O operador <code>mix</code> combina os itens emitidos por dois (ou mais) canais em um \u00fanico canal.</p> <pre><code>meu_canal_1 = Channel.of(1, 2, 3)\nmeu_canal_2 = Channel.of('a', 'b')\nmeu_canal_3 = Channel.of('z')\nmeu_canal_1\n.mix(meu_canal_2, meu_canal_3)\n.view()\n</code></pre> Output<pre><code>1\n2\na\n3\nb\nz\n</code></pre> <p>Warning</p> <p>Os itens no canal resultante possuem a mesma ordem dos seus respectivos canais originais. No entanto, n\u00e3o h\u00e1 garantia que o elemento do segundo canal \u00e9 acrescentado ao final dos elementos do primeiro canal. Como se pode observar acima, o elemento <code>a</code> foi impresso antes de <code>3</code>.</p>"},{"location":"basic_training/operators.pt/#flatten","title":"<code>flatten()</code>","text":"<p>O operador <code>flatten</code> transforma um canal de maneira que cada tupla \u00e9 achatada, isto \u00e9, cada entrada \u00e9 emitida como um \u00fanico elemento pelo canal resultante.</p> <pre><code>foo = [1, 2, 3]\nbar = [4, 5, 6]\nChannel\n.of(foo, bar)\n.flatten()\n.view()\n</code></pre> Output<pre><code>1\n2\n3\n4\n5\n6\n</code></pre>"},{"location":"basic_training/operators.pt/#collect","title":"<code>collect()</code>","text":"<p>O operador <code>collect</code> coleta todos os itens emitidos por um canal em uma lista e retorna o objeto como uma \u00fanica emiss\u00e3o.</p> <pre><code>Channel\n.of(1, 2, 3, 4)\n.collect()\n.view()\n</code></pre> <p>Isto imprime o valor:</p> Output<pre><code>[1, 2, 3, 4]\n</code></pre> <p>Info</p> <p>O resultado do operador <code>collect</code> \u00e9 um canal de valor.</p>"},{"location":"basic_training/operators.pt/#grouptuple","title":"<code>groupTuple()</code>","text":"<p>O operador <code>groupTuple</code> coleta as tuplas (ou listas) de valores emitidos pelo canal de entrada, agrupando os elementos que possuem a mesma chave. Por fim, ele emite uma nova tupla para cada chave distinta.</p> <p>Por exemplo:</p> <pre><code>Channel\n.of([1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'])\n.groupTuple()\n.view()\n</code></pre> Output<pre><code>[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\n</code></pre> <p>Esse operador \u00e9 \u00fatil para processar um grupo, juntando elementos que possuem uma propriedade ou uma chave em comum.</p> <p>Exercise</p> <p>Use <code>fromPath</code> para criar um canal emitindo todos os arquivos no diret\u00f3rio <code>data/meta/</code>, ent\u00e3o use <code>map</code> para associar o prefixo <code>baseName</code> a cada arquivo. Por fim, agrupe todo os arquivos que possuem o mesmo prefixo.</p> Solution <pre><code>Channel\n.fromPath('data/meta/*')\n.map { arquivo -&gt; tuple(arquivo.baseName, arquivo) }\n.groupTuple()\n.view { baseName, arquivo -&gt; \"&gt; $baseName : $arquivo\" }\n</code></pre>"},{"location":"basic_training/operators.pt/#join","title":"<code>join()</code>","text":"<p>O operador <code>join</code> cria um canal que combina os itens emitidos por dois canais que possuam uma chave em comum. Por padr\u00e3o, a chave \u00e9 definida como o primeiro elemento em cada item emitido.</p> <pre><code>esquerda = Channel.of(['X', 1], ['Y', 2], ['Z', 3], ['P', 7])\ndireita = Channel.of(['Z', 6], ['Y', 5], ['X', 4])\nesquerda.join(direita).view()\n</code></pre> Output<pre><code>[Z, 3, 6]\n[Y, 2, 5]\n[X, 1, 4]\n</code></pre> <p>Note</p> <p>Perceba como P est\u00e1 ausente no resultado final.</p>"},{"location":"basic_training/operators.pt/#branch","title":"<code>branch()</code>","text":"<p>O operador <code>branch</code> permite que voc\u00ea envie os itens emitidos por um canal de entrada para um ou mais canais de sa\u00edda.</p> <p>O crit\u00e9rio de sele\u00e7\u00e3o de cada canal de sa\u00edda \u00e9 definido especificando uma clausura que forne\u00e7a uma ou mais express\u00f5es booleanas, cada uma das quais \u00e9 identificada por um r\u00f3tulo \u00fanico. Para a primeira express\u00e3o verdadeira, o item \u00e9 ligado a um canal nomeado com o r\u00f3tulo. Por exemplo:</p> <pre><code>Channel\n.of(1, 2, 3, 40, 50)\n.branch {\npequeno: it &lt; 10\ngrande: it &gt; 10\n}\n.set { resultado }\nresultado.pequeno.view { \"$it \u00e9 pequeno\" }\nresultado.grande.view { \"$it \u00e9 grande\" }\n</code></pre> <p>Info</p> <p>O operador <code>branch</code> retorna um objeto multi-canal (isto \u00e9, uma vari\u00e1vel que possui mais de um canal).</p> <p>Note</p> <p>No exemplo acima, o que aconteceria com um valor igual a 10? Para lidar com isso, voc\u00ea pode usar <code>&gt;=</code>.</p>"},{"location":"basic_training/operators.pt/#outros-recursos","title":"Outros recursos","text":"<p>Veja a documenta\u00e7\u00e3o de operadores no site oficial do Nextflow.</p>"},{"location":"basic_training/processes/","title":"Processes","text":"<p>In Nextflow, a <code>process</code> is the basic computing primitive to execute foreign functions (i.e., custom scripts or tools).</p> <p>The <code>process</code> definition starts with the keyword <code>process</code>, followed by the process name and finally the process body delimited by curly brackets.</p> <p>The <code>process</code> name is commonly written in upper case by convention.</p> <p>A basic <code>process</code>, only using the <code>script</code> definition block, looks like the following:</p> <pre><code>process SAYHELLO {\nscript:\n\"\"\"\n    echo 'Hello world!'\n    \"\"\"\n}\n</code></pre> <p>In more complex examples, the process body can contain up to five definition blocks:</p> <ol> <li>Directives are initial declarations that define optional settings</li> <li>Input defines the expected input channel(s)</li> <li>Output defines the expected output channel(s)</li> <li>When is an optional clause statement to allow conditional processes</li> <li>Script is a string statement that defines the command to be executed by the process' task</li> </ol> <p>The full process syntax is defined as follows:</p> <p>Click the  icons in the code for explanations.</p> <pre><code>process &lt; name &gt; {\n[ directives ] // (1)!\ninput: // (2)!\n&lt; process inputs &gt;\noutput: // (3)!\n&lt; process outputs &gt;\nwhen: // (4)!\n&lt; condition &gt;\n[script|shell|exec]: // (5)!\n\"\"\"\n    &lt; user script to be executed &gt;\n    \"\"\"\n}\n</code></pre> <ol> <li>Zero, one or more process directives</li> <li>Zero, one or more process inputs</li> <li>Zero, one or more process outputs</li> <li>An optional boolean conditional to trigger the process execution</li> <li>The command to be executed</li> </ol>"},{"location":"basic_training/processes/#script","title":"Script","text":"<p>The <code>script</code> block is a string statement that defines the command to be executed by the process.</p> <p>A process can execute only one <code>script</code> block. It must be the last statement when the process contains input and output declarations.</p> <p>The <code>script</code> block can be a single or a multi-line string. The latter simplifies the writing of non-trivial scripts composed of multiple commands spanning over multiple lines. For example:</p> <pre><code>process EXAMPLE {\nscript:\n\"\"\"\n    echo 'Hello world!\\nHola mundo!\\nCiao mondo!\\nHallo Welt!' &gt; file\n    cat file | head -n 1 | head -c 5 &gt; chunk_1.txt\n    gzip -c chunk_1.txt  &gt; chunk_archive.gz\n    \"\"\"\n}\nworkflow {\nEXAMPLE()\n}\n</code></pre> <p>By default, the <code>process</code> command is interpreted as a Bash script. However, any other scripting language can be used by simply starting the script with the corresponding Shebang declaration. For example:</p> <pre><code>process PYSTUFF {\nscript:\n\"\"\"\n    #!/usr/bin/env python\n    x = 'Hello'\n    y = 'world!'\n    print (\"%s - %s\" % (x, y))\n    \"\"\"\n}\nworkflow {\nPYSTUFF()\n}\n</code></pre> <p>Tip</p> <p>Multiple programming languages can be used within the same workflow script. However, for large chunks of code it is better to save them into separate files and invoke them from the process script. One can store the specific scripts in the <code>./bin/</code> folder.</p>"},{"location":"basic_training/processes/#script-parameters","title":"Script parameters","text":"<p>Script parameters (<code>params</code>) can be defined dynamically using variable values. For example:</p> <pre><code>params.data = 'World'\nprocess FOO {\nscript:\n\"\"\"\n    echo Hello $params.data\n    \"\"\"\n}\nworkflow {\nFOO()\n}\n</code></pre> <p>Info</p> <p>A process script can contain any string format supported by the Groovy programming language. This allows us to use string interpolation as in the script above or multiline strings. Refer to String interpolation for more information.</p> <p>Warning</p> <p>Since Nextflow uses the same Bash syntax for variable substitutions in strings, Bash environment variables need to be escaped using the <code>\\</code> character. The escaped version will be resolved later, returning the task directory (e.g. work/7f/f285b80022d9f61e82cd7f90436aa4/), while <code>$PWD</code> would show the directory where you're running Nextflow.</p> <pre><code>process FOO {\nscript:\n\"\"\"\n    echo \"The current directory is \\$PWD\"\n    \"\"\"\n}\nworkflow {\nFOO()\n}\n</code></pre> <p>It can be tricky to write a script that uses many Bash variables. One possible alternative is to use a script string delimited by single-quote characters</p> <pre><code>process BAR {\nscript:\n'''\n    echo \"The current directory is $PWD\"\n    '''\n}\nworkflow {\nBAR()\n}\n</code></pre> <p>However, this blocks the usage of Nextflow variables in the command script.</p> <p>Another alternative is to use a <code>shell</code> statement instead of <code>script</code> and use a different syntax for Nextflow variables, e.g., <code>!{..}</code>. This allows the use of both Nextflow and Bash variables in the same script.</p> <pre><code>params.data = 'le monde'\nprocess BAZ {\nshell:\n'''\n    X='Bonjour'\n    echo $X !{params.data}\n    '''\n}\nworkflow {\nBAZ()\n}\n</code></pre>"},{"location":"basic_training/processes/#conditional-script","title":"Conditional script","text":"<p>The process script can also be defined in a completely dynamic manner using an <code>if</code> statement or any other expression for evaluating a string value. For example:</p> <pre><code>params.compress = 'gzip'\nparams.file2compress = \"$baseDir/data/ggal/transcriptome.fa\"\nprocess FOO {\ninput:\npath file\nscript:\nif (params.compress == 'gzip')\n\"\"\"\n        gzip -c $file &gt; ${file}.gz\n        \"\"\"\nelse if (params.compress == 'bzip2')\n\"\"\"\n        bzip2 -c $file &gt; ${file}.bz2\n        \"\"\"\nelse\nthrow new IllegalArgumentException(\"Unknown compressor $params.compress\")\n}\nworkflow {\nFOO(params.file2compress)\n}\n</code></pre>"},{"location":"basic_training/processes/#inputs","title":"Inputs","text":"<p>Nextflow process instances (tasks) are isolated from each other but can communicate between themselves by sending values through channels.</p> <p>Inputs implicitly determine the dependencies and the parallel execution of the process. The process execution is fired each time new data is ready to be consumed from the input channel:</p> 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 data zdata ydata xChannelProcessdata xoutput xdata youtput ydata zoutput ztask 1task 2task 3 <p>The <code>input</code> block defines the names and qualifiers of variables that refer to channel elements directed at the process. You can only define one <code>input</code> block at a time, and it must contain one or more input declarations.</p> <p>The <code>input</code> block follows the syntax shown below:</p> <pre><code>input:\n&lt;input qualifier&gt; &lt;input name&gt;\n</code></pre>"},{"location":"basic_training/processes/#input-values","title":"Input values","text":"<p>The <code>val</code> qualifier allows you to receive data of any type as input. It can be accessed in the process script by using the specified input name, as shown in the following example:</p> <pre><code>num = Channel.of(1, 2, 3)\nprocess BASICEXAMPLE {\ndebug true\ninput:\nval x\nscript:\n\"\"\"\n    echo process job $x\n    \"\"\"\n}\nworkflow {\nmyrun = BASICEXAMPLE(num)\n}\n</code></pre> <p>In the above example the process is executed three times, each time a value is received from the channel <code>num</code> and used to process the script. Thus, it results in an output similar to the one shown below:</p> <pre><code>process job 3\nprocess job 1\nprocess job 2\n</code></pre> <p>Warning</p> <p>The channel guarantees that items are delivered in the same order as they have been sent - but - since the process is executed in a parallel manner, there is no guarantee that they are processed in the same order as they are received.</p>"},{"location":"basic_training/processes/#input-files","title":"Input files","text":"<p>The <code>path</code> qualifier allows the handling of file values in the process execution context. This means that Nextflow will stage it in the process execution directory, and it can be accessed in the script by using the name specified in the input declaration.</p> <pre><code>reads = Channel.fromPath('data/ggal/*.fq')\nprocess FOO {\ndebug true\ninput:\npath 'sample.fastq'\nscript:\n\"\"\"\n    ls sample.fastq\n    \"\"\"\n}\nworkflow {\nresult = FOO(reads)\n}\n</code></pre> <p>The input file name can also be defined using a variable reference as shown below:</p> <pre><code>reads = Channel.fromPath('data/ggal/*.fq')\nprocess FOO {\ndebug true\ninput:\npath sample\nscript:\n\"\"\"\n    ls  $sample\n    \"\"\"\n}\nworkflow {\nresult = FOO(reads)\n}\n</code></pre> <p>The same syntax is also able to handle more than one input file in the same execution and only requires changing the channel composition.</p> <pre><code>reads = Channel.fromPath('data/ggal/*.fq')\nprocess FOO {\ndebug true\ninput:\npath sample\nscript:\n\"\"\"\n    ls -lh $sample\n    \"\"\"\n}\nworkflow {\nFOO(reads.collect())\n}\n</code></pre> <p>Warning</p> <p>In the past, the <code>file</code> qualifier was used for files, but the <code>path</code> qualifier should be preferred over file to handle process input files when using Nextflow 19.10.0 or later. When a process declares an input file, the corresponding channel elements must be file objects created with the file helper function from the file specific channel factories (e.g., <code>Channel.fromPath</code> or <code>Channel.fromFilePairs</code>).</p> <p>Exercise</p> <p>Write a script that creates a channel containing all read files matching the pattern <code>data/ggal/*_1.fq</code> followed by a process that concatenates them into a single file and prints the first 10 lines.</p> Solution <pre><code>params.reads = \"$baseDir/data/ggal/*_1.fq\"\nChannel\n.fromPath(params.reads)\n.set { read_ch }\nprocess CONCATENATE {\ntag \"Concat all files\"\ninput:\npath '*'\noutput:\npath 'top_10_lines'\nscript:\n\"\"\"\n    cat * &gt; concatenated.txt\n    head -n 10 concatenated.txt &gt; top_10_lines\n    \"\"\"\n}\nworkflow {\nconcat_ch = CONCATENATE(read_ch.collect())\nconcat_ch.view()\n}\n</code></pre>"},{"location":"basic_training/processes/#combine-input-channels","title":"Combine input channels","text":"<p>A key feature of processes is the ability to handle inputs from multiple channels. However, it\u2019s important to understand how channel contents and their semantics affect the execution of a process.</p> <p>Consider the following example:</p> <pre><code>ch1 = Channel.of(1, 2, 3)\nch2 = Channel.of('a', 'b', 'c')\nprocess FOO {\ndebug true\ninput:\nval x\nval y\nscript:\n\"\"\"\n    echo $x and $y\n    \"\"\"\n}\nworkflow {\nFOO(ch1, ch2)\n}\n</code></pre> <p>Both channels emit three values, therefore the process is executed three times, each time with a different pair:</p> <ul> <li><code>(1, a)</code></li> <li><code>(2, b)</code></li> <li><code>(3, c)</code></li> </ul> <p>What is happening is that the process waits until there\u2019s a complete input configuration, i.e., it receives an input value from all the channels declared as input.</p> <p>When this condition is verified, it consumes the input values coming from the respective channels, spawns a task execution, then repeats the same logic until one or more channels have no more content.</p> <p>This means channel values are consumed serially one after another and the first empty channel causes the process execution to stop, even if there are other values in other channels.</p> <p>So what happens when channels do not have the same cardinality (i.e., they emit a different number of elements)?</p> <p>For example:</p> <pre><code>input1 = Channel.of(1, 2)\ninput2 = Channel.of('a', 'b', 'c', 'd')\nprocess FOO {\ndebug true\ninput:\nval x\nval y\nscript:\n\"\"\"\n    echo $x and $y\n    \"\"\"\n}\nworkflow {\nFOO(input1, input2)\n}\n</code></pre> <p>In the above example, the process is only executed twice because the process stops when a channel has no more data to be processed.</p> <p>However, what happens if you replace value <code>x</code> with a <code>value</code> channel?</p> <p>Compare the previous example with the following one:</p> <pre><code>input1 = Channel.value(1)\ninput2 = Channel.of('a', 'b', 'c')\nprocess BAR {\ndebug true\ninput:\nval x\nval y\nscript:\n\"\"\"\n    echo $x and $y\n    \"\"\"\n}\nworkflow {\nBAR(input1, input2)\n}\n</code></pre> Script output<pre><code>1 and b\n1 and a\n1 and c\n</code></pre> <p>This is because value channels can be consumed multiple times and do not affect process termination.</p> <p>Exercise</p> <p>Write a process that is executed for each read file matching the pattern <code>data/ggal/*_1.fq</code> and use the same <code>data/ggal/transcriptome.fa</code> in each execution.</p> Solution <pre><code>params.reads = \"$baseDir/data/ggal/*_1.fq\"\nparams.transcriptome_file = \"$baseDir/data/ggal/transcriptome.fa\"\nChannel\n.fromPath(params.reads)\n.set { read_ch }\nprocess COMMAND {\ntag \"Run_command\"\ninput:\npath reads\npath transcriptome\noutput:\npath result\nscript:\n\"\"\"\n    echo your_command $reads $transcriptome &gt; result\n    \"\"\"\n}\nworkflow {\nconcat_ch = COMMAND(read_ch, params.transcriptome_file)\nconcat_ch.view()\n}\n</code></pre>"},{"location":"basic_training/processes/#input-repeaters","title":"Input repeaters","text":"<p>The <code>each</code> qualifier allows you to repeat the execution of a process for each item in a collection every time new data is received. For example:</p> <pre><code>sequences = Channel.fromPath('data/prots/*.tfa')\nmethods = ['regular', 'espresso', 'psicoffee']\nprocess ALIGNSEQUENCES {\ndebug true\ninput:\npath seq\neach mode\nscript:\n\"\"\"\n    echo t_coffee -in $seq -mode $mode\n    \"\"\"\n}\nworkflow {\nALIGNSEQUENCES(sequences, methods)\n}\n</code></pre> <p>In the above example, every time a file of sequences is received as an input by the process, it executes three tasks, each running a different alignment method set as a <code>mode</code> variable. This is useful when you need to repeat the same task for a given set of parameters.</p> <p>Exercise</p> <p>Extend the previous example so a task is executed for each read file matching the pattern <code>data/ggal/*_1.fq</code> and repeat the same task with both <code>salmon</code> and <code>kallisto</code>.</p> Solution <pre><code>params.reads = \"$baseDir/data/ggal/*_1.fq\"\nparams.transcriptome_file = \"$baseDir/data/ggal/transcriptome.fa\"\nmethods= ['salmon', 'kallisto']\nChannel\n.fromPath(params.reads)\n.set { read_ch }\nprocess COMMAND {\ntag \"Run_command\"\ninput:\npath reads\npath transcriptome\neach mode\noutput:\npath result\nscript:\n\"\"\"\n    echo $mode $reads $transcriptome &gt; result\n    \"\"\"\n}\nworkflow {\nconcat_ch = COMMAND(read_ch, params.transcriptome_file, methods)\nconcat_ch.view { \"To run : ${it.text}\" }\n}\n</code></pre>"},{"location":"basic_training/processes/#outputs","title":"Outputs","text":"<p>The output declaration block defines the channels used by the process to send out the results produced.</p> <p>Only one output block, that can contain one or more output declaration, can be defined. The output block follows the syntax shown below:</p> <pre><code>output:\n&lt;output qualifier&gt; &lt;output name&gt;, emit: &lt;output channel&gt;\n</code></pre>"},{"location":"basic_training/processes/#output-values","title":"Output values","text":"<p>The <code>val</code> qualifier specifies a defined value in the script context. Values are frequently defined in the input and/or output declaration blocks, as shown in the following example:</p> <pre><code>methods = ['prot', 'dna', 'rna']\nprocess FOO {\ninput:\nval x\noutput:\nval x\nscript:\n\"\"\"\n    echo $x &gt; file\n    \"\"\"\n}\nworkflow {\nreceiver_ch = FOO(Channel.of(methods))\nreceiver_ch.view { \"Received: $it\" }\n}\n</code></pre>"},{"location":"basic_training/processes/#output-files","title":"Output files","text":"<p>The <code>path</code> qualifier specifies one or more files produced by the process into the specified channel as an output.</p> <pre><code>process RANDOMNUM {\noutput:\npath 'result.txt'\nscript:\n\"\"\"\n    echo \\$RANDOM &gt; result.txt\n    \"\"\"\n}\nworkflow {\nreceiver_ch = RANDOMNUM()\nreceiver_ch.view { \"Received: \" + it.text }\n}\n</code></pre> <p>In the above example the process <code>RANDOMNUM</code> creates a file named <code>result.txt</code> containing a random number.</p> <p>Since a file parameter using the same name is declared in the output block, the file is sent over the <code>receiver_ch</code> channel when the task is complete. A downstream <code>process</code> declaring the same channel as input will be able to receive it.</p>"},{"location":"basic_training/processes/#multiple-output-files","title":"Multiple output files","text":"<p>When an output file name contains a wildcard character (<code>*</code> or <code>?</code>) it is interpreted as a glob path matcher. This allows us to capture multiple files into a list object and output them as a sole emission. For example:</p> <pre><code>process SPLITLETTERS {\noutput:\npath 'chunk_*'\nscript:\n\"\"\"\n    printf 'Hola' | split -b 1 - chunk_\n    \"\"\"\n}\nworkflow {\nletters = SPLITLETTERS()\nletters\n.flatMap()\n.view { \"File: ${it.name} =&gt; ${it.text}\" }\n}\n</code></pre> <p>Prints the following:</p> <pre><code>File: chunk_aa =&gt; H\nFile: chunk_ab =&gt; o\nFile: chunk_ac =&gt; l\nFile: chunk_ad =&gt; a\n</code></pre> <p>Some caveats on glob pattern behavior:</p> <ul> <li>Input files are not included in the list of possible matches</li> <li>Glob pattern matches both files and directory paths</li> <li>When a two stars pattern <code>**</code> is used to recourse across directories, only file paths are matched i.e., directories are not included in the result list.</li> </ul> <p>Exercise</p> <p>Remove the <code>flatMap</code> operator and see out the output change. The documentation for the <code>flatMap</code> operator is available at this link.</p> Solution <pre><code>File: [chunk_aa, chunk_ab, chunk_ac, chunk_ad] =&gt; [H, o, l, a]\n</code></pre>"},{"location":"basic_training/processes/#dynamic-output-file-names","title":"Dynamic output file names","text":"<p>When an output file name needs to be expressed dynamically, it is possible to define it using a dynamic string that references values defined in the input declaration block or in the script global context. For example:</p> <pre><code>species = ['cat', 'dog', 'sloth']\nsequences = ['AGATAG', 'ATGCTCT', 'ATCCCAA']\nChannel\n.fromList(species)\n.set { species_ch }\nprocess ALIGN {\ninput:\nval x\nval seq\noutput:\npath \"${x}.aln\"\nscript:\n\"\"\"\n    echo align -in $seq &gt; ${x}.aln\n    \"\"\"\n}\nworkflow {\ngenomes = ALIGN(species_ch, sequences)\ngenomes.view()\n}\n</code></pre> <p>In the above example, each time the process is executed an alignment file is produced whose name depends on the actual value of the <code>x</code> input.</p>"},{"location":"basic_training/processes/#composite-inputs-and-outputs","title":"Composite inputs and outputs","text":"<p>So far we have seen how to declare multiple input and output channels that can handle one value at a time. However, Nextflow can also handle a tuple of values.</p> <p>The input and output declarations for tuples must be declared with a <code>tuple</code> qualifier followed by the definition of each element in the tuple.</p> <pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\nprocess FOO {\ninput:\ntuple val(sample_id), path(sample_id_paths)\noutput:\ntuple val(sample_id), path('sample.bam')\nscript:\n\"\"\"\n    echo your_command_here --reads $sample_id_paths &gt; sample.bam\n    \"\"\"\n}\nworkflow {\nbam_ch = FOO(reads_ch)\nbam_ch.view()\n}\n</code></pre> <p>Info</p> <p>In previous versions of Nextflow <code>tuple</code> was called <code>set</code> but it was used the same way with the same semantic.</p> <p>Exercise</p> <p>Modify the script of the previous exercise so that the bam file is named as the given <code>sample_id</code>.</p> Solution <pre><code>reads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\nprocess FOO {\ninput:\ntuple val(sample_id), path(sample_id_paths)\noutput:\ntuple val(sample_id), path(\"${sample_id}.bam\")\nscript:\n\"\"\"\n    echo your_command_here --reads $sample_id_paths &gt; ${sample_id}.bam\n    \"\"\"\n}\nworkflow {\nbam_ch = FOO(reads_ch)\nbam_ch.view()\n}\n</code></pre>"},{"location":"basic_training/processes/#when","title":"When","text":"<p>The <code>when</code> declaration allows you to define a condition that must be verified in order to execute the process. This can be any expression that evaluates a boolean value.</p> <p>It is useful to enable/disable the process execution depending on the state of various inputs and parameters. For example:</p> <pre><code>params.dbtype = 'nr'\nparams.prot = 'data/prots/*.tfa'\nproteins = Channel.fromPath(params.prot)\nprocess FIND {\ndebug true\ninput:\npath fasta\nval type\nwhen:\nfasta.name =~ /^BB11.*/ &amp;&amp; type == 'nr'\nscript:\n\"\"\"\n    echo blastp -query $fasta -db nr\n    \"\"\"\n}\nworkflow {\nresult = FIND(proteins, params.dbtype)\n}\n</code></pre>"},{"location":"basic_training/processes/#directives","title":"Directives","text":"<p>Directive declarations allow the definition of optional settings that affect the execution of the current process without affecting the semantic of the task itself.</p> <p>They must be entered at the top of the process body, before any other declaration blocks (i.e., <code>input</code>, <code>output</code>, etc.).</p> <p>Directives are commonly used to define the amount of computing resources to be used or other meta directives that allow the definition of extra configuration of logging information. For example:</p> <pre><code>process FOO {\ncpus 2\nmemory 1.GB\ncontainer 'image/name'\nscript:\n\"\"\"\n    echo your_command --this --that\n    \"\"\"\n}\n</code></pre> <p> The complete list of directives is available at this link.</p> Name Description <code>cpus</code> Allows you to define the number of (logical) CPUs required by the process\u2019 task. <code>time</code> Allows you to define how long the task is allowed to run (e.g., time 1h: 1 hour, 1s 1 second, 1m 1 minute, 1d 1 day). <code>memory</code> Allows you to define how much memory the task is allowed to use (e.g., 2 GB is 2 GB). Can also use B, KB,MB,GB and TB. <code>disk</code> Allows you to define how much local disk storage the task is allowed to use. <code>tag</code> Allows you to associate each process execution with a custom label to make it easier to identify them in the log file or the trace execution report."},{"location":"basic_training/processes/#organize-outputs","title":"Organize outputs","text":""},{"location":"basic_training/processes/#publishdir-directive","title":"PublishDir directive","text":"<p>Given each task is being executed in separate temporary <code>work/</code> folder (e.g., <code>work/f1/850698\u2026</code>; <code>work/g3/239712\u2026</code>; etc.), we may want to save important, non-intermediary, and/or final files in a results folder.</p> <p>Tip</p> <p>Remember to delete the work folder from time to time to clear your intermediate files and stop them from filling your computer!</p> <p>To store our workflow result files, we need to explicitly mark them using the directive publishDir in the process that\u2019s creating the files. For example:</p> <pre><code>params.outdir = 'my-results'\nparams.prot = 'data/prots/*.tfa'\nproteins = Channel.fromPath(params.prot)\nprocess BLASTSEQ {\npublishDir \"$params.outdir/bam_files\", mode: 'copy'\ninput:\npath fasta\noutput:\npath ('*.txt')\nscript:\n\"\"\"\n    echo blastp $fasta &gt; ${fasta}_result.txt\n    \"\"\"\n}\nworkflow {\nblast_ch = BLASTSEQ(proteins)\nblast_ch.view()\n}\n</code></pre> <p>The above example will copy all blast script files created by the <code>BLASTSEQ</code> process into the directory path <code>my-results</code>.</p> <p>Tip</p> <p>The publish directory can be local or remote. For example, output files could be stored using an AWS S3 bucket by using the <code>s3://</code> prefix in the target path.</p>"},{"location":"basic_training/processes/#manage-semantic-sub-directories","title":"Manage semantic sub-directories","text":"<p>You can use more than one <code>publishDir</code> to keep different outputs in separate directories. For example:</p> <pre><code>params.reads = 'data/reads/*_{1,2}.fq.gz'\nparams.outdir = 'my-results'\nsamples_ch = Channel.fromFilePairs(params.reads, flat: true)\nprocess FOO {\npublishDir \"$params.outdir/$sampleId/\", pattern: '*.fq'\npublishDir \"$params.outdir/$sampleId/counts\", pattern: \"*_counts.txt\"\npublishDir \"$params.outdir/$sampleId/outlooks\", pattern: '*_outlook.txt'\ninput:\ntuple val(sampleId), path('sample1.fq.gz'), path('sample2.fq.gz')\noutput:\npath \"*\"\nscript:\n\"\"\"\n    zcat sample1.fq.gz &gt; sample1.fq\n    zcat sample2.fq.gz &gt; sample2.fq\n    awk '{s++}END{print s/4}' sample1.fq &gt; sample1_counts.txt\n    awk '{s++}END{print s/4}' sample2.fq &gt; sample2_counts.txt\n    head -n 50 sample1.fq &gt; sample1_outlook.txt\n    head -n 50 sample2.fq &gt; sample2_outlook.txt\n    \"\"\"\n}\nworkflow {\nout_channel = FOO(samples_ch)\n}\n</code></pre> <p>The above example will create an output structure in the directory <code>my-results</code>, that contains a separate sub-directory for each given sample ID, each containing the folders <code>counts</code> and <code>outlooks</code>.</p>"},{"location":"basic_training/processes.pt/","title":"Processos","text":"<p>No Nextflow, um processo (<code>process</code>) \u00e9 a primitiva de computa\u00e7\u00e3o b\u00e1sica para executar fun\u00e7\u00f5es estrangeiras (ou seja, scripts personalizados ou ferramentas).</p> <p>A defini\u00e7\u00e3o do processo come\u00e7a com a palavra-chave <code>process</code>, seguida pelo nome do processo e, finalmente, o corpo do processo delimitado por chaves.</p> <p>O nome do processo (<code>process</code>) \u00e9 comumente escrito em letras mai\u00fasculas por conven\u00e7\u00e3o.</p> <p>Um processo b\u00e1sico, usando apenas o bloco de defini\u00e7\u00e3o <code>script</code>, se parece com o seguinte:</p> <pre><code>process DIGAOLA {\nscript:\n\"\"\"\n    echo 'Ol\u00e1 mundo!'\n    \"\"\"\n}\n</code></pre> <p>Em exemplos mais complexos, o corpo do processo pode conter at\u00e9 cinco blocos de defini\u00e7\u00e3o:</p> <ol> <li>Diretivas s\u00e3o declara\u00e7\u00f5es iniciais que definem configura\u00e7\u00f5es opcionais</li> <li>Input (Bloco de entrada) define o(s) canal(is) de entrada esperado(s)</li> <li>Output (Bloco de sa\u00edda) define o(s) canal(is) de sa\u00edda esperado(s)</li> <li>When \u00e9 uma declara\u00e7\u00e3o de cl\u00e1usula opcional para permitir processos condicionais</li> <li>Script \u00e9 uma string que define o comando a ser executado pela tarefa do processo</li> </ol> <p>A sintaxe completa do processo \u00e9 definida da seguinte forma:</p> <p>Clique no \u00edcone  no c\u00f3digo para ver explica\u00e7\u00f5es.</p> <pre><code>process &lt; nome &gt; {\n[ diretivas ] // (1)!\ninput: // (2)!\n&lt; entradas do processo &gt;\noutput: // (3)!\n&lt; sa\u00eddas do processo &gt;\nwhen: // (4)!\n&lt; condi\u00e7\u00e3o &gt;\n[script|shell|exec]: // (5)!\n\"\"\"\n    &lt; script do usu\u00e1rio a ser executado &gt;\n    \"\"\"\n}\n</code></pre> <ol> <li>Zero, uma ou mais diretivas de processo</li> <li>Zero, uma ou mais entradas para o processo</li> <li>Zero, uma ou mais sa\u00eddas para o processo</li> <li>Uma condicional booleana opcional para acionar a execu\u00e7\u00e3o do processo</li> <li>O comando a ser executado</li> </ol>"},{"location":"basic_training/processes.pt/#script","title":"Script","text":"<p>O bloco <code>script</code> \u00e9 uma string que define o comando a ser executado pelo processo.</p> <p>Um processo pode executar apenas um bloco <code>script</code>. Deve ser a \u00faltima instru\u00e7\u00e3o quando o processo cont\u00e9m declara\u00e7\u00f5es de entrada e sa\u00edda.</p> <p>O bloco <code>script</code> pode ser uma string de uma ou v\u00e1rias linhas. A de v\u00e1rias linhas simplifica a escrita de scripts n\u00e3o triviais compostos por v\u00e1rios comandos abrangendo v\u00e1rias linhas. Por exemplo:</p> <pre><code>process EXEMPLO {\nscript:\n\"\"\"\n    echo 'Ol\u00e1 mundo!\\nHola mundo!\\nCiao mondo!\\nHallo Welt!' &gt; arquivo\n    cat arquivo | head -n 1 | head -c 5 &gt; pedaco_1.txt\n    gzip -c pedaco_1.txt &gt; pedacos.gz\n    \"\"\"\n}\nworkflow {\nEXEMPLO()\n}\n</code></pre> <p>Por padr\u00e3o, o comando do processo \u00e9 interpretado como um script Bash. No entanto, qualquer outra linguagem de script pode ser usada simplesmente iniciando o script com a declara\u00e7\u00e3o Shebang adequada. Por exemplo:</p> <pre><code>process CODIGOPYTHON {\nscript:\n\"\"\"\n    #!/usr/bin/env python\n    x = 'Ol\u00e1'\n    y = 'mundo!'\n    print (\"%s - %s\" % (x, y))\n    \"\"\"\n}\nworkflow {\nCODIGOPYTHON()\n}\n</code></pre> <p>Tip</p> <p>V\u00e1rias linguagens de programa\u00e7\u00e3o podem ser usadas no mesmo script de fluxo de trabalho. No entanto, para grandes blocos de c\u00f3digo, \u00e9 melhor salv\u00e1-los em arquivos separados e invoc\u00e1-los a partir do script do processo. Pode-se armazenar os scripts espec\u00edficos na pasta <code>./bin/</code>.</p>"},{"location":"basic_training/processes.pt/#parametros-do-script","title":"Par\u00e2metros do script","text":"<p>Par\u00e2metros de script (<code>params</code>) podem ser definidos dinamicamente usando valores vari\u00e1veis. Por exemplo:</p> <pre><code>params.dado = 'Mundo'\nprocess FOO {\nscript:\n\"\"\"\n    echo Ol\u00e1 $params.dado\n    \"\"\"\n}\nworkflow {\nFOO()\n}\n</code></pre> <p>Info</p> <p>Um script de processo pode conter qualquer formato de string suportado pela linguagem de programa\u00e7\u00e3o Groovy. Isso nos permite usar a interpola\u00e7\u00e3o de strings como no script acima ou strings multilinha. Consulte Interpola\u00e7\u00e3o de string para obter mais informa\u00e7\u00f5es.</p> <p>Warning</p> <p>Como o Nextflow usa a mesma sintaxe Bash para substitui\u00e7\u00f5es de vari\u00e1veis em strings, as vari\u00e1veis de ambiente Bash precisam ser escapadas usando o caractere <code>\\</code>. A vers\u00e3o escapada ser\u00e1 resolvida posteriormente, retornando o diret\u00f3rio da tarefa (por exemplo, work/7f/f285b80022d9f61e82cd7f90436aa4/), enquanto <code>$PWD</code> mostraria o diret\u00f3rio onde voc\u00ea est\u00e1 executando o Nextflow.</p> <pre><code>process FOO {\nscript:\n\"\"\"\n    echo \"O diret\u00f3rio atual \u00e9 \\$PWD\"\n    \"\"\"\n}\nworkflow {\nFOO()\n}\n</code></pre> <p>Pode ser complicado escrever um script que usa muitas vari\u00e1veis Bash. Uma alternativa poss\u00edvel \u00e9 usar uma string de script delimitada por aspas simples</p> <pre><code>process BAR {\nscript:\n'''\n    echo \"The current directory is $PWD\"\n    '''\n}\nworkflow {\nBAR()\n}\n</code></pre> <p>No entanto, isso bloqueia o uso de vari\u00e1veis Nextflow no script de comando.</p> <p>Outra alternativa \u00e9 usar uma instru\u00e7\u00e3o <code>shell</code> em vez de <code>script</code> e usar uma sintaxe diferente para vari\u00e1veis do Nextflow, por exemplo, <code>!{..}</code>. Isso permite o uso das vari\u00e1veis Nextflow e Bash no mesmo script.</p> <pre><code>params.dado = 'le monde'\nprocess BAZ {\nshell:\n'''\n    X='Bonjour'\n    echo $X !{params.dado}\n    '''\n}\nworkflow {\nBAZ()\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#scripts-condicionais","title":"Scripts condicionais","text":"<p>O script do processo tamb\u00e9m pode ser definido de maneira completamente din\u00e2mica usando uma instru\u00e7\u00e3o <code>if</code> ou qualquer outra express\u00e3o para avaliar um valor de string. Por exemplo:</p> <pre><code>params.compressor = 'gzip'\nparams.arquivo_a_comprimir = \"$baseDir/data/ggal/transcriptome.fa\"\nprocess FOO {\ninput:\npath arquivo\nscript:\nif (params.compressor == 'gzip')\n\"\"\"\n        gzip -c $arquivo &gt; ${arquivo}.gz\n        \"\"\"\nelse if (params.compressor == 'bzip2')\n\"\"\"\n        bzip2 -c $arquivo &gt; ${arquivo}.bz2\n        \"\"\"\nelse\nthrow new IllegalArgumentException(\"Compressor $params.compressor desconhecido\")\n}\nworkflow {\nFOO(params.arquivo_a_comprimir)\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#canais-de-entradas","title":"Canais de entradas","text":"<p>As inst\u00e2ncias dos processos (tarefas) Nextflow s\u00e3o isoladas umas das outras, mas podem se comunicar entre si enviando valores por meio de canais.</p> <p>As entradas determinam implicitamente as depend\u00eancias e a execu\u00e7\u00e3o paralela do processo. A execu\u00e7\u00e3o do processo \u00e9 disparada cada vez que dados novos est\u00e3o prontos para serem consumidos do canal de entrada:</p> dado zdado ydado xCanalProcessodado xsa\u00edda xdado ysa\u00edda ydado zsa\u00edda ztarefa 1tarefa 2tarefa 3 <p>O bloco <code>input</code> define os nomes e qualificadores das vari\u00e1veis que se referem aos elementos do canal direcionados ao processo. Voc\u00ea s\u00f3 pode definir um bloco <code>input</code> por vez e deve conter uma ou mais declara\u00e7\u00f5es de entrada.</p> <p>O bloco <code>input</code> segue a sintaxe mostrada abaixo:</p> <pre><code>input:\n&lt;qualificador da vari\u00e1vel de entrada&gt; &lt;nome da vari\u00e1vel de entrada&gt;\n</code></pre>"},{"location":"basic_training/processes.pt/#valores-de-entrada","title":"Valores de entrada","text":"<p>O qualificador <code>val</code> permite receber dados de qualquer tipo como entrada. Ele pode ser acessado no script do processo usando o nome de entrada especificado, conforme mostrado no exemplo a seguir:</p> <pre><code>num = Channel.of(1, 2, 3)\nprocess EXEMPLOBASICO {\ndebug true\ninput:\nval x\nscript:\n\"\"\"\n    echo tarefa $x do processo\n    \"\"\"\n}\nworkflow {\nminha_execucacao = EXEMPLOBASICO(num)\n}\n</code></pre> <p>No exemplo acima, o processo \u00e9 executado tr\u00eas vezes, cada vez que um valor \u00e9 recebido do canal <code>num</code> e usado para processar o script. Assim, resulta em uma sa\u00edda semelhante \u00e0 mostrada abaixo:</p> <pre><code>tarefa 3 do processo\ntarefa 1 do processo\ntarefa 2 do processo\n</code></pre> <p>Warning</p> <p>O canal garante que os itens sejam entregues na mesma ordem em que foram enviados - mas - como o processo \u00e9 executado de forma paralela, n\u00e3o h\u00e1 garantia de que sejam processados na mesma ordem em que foram recebidos.</p>"},{"location":"basic_training/processes.pt/#arquivo-e-caminhos-de-entrada","title":"Arquivo e caminhos de entrada","text":"<p>O qualificador <code>path</code> permite a manipula\u00e7\u00e3o de arquivos no contexto de execu\u00e7\u00e3o do processo. Isso significa que o Nextflow ir\u00e1 mover os arquivos necess\u00e1rios para o diret\u00f3rio de execu\u00e7\u00e3o do processo e estes poder\u00e3o ser acessados no script usando o nome especificado na declara\u00e7\u00e3o de entrada.</p> <pre><code>leituras = Channel.fromPath('data/ggal/*.fq')\nprocess FOO {\ndebug true\ninput:\npath 'amostra.fastq'\nscript:\n\"\"\"\n    ls amostra.fastq\n    \"\"\"\n}\nworkflow {\nresultado = FOO(leituras)\n}\n</code></pre> <p>O nome do arquivo de entrada tamb\u00e9m pode ser definido usando uma refer\u00eancia de vari\u00e1vel conforme mostrado abaixo:</p> <pre><code>leituras = Channel.fromPath('data/ggal/*.fq')\nprocess FOO {\ndebug true\ninput:\npath amostra\nscript:\n\"\"\"\n    ls $amostra\n    \"\"\"\n}\nworkflow {\nresultado = FOO(leituras)\n}\n</code></pre> <p>A mesma sintaxe tamb\u00e9m \u00e9 capaz de lidar com mais de um arquivo de entrada na mesma execu\u00e7\u00e3o e requer apenas a altera\u00e7\u00e3o da composi\u00e7\u00e3o do canal.</p> <pre><code>leituras = Channel.fromPath('data/ggal/*.fq')\nprocess FOO {\ndebug true\ninput:\npath amostra\nscript:\n\"\"\"\n    ls -lh $amostra\n    \"\"\"\n}\nworkflow {\nFOO(leituras.collect())\n}\n</code></pre> <p>Warning</p> <p>No passado, o qualificador <code>file</code> era usado para arquivos, mas o qualificador <code>path</code> deve ser preferido ao <code>file</code> para lidar com arquivos de entrada de processo ao usar o Nextflow 19.10.0 ou posterior. Quando um processo declara um arquivo de entrada, os elementos de canal correspondentes devem ser objetos file criados com a fun\u00e7\u00e3o auxiliar de arquivo das f\u00e1bricas de canal espec\u00edficas de arquivo (por exemplo, <code>Channel.fromPath</code> ou <code>Channel.fromFilePairs</code>).</p> <p>Exercise</p> <p>Escreva um script que crie um canal contendo todos as leituras correspondentes ao padr\u00e3o <code>data/ggal/*_1.fq</code> seguido por um processo que os concatene em um \u00fanico arquivo e imprima as primeiras 10 linhas.</p> Solution <pre><code>params.leituras = \"$baseDir/data/ggal/*_1.fq\"\nChannel\n.fromPath(params.leituras)\n.set { canal_leituras }\nprocess CONCATENE {\ntag \"Concatene todos os arquivos\"\ninput:\npath '*'\noutput:\npath 'top_10_linhas'\nscript:\n\"\"\"\n    cat * &gt; concatenado.txt\n    head -n 10 concatenado.txt &gt; top_10_linhas\n    \"\"\"\n}\nworkflow {\ncanal_concatenado = CONCATENE(canal_leituras.collect())\ncanal_concatenado.view()\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#combinando-canais-de-entrada","title":"Combinando canais de entrada","text":"<p>Uma caracter\u00edstica fundamental dos processos \u00e9 a capacidade de lidar com entradas de v\u00e1rios canais. No entanto, \u00e9 importante entender como o conte\u00fado do canal e sua sem\u00e2ntica afetam a execu\u00e7\u00e3o de um processo.</p> <p>Considere o seguinte exemplo:</p> <pre><code>canal1 = Channel.of(1, 2, 3)\ncanal2 = Channel.of('a', 'b', 'c')\nprocess FOO {\ndebug true\ninput:\nval x\nval y\nscript:\n\"\"\"\n    echo $x e $y\n    \"\"\"\n}\nworkflow {\nFOO(canal1, canal2)\n}\n</code></pre> <p>Ambos os canais emitem tr\u00eas valores, portanto o processo \u00e9 executado tr\u00eas vezes, cada vez com um par diferente:</p> <ul> <li><code>(1, a)</code></li> <li><code>(2, b)</code></li> <li><code>(3, c)</code></li> </ul> <p>O que est\u00e1 acontecendo \u00e9 que o processo espera at\u00e9 que haja uma configura\u00e7\u00e3o de entrada completa, ou seja, recebe um valor de entrada de todos os canais declarados como entrada.</p> <p>Quando essa condi\u00e7\u00e3o \u00e9 verificada, ele consome os valores de entrada provenientes dos respectivos canais, gera uma execu\u00e7\u00e3o de tarefa e repete a mesma l\u00f3gica at\u00e9 que um ou mais canais n\u00e3o tenham mais conte\u00fado.</p> <p>Isso significa que os valores do canal s\u00e3o consumidos serialmente um ap\u00f3s o outro e o primeiro canal vazio faz com que a execu\u00e7\u00e3o do processo pare, mesmo que existam outros valores em outros canais.</p> <p>Ent\u00e3o, o que acontece quando os canais n\u00e3o t\u00eam a mesma cardinalidade (isto \u00e9, eles emitem um n\u00famero diferente de elementos)?</p> <p>Por exemplo:</p> <pre><code>entrada1 = Channel.of(1, 2)\nentrada2 = Channel.of('a', 'b', 'c', 'd')\nprocess FOO {\ndebug true\ninput:\nval x\nval y\nscript:\n\"\"\"\n    echo $x e $y\n    \"\"\"\n}\nworkflow {\nFOO(entrada1, entrada2)\n}\n</code></pre> <p>No exemplo acima, o processo s\u00f3 \u00e9 executado duas vezes porque o processo para quando um canal n\u00e3o tem mais dados para serem processados.</p> <p>No entanto, o que acontece se voc\u00ea substituir o valor <code>x</code> por um canal de valor?</p> <p>Compare o exemplo anterior com o seguinte:</p> <pre><code>entrada1 = Channel.value(1)\nentrada2 = Channel.of('a', 'b', 'c')\nprocess BAR {\ndebug true\ninput:\nval x\nval y\nscript:\n\"\"\"\n    echo $x e $y\n    \"\"\"\n}\nworkflow {\nBAR(entrada1, entrada2)\n}\n</code></pre> Script output<pre><code>1 e b\n1 e a\n1 e c\n</code></pre> <p>Isso ocorre porque os canais de valor podem ser consumidos v\u00e1rias vezes e n\u00e3o afetam o t\u00e9rmino do processo.</p> <p>Exercise</p> <p>Escreva um processo que \u00e9 executado para cada arquivo de leitura correspondente ao padr\u00e3o <code>data/ggal/*_1.fq</code> e use o mesmo <code>data/ggal/transcriptome.fa</code> em cada execu\u00e7\u00e3o.</p> Solution <pre><code>params.leituras = \"$baseDir/data/ggal/*_1.fq\"\nparams.arquivo_transcriptoma = \"$baseDir/data/ggal/transcriptome.fa\"\nChannel\n.fromPath(params.leituras)\n.set { canal_leituras }\nprocess COMANDO {\ntag \"Execute_comando\"\ninput:\npath leituras\npath transcriptoma\noutput:\npath resultado\nscript:\n\"\"\"\n    echo seu_comando $leituras $transcriptoma &gt; resultado\n    \"\"\"\n}\nworkflow {\ncanal_concatenado = COMANDO(canal_leituras, params.arquivo_transcriptoma)\ncanal_concatenado.view()\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#repetidores-de-entradas","title":"Repetidores de entradas","text":"<p>O qualificador <code>each</code> permite que voc\u00ea repita a execu\u00e7\u00e3o de um processo para cada item em uma cole\u00e7\u00e3o toda vez que novos dados s\u00e3o recebidos. Por exemplo:</p> <pre><code>sequencias = Channel.fromPath('data/prots/*.tfa')\nmetodos = ['regular', 'espresso', 'psicoffee']\nprocess ALINHESEQUENCIAS {\ndebug true\ninput:\npath sequencia\neach modo\nscript:\n\"\"\"\n    echo t_coffee -in $sequencia -mode $modo\n    \"\"\"\n}\nworkflow {\nALINHESEQUENCIAS(sequencias, metodos)\n}\n</code></pre> <p>No exemplo acima, toda vez que um arquivo de sequ\u00eancias \u00e9 recebido como entrada pelo processo, ele executa tr\u00eas tarefas, cada uma executando um m\u00e9todo de alinhamento diferente definido como uma vari\u00e1vel <code>modo</code>. Isso \u00e9 \u00fatil quando voc\u00ea precisa repetir a mesma tarefa para um determinado conjunto de par\u00e2metros.</p> <p>Exercise</p> <p>Estenda o exemplo anterior para que uma tarefa seja executada para cada arquivo de leitura correspondente ao padr\u00e3o <code>data/ggal/*_1.fq</code> e repita a mesma tarefa com <code>salmon</code> e <code>kallisto</code>.</p> Solution <pre><code>params.leituras = \"$baseDir/data/ggal/*_1.fq\"\nparams.arquivo_transcriptoma = \"$baseDir/data/ggal/transcriptome.fa\"\nmetodos= ['salmon', 'kallisto']\nChannel\n.fromPath(params.leituras)\n.set { canal_leituras }\nprocess COMANDO {\ntag \"Execute_comando\"\ninput:\npath leituras\npath transcriptoma\neach modo\noutput:\npath resultado\nscript:\n\"\"\"\n    echo $modo $leituras $transcriptoma &gt; resultado\n    \"\"\"\n}\nworkflow {\ncanal_concatenado = COMANDO(canal_leituras, params.arquivo_transcriptoma, metodos)\ncanal_concatenado.view { \"Para executar : ${it.text}\" }\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#canais-de-saida","title":"Canais de sa\u00edda","text":"<p>O bloco <code>output</code> define os canais usados pelo processo para enviar os resultados produzidos.</p> <p>Apenas um bloco de sa\u00edda, que pode conter uma ou mais declara\u00e7\u00f5es de sa\u00edda, pode ser definido. O bloco de sa\u00edda segue a sintaxe mostrada abaixo:</p> <pre><code>output:\n&lt;qualificador da vari\u00e1vel de sa\u00edda&gt; &lt;nome da vari\u00e1vel de sa\u00edda&gt;, emit: &lt;nome do canal de sa\u00edda&gt;\n</code></pre>"},{"location":"basic_training/processes.pt/#valores-de-saida","title":"Valores de sa\u00edda","text":"<p>O qualificador <code>val</code> especifica um valor definido no contexto do script. Os valores s\u00e3o frequentemente definidos nos blocos de input e/ou output, conforme mostrado no exemplo a seguir:</p> <pre><code>metodos = ['prot', 'dna', 'rna']\nprocess FOO {\ninput:\nval x\noutput:\nval x\nscript:\n\"\"\"\n    echo $x &gt; arquivo\n    \"\"\"\n}\nworkflow {\ncanal_de_recebimento = FOO(Channel.of(metodos))\ncanal_de_recebimento.view { \"Recebido: $it\" }\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#caminhos-e-arquivos-de-saida","title":"Caminhos e arquivos de sa\u00edda","text":"<p>O qualificador <code>path</code> especifica um ou mais arquivos produzidos pelo processo no canal especificado como uma sa\u00edda.</p> <pre><code>process NUMALEATORIO {\noutput:\npath 'resultado.txt'\nscript:\n\"\"\"\n    echo \\$RANDOM &gt; resultado.txt\n    \"\"\"\n}\nworkflow {\ncanal_de_recebimento = NUMALEATORIO()\ncanal_de_recebimento.view { \"Recebido: \" + it.text }\n}\n</code></pre> <p>No exemplo acima, o processo <code>NUMALEATORIO</code> cria um arquivo chamado <code>resultado.txt</code> contendo um n\u00famero aleat\u00f3rio.</p> <p>Como um par\u00e2metro de arquivo usando o mesmo nome \u00e9 declarado no bloco de sa\u00edda, o arquivo \u00e9 enviado pelo canal <code>canal_de_recebimento</code> quando a tarefa \u00e9 conclu\u00edda. Um processo posterior declarando o mesmo canal como input ser\u00e1 capaz de receb\u00ea-lo.</p>"},{"location":"basic_training/processes.pt/#multiplos-arquivos-de-saida","title":"M\u00faltiplos arquivos de sa\u00edda","text":"<p>Quando um nome de arquivo de sa\u00edda cont\u00e9m um caractere curinga (<code>*</code> ou <code>?</code>), ele \u00e9 interpretado como um glob de correspond\u00eancia para um caminho. Isso nos permite capturar v\u00e1rios arquivos em um objeto de lista e exibi-los como uma \u00fanica emiss\u00e3o. Por exemplo:</p> <pre><code>process SEPARARLETRAS {\noutput:\npath 'pedaco_*'\nscript:\n\"\"\"\n    printf 'Hola' | split -b 1 - pedaco_\n    \"\"\"\n}\nworkflow {\nletters = SEPARARLETRAS()\nletters\n.flatMap()\n.view { \"Arquivo: ${it.name} =&gt; ${it.text}\" }\n}\n</code></pre> <p>Imprime o seguinte:</p> <pre><code>Arquivo: pedaco_aa =&gt; H\nArquivo: pedaco_ab =&gt; o\nArquivo: pedaco_ac =&gt; l\nArquivo: pedaco_ad =&gt; a\n</code></pre> <p>Algumas advert\u00eancias sobre o comportamento de padr\u00f5es de glob:</p> <ul> <li>Os arquivos de entrada n\u00e3o est\u00e3o inclu\u00eddos na lista de poss\u00edveis correspond\u00eancias</li> <li>O padr\u00e3o glob corresponde tanto a arquivos quanto caminhos de diret\u00f3rio</li> <li>Quando um padr\u00e3o de duas estrelas <code>**</code> \u00e9 usado para acessar os diret\u00f3rios, apenas os caminhos de arquivo s\u00e3o correspondidos, ou seja, os diret\u00f3rios n\u00e3o s\u00e3o inclu\u00eddos na lista de resultados.</li> </ul> <p>Exercise</p> <p>Remova o operador <code>flatMap</code> e veja a mudan\u00e7a de sa\u00edda. A documenta\u00e7\u00e3o para o operador <code>flatMap</code> est\u00e1 dispon\u00edvel nesse link.</p> Solution <pre><code>File: [pedaco_aa, pedaco_ab, pedaco_ac, pedaco_ad] =&gt; [H, o, l, a]\n</code></pre>"},{"location":"basic_training/processes.pt/#nomes-dinamicos-de-arquivos-de-saida","title":"Nomes din\u00e2micos de arquivos de sa\u00edda","text":"<p>Quando um nome de arquivo de sa\u00edda precisa ser expresso dinamicamente, \u00e9 poss\u00edvel defini-lo usando uma string din\u00e2mica que faz refer\u00eancia a valores definidos no bloco de declara\u00e7\u00e3o de entrada ou no contexto global do script. Por exemplo:</p> <pre><code>especies = ['gato', 'cachorro', 'pregui\u00e7a']\nsequencias = ['AGATAG', 'ATGCTCT', 'ATCCCAA']\nChannel\n.fromList(especies)\n.set { canal_especies }\nprocess ALINHAR {\ninput:\nval x\nval sequencia\noutput:\npath \"${x}.aln\"\nscript:\n\"\"\"\n    echo alinhar -in $sequencia &gt; ${x}.aln\n    \"\"\"\n}\nworkflow {\ngenomas = ALINHAR(canal_especies, sequencias)\ngenomas.view()\n}\n</code></pre> <p>No exemplo acima, cada vez que o processo \u00e9 executado, \u00e9 gerado um arquivo de alinhamento cujo nome depende do valor da entrada <code>x</code>.</p>"},{"location":"basic_training/processes.pt/#entradas-e-saidas-compostas","title":"Entradas e sa\u00eddas compostas","text":"<p>At\u00e9 agora, vimos como declarar v\u00e1rios canais de entrada e sa\u00edda que podem lidar com um valor por vez. No entanto, o Nextflow tamb\u00e9m pode lidar com tuplas de valores.</p> <p>As declara\u00e7\u00f5es de entrada e sa\u00edda para tuplas devem ser declaradas com um qualificador <code>tuple</code> seguido pela defini\u00e7\u00e3o de cada elemento na tupla.</p> <pre><code>canal_leituras = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\nprocess FOO {\ninput:\ntuple val(id_amostra), path(arquivos_amostra)\noutput:\ntuple val(id_amostra), path('sample.bam')\nscript:\n\"\"\"\n    echo seu_comando_aqui --leituras $arquivos_amostra &gt; sample.bam\n    \"\"\"\n}\nworkflow {\ncanal_bam = FOO(canal_leituras)\ncanal_bam.view()\n}\n</code></pre> <p>Info</p> <p>Nas vers\u00f5es anteriores do Nextflow <code>tuple</code> era chamado <code>set</code>, mas era usado da mesma forma com a mesma sem\u00e2ntica.</p> <p>Exercise</p> <p>Modifique o script do exerc\u00edcio anterior para que o arquivo bam seja nomeado com o <code>id_amostra</code> fornecido.</p> Solution <pre><code>canal_leituras = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\nprocess FOO {\ninput:\ntuple val(id_amostra), path(arquivos_amostra)\noutput:\ntuple val(id_amostra), path(\"${id_amostra}.bam\")\nscript:\n\"\"\"\n    echo seu_comando_aqui --leituras $arquivos_amostra &gt; ${id_amostra}.bam\n    \"\"\"\n}\nworkflow {\ncanal_bam = FOO(canal_leituras)\ncanal_bam.view()\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#when","title":"When","text":"<p>A declara\u00e7\u00e3o <code>when</code> permite que voc\u00ea defina uma condi\u00e7\u00e3o que deve ser verificada para executar o processo. Pode ser qualquer express\u00e3o que avalie um valor booleano.</p> <p>\u00c9 \u00fatil habilitar/desabilitar a execu\u00e7\u00e3o do processo dependendo do estado de v\u00e1rias entradas e par\u00e2metros. Por exemplo:</p> <pre><code>params.tipo_banco = 'nr'\nparams.prot = 'data/prots/*.tfa'\nproteinas = Channel.fromPath(params.prot)\nprocess ENCONTRAR {\ndebug true\ninput:\npath fasta\nval tipo\nwhen:\nfasta.name =~ /^BB11.*/ &amp;&amp; tipo == 'nr'\nscript:\n\"\"\"\n    echo blastp -checar $fasta -tipo_banco nr\n    \"\"\"\n}\nworkflow {\nresultado = ENCONTRAR(proteinas, params.tipo_banco)\n}\n</code></pre>"},{"location":"basic_training/processes.pt/#diretivas","title":"Diretivas","text":"<p>As declara\u00e7\u00f5es de diretiva permitem a defini\u00e7\u00e3o de configura\u00e7\u00f5es opcionais que afetam a execu\u00e7\u00e3o do processo atual sem afetar a sem\u00e2ntica da pr\u00f3pria tarefa.</p> <p>Elas devem ser inseridas no topo do corpo do processo, antes de quaisquer outros blocos de declara\u00e7\u00e3o (ou seja, <code>input</code>, <code>output</code>, etc.).</p> <p>Diretivas s\u00e3o comumente usadas para definir a quantidade de recursos computacionais a serem usados ou outras meta diretivas que permitem a defini\u00e7\u00e3o de configura\u00e7\u00e3o extra de informa\u00e7\u00f5es de log. Por exemplo:</p> <pre><code>process FOO {\ncpus 2\nmemory 1.GB\ncontainer 'nome/imagem'\nscript:\n\"\"\"\n    echo seu_comando --isso --aquilo\n    \"\"\"\n}\n</code></pre> <p> A lista completa de diretivas est\u00e1 dispon\u00edvel aqui.</p> Nome Descri\u00e7\u00e3o <code>cpus</code> Permite definir o n\u00famero de CPUs (l\u00f3gicas) necess\u00e1rias para a tarefa do processo. <code>time</code> Permite definir por quanto tempo a tarefa pode ser executado (por exemplo, tempo 1h: 1 hora, 1s 1 segundo, 1m 1 minuto, 1d 1 dia). <code>memory</code> Permite definir quanta mem\u00f3ria a tarefa pode usar (por exemplo, 2 GB \u00e9 2 GB). Tamb\u00e9m pode usar B, KB, MB, GB e TB. <code>disk</code> Permite definir a quantidade de armazenamento em disco local que a tarefa pode usar. <code>tag</code> Permite associar cada execu\u00e7\u00e3o de processo a um r\u00f3tulo personalizado para facilitar sua identifica\u00e7\u00e3o no arquivo de log ou no relat\u00f3rio de execu\u00e7\u00e3o do rastreamento."},{"location":"basic_training/processes.pt/#organizando-as-saidas","title":"Organizando as sa\u00eddas","text":""},{"location":"basic_training/processes.pt/#a-diretiva-publishdir","title":"A diretiva PublishDir","text":"<p>Dado que cada tarefa est\u00e1 sendo executada em uma pasta <code>work/</code> tempor\u00e1ria separada (por exemplo, <code>work/f1/850698\u2026</code>; <code>work/g3/239712\u2026</code>; etc.), podemos salvar informa\u00e7\u00f5es importantes, n\u00e3o intermedi\u00e1rias, e/ou arquivos finais em uma pasta de resultados.</p> <p>Tip</p> <p>Lembre-se de excluir a pasta de trabalho (<code>work</code>) de vez em quando para limpar seus arquivos intermedi\u00e1rios e impedir que eles encham seu computador!</p> <p>Para armazenar nossos arquivos de resultado do fluxo de trabalho, precisamos marc\u00e1-los explicitamente usando a diretiva publishDir no processo que est\u00e1 criando os arquivos. Por exemplo:</p> <pre><code>params.diretorio_saida = 'meus-resultados'\nparams.prot = 'data/prots/*.tfa'\nproteinas = Channel.fromPath(params.prot)\nprocess BLASTSEQ {\npublishDir \"$params.diretorio_saida/arquivos_bam\", mode: 'copy'\ninput:\npath fasta\noutput:\npath ('*.txt')\nscript:\n\"\"\"\n    echo blastp $fasta &gt; ${fasta}_resultado.txt\n    \"\"\"\n}\nworkflow {\ncanal_blast = BLASTSEQ(proteinas)\ncanal_blast.view()\n}\n</code></pre> <p>O exemplo acima copiar\u00e1 todos os arquivos de script blast criados pelo processo <code>BLASTSEQ</code> no caminho do diret\u00f3rio <code>meus-resultados</code>.</p> <p>Tip</p> <p>O diret\u00f3rio de publica\u00e7\u00e3o pode ser local ou remoto. Por exemplo, os arquivos de sa\u00edda podem ser armazenados usando um bucket AWS S3 usando o prefixo <code>s3://</code> no caminho de destino.</p>"},{"location":"basic_training/processes.pt/#gerenciando-semantica-de-subdiretorios","title":"Gerenciando sem\u00e2ntica de subdiret\u00f3rios","text":"<p>Voc\u00ea pode usar mais de um <code>publishDir</code> para manter sa\u00eddas diferentes em diret\u00f3rios separados. Por exemplo:</p> <pre><code>params.leituras = 'data/reads/*_{1,2}.fq.gz'\nparams.diretorio_saida = 'meus-resultados'\ncanal_amostras = Channel.fromFilePairs(params.leituras, flat: true)\nprocess FOO {\npublishDir \"$params.diretorio_saida/$id_amostra/\", pattern: '*.fq'\npublishDir \"$params.diretorio_saida/$id_amostra/contagens\", pattern: \"*_contagens.txt\"\npublishDir \"$params.diretorio_saida/$id_amostra/panoramas\", pattern: '*_panorama.txt'\ninput:\ntuple val(id_amostra), path('amostra1.fq.gz'), path('amostra2.fq.gz')\noutput:\npath \"*\"\nscript:\n\"\"\"\n    zcat amostra1.fq.gz &gt; amostra1.fq\n    zcat amostra2.fq.gz &gt; amostra2.fq\n    awk '{s++}END{print s/4}' amostra1.fq &gt; amostra1_contagens.txt\n    awk '{s++}END{print s/4}' amostra2.fq &gt; amostra2_contagens.txt\n    head -n 50 amostra1.fq &gt; amostra1_panorama.txt\n    head -n 50 amostra2.fq &gt; amostra2_panorama.txt\n    \"\"\"\n}\nworkflow {\ncanal_saida = FOO(canal_amostras)\n}\n</code></pre> <p>O exemplo acima criar\u00e1 uma estrutura de sa\u00edda no diret\u00f3rio <code>meus-resultados</code>, que cont\u00e9m um subdiret\u00f3rio separado para cada ID de amostra fornecido, cada um contendo as pastas <code>contagens</code> e <code>panoramas</code>.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/","title":"Workflow simple de RNA-Seq","text":"<p>Pour d\u00e9montrer un sc\u00e9nario biom\u00e9dical r\u00e9el, nous mettrons en \u0153uvre une preuve de concept de flux de travail RNA-Seq qui :</p> <ol> <li>Indexer un fichier de transcriptome</li> <li>Effectuer des contr\u00f4les de qualit\u00e9</li> <li>Effectuer la quantification</li> <li>Cr\u00e9er un rapport MultiQC</li> </ol> <p>Pour ce faire, nous utiliserons une s\u00e9rie de sept scripts, chacun d'entre eux s'appuyant sur le pr\u00e9c\u00e9dent pour cr\u00e9er un flux de travail complet. Vous pouvez les trouver dans le dossier du didacticiel (<code>script1.nf</code> - <code>script7.nf</code>). Ces scripts utiliseront des outils tiers connus des bioinformaticiens mais qui peuvent \u00eatre nouveaux pour vous, nous les pr\u00e9senterons donc bri\u00e8vement ci-dessous.</p> <ol> <li>Salmon est un outil permettant de quantifier les mol\u00e9cules connues sous le nom de transcrits \u00e0 l'aide d'un type de donn\u00e9es appel\u00e9 donn\u00e9es RNA-seq.</li> <li>FastQC est un outil permettant d'effectuer un contr\u00f4le de la qualit\u00e9 des donn\u00e9es de s\u00e9quences \u00e0 haut d\u00e9bit. Vous pouvez le consid\u00e9rer comme un moyen d'\u00e9valuer la qualit\u00e9 de vos donn\u00e9es.</li> <li>MultiQC recherche les journaux d'analyse dans un r\u00e9pertoire donn\u00e9 et compile un rapport HTML. Il s'agit d'un outil d'usage g\u00e9n\u00e9ral, parfait pour r\u00e9sumer les r\u00e9sultats de nombreux outils bioinformatiques.</li> </ol> <p>M\u00eame si ces outils ne sont pas ceux que vous utiliserez dans votre pipeline, ils peuvent \u00eatre remplac\u00e9s par n'importe quel outil courant de votre secteur. C'est ca la puissance de Nextflow !</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#definir-les-parametres-du-workflow","title":"D\u00e9finir les param\u00e8tres du workflow","text":"<p>Les param\u00e8tres sont des entr\u00e9es et des options qui peuvent \u00eatre modifi\u00e9es lors de l'ex\u00e9cution du flux de travail.</p> <p>Le script <code>script1.nf</code> d\u00e9finit les param\u00e8tres d'entr\u00e9e du workflow.</p> <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nprintln \"reads: $params.reads\"\n</code></pre> <p>Ex\u00e9cutez-le en utilisant la commande suivante :</p> <pre><code>nextflow run script1.nf\n</code></pre> <p>Essayez de sp\u00e9cifier un param\u00e8tre d'entr\u00e9e diff\u00e9rent dans votre commande d'ex\u00e9cution, par exemple :</p> <pre><code>nextflow run script1.nf --reads '/workspace/gitpod/nf-training/data/ggal/lung_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#exercices","title":"Exercices","text":"<p>Exercise</p> <p>Modifiez le fichier <code>script1.nf</code> en ajoutant un quatri\u00e8me param\u00e8tre nomm\u00e9 <code>outdir</code> et d\u00e9finissez-le comme chemin par d\u00e9faut qui sera utilis\u00e9 comme r\u00e9pertoire de sortie du workflow.</p> Solution <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n</code></pre> <p>Exercise</p> <p>Modifier <code>script1.nf</code> pour imprimer tous les param\u00e8tres du workflow en utilisant une seule commande <code>log.info</code> sous la forme d'une cha\u00eene multiligne.</p> <p> regarde l'example ici.</p> Solution <p>Ajoutez ce qui suit \u00e0 votre fichier script:</p> <pre><code>log.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n.stripIndent(true)\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#resume","title":"R\u00e9sum\u00e9","text":"<p>Au cours de cette \u00e9tape, vous avez appris:</p> <ol> <li>Comment d\u00e9finir les param\u00e8tres dans votre script de workflow</li> <li>Comment passer des param\u00e8tres en utilisant la ligne de commande</li> <li>L'utilisation des variables <code>$var</code> et <code>${var}</code>.</li> <li>Comment utiliser des cha\u00eenes de caract\u00e8res multilignes</li> <li>Comment utiliser <code>log.info</code> pour imprimer des informations et les sauvegarder dans le fichier d'ex\u00e9cution du journal.</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.fr/#creer-un-fichier-dindex-du-transcriptome","title":"Cr\u00e9er un fichier d'index du transcriptome","text":"<p>Nextflow permet l'ex\u00e9cution de n'importe quelle commande ou script en utilisant une d\u00e9finition <code>processus</code>.</p> <p>Un \"processus\" est d\u00e9fini par trois d\u00e9clarations principales : le processus <code>input</code>, <code>output</code> et la commande <code>script</code>.</p> <p>Pour ajouter une \u00e9tape de traitement <code>INDEX</code> du transcriptome, essayez d'ajouter les blocs de code suivants \u00e0 votre <code>script1.nf</code>. Alternativement, ces blocs de code ont d\u00e9j\u00e0 \u00e9t\u00e9 ajout\u00e9s \u00e0 <code>script2.nf</code>.</p> <pre><code>/*\n * d\u00e9finir le processus INDEX qui cr\u00e9e un index binaire\n * compte tenu du fichier de transcriptome\n */\nprocess INDEX {\ninput:\npath transcriptome\noutput:\npath 'salmon_index'\nscript:\n\"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> <p>En outre, ajoutez un champ d'application de workflow contenant une d\u00e9finition de canal d'entr\u00e9e et le processus d'indexation :</p> <pre><code>workflow {\nindex_ch = INDEX(params.transcriptome_file)\n}\n</code></pre> <p>Ici, le param\u00e8tre <code>params.transcriptome_file</code> est utilis\u00e9 comme entr\u00e9e pour le processus <code>INDEX</code>. Le processus <code>INDEX</code> (utilisant l'outil <code>salmon</code>) cr\u00e9e <code>salmon_index</code>, un transcriptome index\u00e9 qui est transmis en sortie au canal <code>index_ch</code>.</p> <p>Info</p> <p>La d\u00e9claration <code>input</code> d\u00e9finit une variable de chemin <code>transcriptome</code> qui est utilis\u00e9e dans le <code>script</code> comme r\u00e9f\u00e9rence (en utilisant le symbole du dollar) dans la ligne de commande Salmon.</p> <p>Warning</p> <p>Les besoins en ressources tels que les CPUs et les limites de m\u00e9moire peuvent changer avec les diff\u00e9rentes ex\u00e9cutions de workflow et les plateformes. Nextflow peut utiliser <code>$task.cpus</code> comme variable pour le nombre de CPU. Voir process directives documentation pour plus de d\u00e9tails.</p> <p>Ex\u00e9cutez-le en utilisant la commande :</p> <pre><code>nextflow run script2.nf\n</code></pre> <p>L'ex\u00e9cution \u00e9chouera car <code>salmon</code> n'est pas install\u00e9 dans votre environnement.</p> <p>Ajoutez l'option de ligne de commande <code>-with-docker</code> pour lancer l'ex\u00e9cution \u00e0 travers un conteneur Docker, comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run script2.nf -with-docker\n</code></pre> <p>Cette fois l'ex\u00e9cution fonctionnera car elle utilise le conteneur Docker <code>nextflow/rnaseq-nf</code> qui est d\u00e9fini dans le fichier <code>nextflow.config</code> dans votre r\u00e9pertoire courant. Si vous ex\u00e9cutez ce script localement, vous devrez t\u00e9l\u00e9charger Docker sur votre machine, vous connecter et activer Docker, et autoriser le script \u00e0 t\u00e9l\u00e9charger le conteneur contenant les scripts d'ex\u00e9cution. Vous pouvez en savoir plus sur Docker ici.</p> <p>Pour \u00e9viter d'ajouter <code>-with-docker</code> \u00e0 chaque fois que vous ex\u00e9cutez le script, ajoutez la ligne suivante au fichier <code>nextflow.config</code> :</p> <pre><code>docker.enabled = true\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#exercices_1","title":"Exercices","text":"<p>Exercise</p> <p>Activez l'ex\u00e9cution Docker par d\u00e9faut en ajoutant le param\u00e8tre ci-dessus dans le fichier <code>nextflow.config</code>.</p> <p>Exercise</p> <p>Imprimer la sortie du canal <code>index_ch</code> en utilisant l'op\u00e9rateur view.</p> Solution <p>Ajoutez ce qui suit \u00e0 la fin de votre bloc de workflow dans votre fichier script</p> <pre><code>index_ch.view()\n</code></pre> <p>Exercise</p> <p>Si vous disposez de plus d'unit\u00e9s centrales, essayez de modifier votre script pour demander plus de ressources pour ce processus. Par exemple, voir la directive docs. <code>$task.cpus</code> est d\u00e9j\u00e0 sp\u00e9cifi\u00e9 dans ce script, donc d\u00e9finir le nombre de CPUs comme une directive indiquera \u00e0 Nextflow comment ex\u00e9cuter ce processus, en termes de nombre de CPUs.</p> Solution <p>Ajouter <code>cpus 2</code> au d\u00e9but du processus d'indexation :</p> <pre><code>process INDEX {\ncpus 2\ninput:\n...\n</code></pre> <p>V\u00e9rifiez ensuite qu'il a fonctionn\u00e9 en regardant le script ex\u00e9cut\u00e9 dans le r\u00e9pertoire work. Cherchez l'hexad\u00e9cimal (par exemple <code>work/7f/f285b80022d9f61e82cd7f90436aa4/</code>), puis <code>cat</code> le fichier <code>.command.sh</code>.</p> <p>Exercice Bonus</p> <p>Utilisez la commande <code>tree work</code> pour voir comment Nextflow organise le r\u00e9pertoire work du processus. V\u00e9rifiez ici si vous avez besoin de t\u00e9l\u00e9charger <code>tree</code>.</p> Solution <p>Il devrait ressembler \u00e0 ceci :</p> <pre><code>work\n\u251c\u2500\u2500 17\n\u2502   \u2514\u2500\u2500 263d3517b457de4525513ae5e34ea8\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u251c\u2500\u2500 complete_ref_lens.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctable.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctg_offsets.bin\n\u2502       \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n\u2502       \u2502   \u251c\u2500\u2500 eqtable.bin\n\u2502       \u2502   \u251c\u2500\u2500 info.json\n\u2502       \u2502   \u251c\u2500\u2500 mphf.bin\n\u2502       \u2502   \u251c\u2500\u2500 pos.bin\n\u2502       \u2502   \u251c\u2500\u2500 pre_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 rank.bin\n\u2502       \u2502   \u251c\u2500\u2500 refAccumLengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 ref_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 reflengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 refseq.bin\n\u2502       \u2502   \u251c\u2500\u2500 seq.bin\n\u2502       \u2502   \u2514\u2500\u2500 versionInfo.json\n\u2502       \u2514\u2500\u2500 transcriptome.fa -&gt; /workspace/Gitpod_test/data/ggal/transcriptome.fa\n\u251c\u2500\u2500 7f\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#resume_1","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment d\u00e9finir un processus ex\u00e9cutant une commande personnalis\u00e9e</li> <li>Comment d\u00e9clarer les entr\u00e9es d'un processus</li> <li>Comment d\u00e9clarer les sorties d'un processus</li> <li>Comment imprimer le contenu d'un canal</li> <li>Comment acc\u00e9der au nombre de CPU disponibles</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.fr/#collecter-les-fichiers-lus-par-paires","title":"Collecter les fichiers lus par paires","text":"<p>Cette \u00e9tape montre comment faire correspondre les fichiers read par paires, afin qu'ils puissent \u00eatre mis en correspondance par Salmon.</p> <p>Modifiez le script <code>script3.nf</code> en ajoutant la d\u00e9claration suivante \u00e0 la derni\u00e8re ligne du fichier :</p> <pre><code>read_pairs_ch.view()\n</code></pre> <p>Sauvegardez-le et ex\u00e9cutez-le avec la commande suivante :</p> <pre><code>nextflow run script3.nf\n</code></pre> <p>Il s'affichera quelque chose de similaire \u00e0 ceci :</p> <pre><code>[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n</code></pre> <p>L'exemple ci-dessus montre comment le canal <code>read_pairs_ch</code> \u00e9met des tuples compos\u00e9s de deux \u00e9l\u00e9ments, le premier \u00e9tant le pr\u00e9fixe de la paire lue et le second une liste repr\u00e9sentant les fichiers actuels.</p> <p>Essayez \u00e0 nouveau en sp\u00e9cifiant diff\u00e9rents fichiers de lecture \u00e0 l'aide d'un motif global :</p> <pre><code>nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Warning</p> <p>Les chemins d'acc\u00e8s aux fichiers qui incluent un ou plusieurs jokers, c'est-\u00e0-dire <code>*</code>, <code>?</code>, etc., DOIVENT \u00eatre entour\u00e9s de caract\u00e8res entre guillemets simples afin d'\u00e9viter que Bash n'\u00e9tende le glob.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#exercices_2","title":"Exercices","text":"<p>Exercise</p> <p>Utilisez l'op\u00e9rateur set \u00e0 la place de l'affectation <code>=</code> pour d\u00e9finir le canal <code>read_pairs_ch</code>.</p> Solution <pre><code>Channel\n.fromFilePairs(params.reads)\n.set { read_pairs_ch }\n</code></pre> <p>Exercise</p> <p>Utilisez l'option <code>checkIfExists</code> pour la fabrique de canaux fromFilePairs pour v\u00e9rifier si le chemin sp\u00e9cifi\u00e9 contient des paires de fichiers.</p> Solution <pre><code>Channel\n.fromFilePairs(params.reads, checkIfExists: true)\n.set { read_pairs_ch }\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#resume_2","title":"r\u00e9sum\u00e9","text":"<p>Au cours de cette \u00e9tape, vous avez appris:</p> <ol> <li>Comment utiliser <code>fromFilePairs</code> pour g\u00e9rer les fichiers de paires de lecture</li> <li>Comment utiliser l'option <code>checkIfExists</code> pour v\u00e9rifier l'existence des fichiers d'entr\u00e9e</li> <li>Comment utiliser l'op\u00e9rateur <code>set</code> pour d\u00e9finir une nouvelle variable de canal.</li> </ol> <p>Info</p> <p>La d\u00e9claration d'un canal peut se situer avant le champ d'application du workflow ou \u00e0 l'int\u00e9rieur de celui-ci. Tant qu'elle se situe en amont du processus qui requiert le canal sp\u00e9cifique.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#quantification-de-lexpression","title":"Quantification de l'expression","text":"<p>Le fichier <code>script4.nf</code> ajoute un processus de <code>QUANTIFICATION</code> de l'expression des g\u00e8nes et un appel \u00e0 ce processus dans le champ d'application du flux de travail. La quantification n\u00e9cessite les fichiers fastq du transcriptome index\u00e9 et de la paire reads de RNA-Seq.</p> <p>Dans le workflow scope, notez comment le canal <code>index_ch</code> est assign\u00e9 comme sortie dans le processus <code>INDEX</code>.</p> <p>Ensuite, notez que le premier canal d'entr\u00e9e pour le processus <code>QUANTIFICATION</code> est le <code>index_ch</code> pr\u00e9c\u00e9demment d\u00e9clar\u00e9, qui contient le <code>chemin</code> vers le <code>salmon_index</code>.</p> <p>Notez \u00e9galement que le second canal d'entr\u00e9e pour le processus de <code>QUANTIFICATION</code> est le <code>read_pair_ch</code> que nous venons de cr\u00e9er. C'est un <code>tuple</code> compos\u00e9 de deux \u00e9l\u00e9ments (une valeur : <code>sample_id</code> et une liste de chemins vers les lectures fastq : <code>reads</code>) afin de correspondre \u00e0 la structure des \u00e9l\u00e9ments \u00e9mis par la fabrique de canaux <code>fromFilePairs</code>.</p> <p>Ex\u00e9cutez-la en utilisant la commande suivante :</p> <pre><code>nextflow run script4.nf -resume\n</code></pre> <p>Vous verrez l'ex\u00e9cution du processus <code>QUANTIFICATION</code>.</p> <p>Lors de l'utilisation de l'option <code>-resume</code>, toute \u00e9tape qui a d\u00e9j\u00e0 \u00e9t\u00e9 trait\u00e9e est saut\u00e9e.</p> <p>Essayez d'ex\u00e9cuter \u00e0 nouveau le m\u00eame script avec davantage de fichiers lus, comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run script4.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Vous remarquerez que le processus <code>QUANTIFICATION</code> est ex\u00e9cut\u00e9 plusieurs fois.</p> <p>Nextflow parall\u00e9lise l'ex\u00e9cution de votre workflow en fournissant simplement plusieurs jeux de donn\u00e9es d'entr\u00e9e \u00e0 votre script.</p> <p>Tip</p> <p>Il peut \u00eatre utile d'appliquer des param\u00e8tres facultatifs \u00e0 un processus sp\u00e9cifique \u00e0 l'aide de directives en les sp\u00e9cifiant dans le corps du processus.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#exercices_3","title":"Exercices","text":"<p>Exercise</p> <p>Ajout d'une directive tag au processus <code>QUANTIFICATION</code> pour fournir un journal d'ex\u00e9cution plus lisible.</p> Solution <p>Ajoutez ce qui suit avant la d\u00e9claration d'entr\u00e9e :</p> <pre><code>tag \"Salmon on $sample_id\"\n</code></pre> <p>Exercise</p> <p>Ajoutez une directive publishDir au processus <code>QUANTIFICATION</code> pour stocker les r\u00e9sultats du processus dans un r\u00e9pertoire de votre choix.</p> Solution <p>Ajoutez ce qui suit avant la d\u00e9claration <code>input</code> dans le processus <code>QUANTIFICATION</code> :</p> <pre><code>publishDir params.outdir, mode: 'copy'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#resume_3","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment connecter deux processus ensemble en utilisant les d\u00e9clarations de canal</li> <li>Comment reprendre l'ex\u00e9cution du script et sauter les \u00e9tapes mises en cache</li> <li>Comment utiliser la directive <code>tag</code> pour fournir une sortie d'ex\u00e9cution plus lisible</li> <li>Comment utiliser la directive <code>publishDir</code> pour stocker les r\u00e9sultats d'un processus dans un chemin de votre choix.</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.fr/#controle-qualite","title":"Contr\u00f4le qualit\u00e9","text":"<p>Ensuite, nous impl\u00e9mentons une \u00e9tape de contr\u00f4le de qualit\u00e9 <code>FASTQC</code> pour vos reads d'entr\u00e9e (en utilisant le label <code>fastqc</code>). Les entr\u00e9es sont les m\u00eames que les paires de reads utilis\u00e9es dans l'\u00e9tape <code>QUANTIFICATION</code>.</p> <p>Vous pouvez l'ex\u00e9cuter en utilisant la commande suivante :</p> <pre><code>nextflow run script5.nf -resume\n</code></pre> <p>Nextflow DSL2 sait qu'il faut diviser les <code>reads_pair_ch</code> en deux canaux identiques car ils sont requis deux fois en tant qu'entr\u00e9e pour les processus <code>FASTQC</code> et <code>QUANTIFICATION</code>.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#rapport-de-multiqc","title":"Rapport de MultiQC","text":"<p>Cette \u00e9tape rassemble les r\u00e9sultats des processus de <code>QUANTIFICATION</code> et de <code>FASTQC</code> pour cr\u00e9er un rapport final \u00e0 l'aide de l'outil MultiQC.</p> <p>Ex\u00e9cutez le script suivant avec la commande suivante :</p> <pre><code>nextflow run script6.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Il cr\u00e9e le rapport final dans le dossier <code>results</code> du r\u00e9pertoire <code>work</code> actuel.</p> <p>Dans ce script, notez l'utilisation des op\u00e9rateurs mix et collect combin\u00e9s ensemble pour rassembler les sorties des processus <code>QUANTIFICATION</code> et <code>FASTQC</code> en une seule entr\u00e9e. Les op\u00e9rateurs Operators peuvent \u00eatre utilis\u00e9s pour combiner et transformer les canaux.</p> <pre><code>MULTIQC(quant_ch.mix(fastqc_ch).collect())\n</code></pre> <p>Nous voulons qu'une seule t\u00e2che de MultiQC soit ex\u00e9cut\u00e9e pour produire un rapport. Nous utilisons donc l'op\u00e9rateur de canal <code>mix</code> pour combiner les deux canaux, suivi de l'op\u00e9rateur <code>collect</code>, pour retourner le contenu complet du canal en un seul \u00e9l\u00e9ment.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#resume_4","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment collecter plusieurs sorties vers une seule entr\u00e9e avec l'op\u00e9rateur <code>collect</code>.</li> <li>Comment <code>mixer</code> deux canaux en un seul canal</li> <li>Comment encha\u00eener deux ou plusieurs op\u00e9rateurs ensemble</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.fr/#gerer-levenement-dachevement","title":"G\u00e9rer l'\u00e9v\u00e9nement d'ach\u00e8vement","text":"<p>Cette \u00e9tape montre comment ex\u00e9cuter une action lorsque le flux de travail a termin\u00e9 son ex\u00e9cution.</p> <p>Notez que les processus Nextflow d\u00e9finissent l'ex\u00e9cution de t\u00e2ches asynchrones, c'est-\u00e0-dire qu'elles ne sont pas ex\u00e9cut\u00e9es l'une apr\u00e8s l'autre comme si elles \u00e9taient \u00e9crites dans le script du workflow dans un langage de programmation imp\u00e9ratif commun.</p> <p>Le script utilise le gestionnaire d'\u00e9v\u00e9nement <code>workflow.onComplete</code> pour imprimer un message de confirmation lorsque le script est termin\u00e9.</p> <p>Essayez de l'ex\u00e9cuter en utilisant la commande suivante :</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#notifications-par-e-mail","title":"Notifications par e-mail","text":"<p>Envoyer un e-mail de notification lorsque l'ex\u00e9cution du flux de travail est termin\u00e9e en utilisant l'option de ligne de commande <code>-N &lt;adresse e-mail&gt;</code>.</p> <p>Note : ceci n\u00e9cessite la configuration d'un serveur SMTP dans le fichier de configuration de nextflow. Vous trouverez ci-dessous un exemple de fichier <code>nextflow.config</code> montrant les param\u00e8tres \u00e0 configurer :</p> <pre><code>mail {\nfrom = 'info@nextflow.io'\nsmtp.host = 'email-smtp.eu-west-1.amazonaws.com'\nsmtp.port = 587\nsmtp.user = \"xxxxx\"\nsmtp.password = \"yyyyy\"\nsmtp.auth = true\nsmtp.starttls.enable = true\nsmtp.starttls.required = true\n}\n</code></pre> <p>Voir mail documentation pour plus de d\u00e9tails.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#scripts-personnalises","title":"Scripts personnalis\u00e9s","text":"<p>Les workflows du monde r\u00e9el utilisent beaucoup de scripts utilisateurs personnalis\u00e9s (BASH, R, Python, etc.). Nextflow vous permet d'utiliser et de g\u00e9rer ces scripts de mani\u00e8re coh\u00e9rente. Il suffit de les placer dans un r\u00e9pertoire nomm\u00e9 <code>bin</code> \u00e0 la racine du projet de workflow. Ils seront automatiquement ajout\u00e9s au <code>PATH</code> d'ex\u00e9cution du workflow.</p> <p>Par exemple, cr\u00e9ez un fichier nomm\u00e9 <code>fastqc.sh</code> avec le contenu suivant :</p> <pre><code>#!/bin/bash\nset -e\nset -u\n\nsample_id=${1}\nreads=${2}\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n</code></pre> <p>Sauvegardez-le, donnez-lui la permission d'ex\u00e9cuter et placez-le dans le r\u00e9pertoire <code>bin</code> comme indiqu\u00e9 ci-dessous :</p> <pre><code>chmod +x fastqc.sh\nmkdir -p bin\nmv fastqc.sh bin\n</code></pre> <p>Ensuite, ouvrez le fichier <code>script7.nf</code> et remplacez le script du processus <code>FASTQC</code> par le code suivant :</p> <pre><code>script:\n\"\"\"\nfastqc.sh \"$sample_id\" \"$reads\"\n\"\"\"\n</code></pre> <p>Ex\u00e9cutez-la comme auparavant :</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.fr/#resume_5","title":"R\u00e9sum\u00e9","text":"<p>Dans cette \u00e9tape, vous avez appris</p> <ol> <li>Comment \u00e9crire ou utiliser des scripts personnalis\u00e9s existants dans votre workflow Nextflow.</li> <li>Comment \u00e9viter l'utilisation de chemins absolus en ayant vos scripts dans le dossier <code>bin/</code>.</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.fr/#mesures-et-rapports","title":"Mesures et rapports","text":"<p>Nextflow peut produire de multiples rapports et graphiques fournissant plusieurs mesures de temps d'ex\u00e9cution et des informations sur l'ex\u00e9cution.</p> <p>Ex\u00e9cutez le workflow rnaseq-nf pr\u00e9c\u00e9demment introduit comme indiqu\u00e9 ci-dessous :</p> <pre><code>nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png\n</code></pre> <p>L'option <code>-with-docker</code> lance chaque t\u00e2che de l'ex\u00e9cution comme une commande d'ex\u00e9cution d'un conteneur Docker.</p> <p>L'option <code>-with-report</code> permet la cr\u00e9ation du rapport d'ex\u00e9cution du workflow. Ouvrez le fichier <code>report.html</code> avec un navigateur pour voir le rapport cr\u00e9\u00e9 avec la commande ci-dessus.</p> <p>L'option <code>-with-trace</code> permet la cr\u00e9ation d'un fichier de valeurs s\u00e9par\u00e9es par des tabulations (TSV) contenant des informations d'ex\u00e9cution pour chaque t\u00e2che ex\u00e9cut\u00e9e. Consultez le fichier <code>trace.txt</code> pour un exemple.</p> <p>L'option <code>-with-timeline</code> permet la cr\u00e9ation d'un rapport sur la chronologie du flux de travail montrant comment les processus ont \u00e9t\u00e9 ex\u00e9cut\u00e9s au fil du temps. Cela peut \u00eatre utile pour identifier les t\u00e2ches qui prennent le plus de temps et les goulots d'\u00e9tranglement. Voir un exemple \u00e0 ce lien.</p> <p>Enfin, l'option <code>-with-dag</code> permet le rendu de la repr\u00e9sentation graphique acyclique directe de l'ex\u00e9cution du workflow. Note : Cette fonctionnalit\u00e9 n\u00e9cessite l'installation de Graphviz sur votre ordinateur. Voir ici pour plus de d\u00e9tails. Essayez ensuite d'ex\u00e9cuter :</p> <pre><code>open dag.png\n</code></pre> <p>Warning</p> <p>Les mesures de temps d'ex\u00e9cution peuvent \u00eatre incompl\u00e8tes pour les ex\u00e9cutions avec des t\u00e2ches courtes, comme dans le cas de ce tutoriel.</p> <p>Info</p> <p>Vous pouvez visualiser les fichiers HTML en cliquant avec le bouton droit de la souris sur le nom du fichier dans la barre lat\u00e9rale gauche et en choisissant l'option de menu Preview.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#executer-un-projet-a-partir-de-github","title":"Ex\u00e9cuter un projet \u00e0 partir de GitHub","text":"<p>Nextflow permet l'ex\u00e9cution d'un projet de workflow directement \u00e0 partir d'un d\u00e9positoire GitHub (ou de services similaires, par exemple BitBucket et GitLab).</p> <p>Cela simplifie le partage et le d\u00e9ploiement de projets complexes et le suivi des modifications de mani\u00e8re coh\u00e9rente.</p> <p>Le d\u00e9positoire GitHub suivant h\u00e9berge une version compl\u00e8te du flux de travail pr\u00e9sent\u00e9 dans ce tutoriel : https://github.com/nextflow-io/rnaseq-nf</p> <p>Vous pouvez l'ex\u00e9cuter en sp\u00e9cifiant le nom du projet et en lan\u00e7ant chaque t\u00e2che de l'ex\u00e9cution comme une commande run d'un conteneur Docker :</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -with-docker\n</code></pre> <p>Il t\u00e9l\u00e9charge automatiquement le conteneur et le stocke dans le dossier <code>$HOME/.nextflow</code>.</p> <p>Utilisez la commande <code>info</code> pour afficher les informations sur le projet :</p> <pre><code>nextflow info nextflow-io/rnaseq-nf\n</code></pre> <p>Nextflow permet l'ex\u00e9cution d'une r\u00e9vision sp\u00e9cifique de votre projet en utilisant l'option de ligne de commande <code>-r</code>. Par exemple, l'option de ligne de commande <code>-r</code> permet d'ex\u00e9cuter une r\u00e9vision sp\u00e9cifique de votre projet :</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -r v2.1 -with-docker\n</code></pre> <p>Les r\u00e9visions sont d\u00e9finies \u00e0 l'aide de tags Git ou de branches d\u00e9finies dans le r\u00e9f\u00e9rentiel du projet.</p> <p>Les tags permettent un contr\u00f4le pr\u00e9cis des modifications apport\u00e9es \u00e0 vos fichiers de projet et \u00e0 vos d\u00e9pendances au fil du temps.</p>"},{"location":"basic_training/rnaseq_pipeline.fr/#plus-de-ressources","title":"Plus de ressources","text":"<ul> <li>Documentation Nextflow - L'accueil de la documentation Nextflow.</li> <li>Mod\u00e8les Nextflow - Une collection de mod\u00e8les de mise en \u0153uvre de Nextflow.</li> <li>CalliNGS-NF - Un workflow d'appel de variants mettant en \u0153uvre les meilleures pratiques de GATK.</li> <li>nf-core - Une collection communautaire de workflows g\u00e9nomiques pr\u00eats \u00e0 la production.</li> </ul>"},{"location":"basic_training/rnaseq_pipeline/","title":"Simple RNA-Seq workflow","text":"<p>To demonstrate a real-world biomedical scenario, we will implement a proof of concept RNA-Seq workflow which:</p> <ol> <li>Indexes a transcriptome file</li> <li>Performs quality controls</li> <li>Performs quantification</li> <li>Creates a MultiQC report</li> </ol> <p>This will be done using a series of seven scripts, each of which builds on the previous to create a complete workflow. You can find these in the tutorial folder (<code>script1.nf</code> - <code>script7.nf</code>). These scripts will make use of third-party tools that are known by bioinformaticians but that may be new to you so we'll briefly introduce them below.</p> <ol> <li>Salmon is a tool for quantifying molecules known as transcripts through a type of data called RNA-seq data.</li> <li>FastQC is a tool to perform quality control for high throughput sequence data. You can think of it as a way to assess the quality of your data.</li> <li>MultiQC searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarizing the output from numerous bioinformatics tools.</li> </ol> <p>Though these tools may not be the ones you will use in your pipeline, they could just be replaced by any common tool of your area. That's the power of Nextflow!</p>"},{"location":"basic_training/rnaseq_pipeline/#define-the-workflow-parameters","title":"Define the workflow parameters","text":"<p>Parameters are inputs and options that can be changed when the workflow is run.</p> <p>The script <code>script1.nf</code> defines the workflow input parameters.</p> <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nprintln \"reads: $params.reads\"\n</code></pre> <p>Run it by using the following command:</p> <pre><code>nextflow run script1.nf\n</code></pre> <p>Try to specify a different input parameter in your execution command, for example:</p> <pre><code>nextflow run script1.nf --reads '/workspace/gitpod/nf-training/data/ggal/lung_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#exercises","title":"Exercises","text":"<p>Exercise</p> <p>Modify the <code>script1.nf</code> by adding a fourth parameter named <code>outdir</code> and set it to a default path that will be used as the workflow output directory.</p> Solution <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n</code></pre> <p>Exercise</p> <p>Modify <code>script1.nf</code> to print all of the workflow parameters by using a single <code>log.info</code> command as a multiline string statement.</p> <p> See an example here.</p> Solution <p>Add the following to your script file:</p> <pre><code>log.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n.stripIndent(true)\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#summary","title":"Summary","text":"<p>In this step you have learned:</p> <ol> <li>How to define parameters in your workflow script</li> <li>How to pass parameters by using the command line</li> <li>The use of <code>$var</code> and <code>${var}</code> variable placeholders</li> <li>How to use multiline strings</li> <li>How to use <code>log.info</code> to print information and save it in the log execution file</li> </ol>"},{"location":"basic_training/rnaseq_pipeline/#create-a-transcriptome-index-file","title":"Create a transcriptome index file","text":"<p>Nextflow allows the execution of any command or script by using a <code>process</code> definition.</p> <p>A <code>process</code> is defined by providing three main declarations: the process <code>input</code>, <code>output</code> and command <code>script</code>.</p> <p>To add a transcriptome <code>INDEX</code> processing step, try adding the following code blocks to your <code>script1.nf</code>. Alternatively, these code blocks have already been added to <code>script2.nf</code>.</p> <pre><code>/*\n * define the INDEX process that creates a binary index\n * given the transcriptome file\n */\nprocess INDEX {\ninput:\npath transcriptome\noutput:\npath 'salmon_index'\nscript:\n\"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> <p>Additionally, add a workflow scope containing an input channel definition and the index process:</p> <pre><code>workflow {\nindex_ch = INDEX(params.transcriptome_file)\n}\n</code></pre> <p>Here, the <code>params.transcriptome_file</code> parameter is used as the input for the <code>INDEX</code> process. The <code>INDEX</code> process (using the <code>salmon</code> tool) creates <code>salmon_index</code>, an indexed transcriptome that is passed as an output to the <code>index_ch</code> channel.</p> <p>Info</p> <p>The <code>input</code> declaration defines a <code>transcriptome</code> path variable which is used in the <code>script</code> as a reference (using the dollar symbol) in the Salmon command line.</p> <p>Warning</p> <p>Resource requirements such as CPUs and memory limits can change with different workflow executions and platforms. Nextflow can use <code>$task.cpus</code> as a variable for the number of CPUs. See process directives documentation for more details.</p> <p>Run it by using the command:</p> <pre><code>nextflow run script2.nf\n</code></pre> <p>The execution will fail because <code>salmon</code> is not installed in your environment.</p> <p>Add the command line option <code>-with-docker</code> to launch the execution through a Docker container, as shown below:</p> <pre><code>nextflow run script2.nf -with-docker\n</code></pre> <p>This time the execution will work because it uses the Docker container <code>nextflow/rnaseq-nf</code> that is defined in the <code>nextflow.config</code> file in your current directory. If you are running this script locally then you will need to download Docker to your machine, log in and activate Docker, and allow the script to download the container containing the run scripts. You can learn more about Docker here.</p> <p>To avoid adding <code>-with-docker</code> each time you execute the script, add the following line to the <code>nextflow.config</code> file:</p> <pre><code>docker.enabled = true\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#exercises_1","title":"Exercises","text":"<p>Exercise</p> <p>Enable the Docker execution by default by adding the above setting in the <code>nextflow.config</code> file.</p> <p>Exercise</p> <p>Print the output of the <code>index_ch</code> channel by using the view operator.</p> Solution <p>Add the following to the end of your workflow block in your script file</p> <pre><code>index_ch.view()\n</code></pre> <p>Exercise</p> <p>If you have more CPUs available, try changing your script to request more resources for this process. For example, see the directive docs. <code>$task.cpus</code> is already specified in this script, so setting the number of CPUs as a directive will tell Nextflow how to execute this process, in terms of number of CPUs.</p> Solution <p>Add <code>cpus 2</code> to the top of the index process:</p> <pre><code>process INDEX {\ncpus 2\ninput:\n...\n</code></pre> <p>Then check it worked by looking at the script executed in the work directory. Look for the hexadecimal (e.g. <code>work/7f/f285b80022d9f61e82cd7f90436aa4/</code>), Then <code>cat</code> the <code>.command.sh</code> file.</p> <p>Bonus Exercise</p> <p>Use the command <code>tree work</code> to see how Nextflow organizes the process work directory. Check here if you need to download <code>tree</code>.</p> Solution <p>It should look something like this:</p> <pre><code>work\n\u251c\u2500\u2500 17\n\u2502   \u2514\u2500\u2500 263d3517b457de4525513ae5e34ea8\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u251c\u2500\u2500 complete_ref_lens.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctable.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctg_offsets.bin\n\u2502       \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n\u2502       \u2502   \u251c\u2500\u2500 eqtable.bin\n\u2502       \u2502   \u251c\u2500\u2500 info.json\n\u2502       \u2502   \u251c\u2500\u2500 mphf.bin\n\u2502       \u2502   \u251c\u2500\u2500 pos.bin\n\u2502       \u2502   \u251c\u2500\u2500 pre_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 rank.bin\n\u2502       \u2502   \u251c\u2500\u2500 refAccumLengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 ref_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 reflengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 refseq.bin\n\u2502       \u2502   \u251c\u2500\u2500 seq.bin\n\u2502       \u2502   \u2514\u2500\u2500 versionInfo.json\n\u2502       \u2514\u2500\u2500 transcriptome.fa -&gt; /workspace/Gitpod_test/data/ggal/transcriptome.fa\n\u251c\u2500\u2500 7f\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#summary_1","title":"Summary","text":"<p>In this step you have learned:</p> <ol> <li>How to define a process executing a custom command</li> <li>How process inputs are declared</li> <li>How process outputs are declared</li> <li>How to print the content of a channel</li> <li>How to access the number of available CPUs</li> </ol>"},{"location":"basic_training/rnaseq_pipeline/#collect-read-files-by-pairs","title":"Collect read files by pairs","text":"<p>This step shows how to match read files into pairs, so they can be mapped by Salmon.</p> <p>Edit the script <code>script3.nf</code> by adding the following statement as the last line of the file:</p> <pre><code>read_pairs_ch.view()\n</code></pre> <p>Save it and execute it with the following command:</p> <pre><code>nextflow run script3.nf\n</code></pre> <p>It will print something similar to this:</p> <pre><code>[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n</code></pre> <p>The above example shows how the <code>read_pairs_ch</code> channel emits tuples composed of two elements, where the first is the read pair prefix and the second is a list representing the actual files.</p> <p>Try it again specifying different read files by using a glob pattern:</p> <pre><code>nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Warning</p> <p>File paths that include one or more wildcards ie. <code>*</code>, <code>?</code>, etc., MUST be wrapped in single-quoted characters to avoid Bash expanding the glob.</p>"},{"location":"basic_training/rnaseq_pipeline/#exercises_2","title":"Exercises","text":"<p>Exercise</p> <p>Use the set operator in place of <code>=</code> assignment to define the <code>read_pairs_ch</code> channel.</p> Solution <pre><code>Channel\n.fromFilePairs(params.reads)\n.set { read_pairs_ch }\n</code></pre> <p>Exercise</p> <p>Use the <code>checkIfExists</code> option for the fromFilePairs channel factory to check if the specified path contains file pairs.</p> Solution <pre><code>Channel\n.fromFilePairs(params.reads, checkIfExists: true)\n.set { read_pairs_ch }\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#summary_2","title":"Summary","text":"<p>In this step you have learned:</p> <ol> <li>How to use <code>fromFilePairs</code> to handle read pair files</li> <li>How to use the <code>checkIfExists</code> option to check for the existence of input files</li> <li>How to use the <code>set</code> operator to define a new channel variable</li> </ol> <p>Info</p> <p>The declaration of a channel can be before the workflow scope or within it. As long as it is upstream of the process that requires the specific channel.</p>"},{"location":"basic_training/rnaseq_pipeline/#perform-expression-quantification","title":"Perform expression quantification","text":"<p><code>script4.nf</code> adds a gene expression <code>QUANTIFICATION</code> process and a call to it within the workflow scope. Quantification requires the index transcriptome and RNA-Seq read pair fastq files.</p> <p>In the workflow scope, note how the <code>index_ch</code> channel is assigned as output in the <code>INDEX</code> process.</p> <p>Next, note that the first input channel for the <code>QUANTIFICATION</code> process is the previously declared <code>index_ch</code>, which contains the <code>path</code> to the <code>salmon_index</code>.</p> <p>Also, note that the second input channel for the <code>QUANTIFICATION</code> process, is the <code>read_pair_ch</code> we just created. This being a <code>tuple</code> composed of two elements (a value: <code>sample_id</code> and a list of paths to the fastq reads: <code>reads</code>) in order to match the structure of the items emitted by the <code>fromFilePairs</code> channel factory.</p> <p>Execute it by using the following command:</p> <pre><code>nextflow run script4.nf -resume\n</code></pre> <p>You will see the execution of the <code>QUANTIFICATION</code> process.</p> <p>When using the <code>-resume</code> option, any step that has already been processed is skipped.</p> <p>Try to execute the same script again with more read files, as shown below:</p> <pre><code>nextflow run script4.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>You will notice that the <code>QUANTIFICATION</code> process is executed multiple times.</p> <p>Nextflow parallelizes the execution of your workflow simply by providing multiple sets of input data to your script.</p> <p>Tip</p> <p>It may be useful to apply optional settings to a specific process using directives by specifying them in the process body.</p>"},{"location":"basic_training/rnaseq_pipeline/#exercises_3","title":"Exercises","text":"<p>Exercise</p> <p>Add a tag directive to the <code>QUANTIFICATION</code> process to provide a more readable execution log.</p> Solution <p>Add the following before the input declaration:</p> <pre><code>tag \"Salmon on $sample_id\"\n</code></pre> <p>Exercise</p> <p>Add a publishDir directive to the <code>QUANTIFICATION</code> process to store the process results in a directory of your choice.</p> Solution <p>Add the following before the <code>input</code> declaration in the <code>QUANTIFICATION</code> process:</p> <pre><code>publishDir params.outdir, mode: 'copy'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#summary_3","title":"Summary","text":"<p>In this step you have learned:</p> <ol> <li>How to connect two processes together by using the channel declarations</li> <li>How to <code>resume</code> the script execution and skip cached steps</li> <li>How to use the <code>tag</code> directive to provide a more readable execution output</li> <li>How to use the <code>publishDir</code> directive to store a process results in a path of your choice</li> </ol>"},{"location":"basic_training/rnaseq_pipeline/#quality-control","title":"Quality control","text":"<p>Next, we implement a <code>FASTQC</code> quality control step for your input reads (using the label <code>fastqc</code>). The inputs are the same as the read pairs used in the <code>QUANTIFICATION</code> step.</p> <p>You can run it by using the following command:</p> <pre><code>nextflow run script5.nf -resume\n</code></pre> <p>Nextflow DSL2 knows to split the <code>reads_pair_ch</code> into two identical channels as they are required twice as an input for both of the <code>FASTQC</code> and the <code>QUANTIFICATION</code> process.</p>"},{"location":"basic_training/rnaseq_pipeline/#multiqc-report","title":"MultiQC report","text":"<p>This step collects the outputs from the <code>QUANTIFICATION</code> and <code>FASTQC</code> processes to create a final report using the MultiQC tool.</p> <p>Execute the next script with the following command:</p> <pre><code>nextflow run script6.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>It creates the final report in the <code>results</code> folder in the current <code>work</code> directory.</p> <p>In this script, note the use of the mix and collect operators chained together to gather the outputs of the <code>QUANTIFICATION</code> and <code>FASTQC</code> processes as a single input. Operators can be used to combine and transform channels.</p> <pre><code>MULTIQC(quant_ch.mix(fastqc_ch).collect())\n</code></pre> <p>We only want one task of MultiQC to be executed to produce one report. Therefore, we use the <code>mix</code> channel operator to combine the two channels followed by the <code>collect</code> operator, to return the complete channel contents as a single element.</p>"},{"location":"basic_training/rnaseq_pipeline/#summary_4","title":"Summary","text":"<p>In this step you have learned:</p> <ol> <li>How to collect many outputs to a single input with the <code>collect</code> operator</li> <li>How to <code>mix</code> two channels into a single channel</li> <li>How to chain two or more operators together</li> </ol>"},{"location":"basic_training/rnaseq_pipeline/#handle-completion-event","title":"Handle completion event","text":"<p>This step shows how to execute an action when the workflow completes the execution.</p> <p>Note that Nextflow processes define the execution of asynchronous tasks i.e. they are not executed one after another as if they were written in the workflow script in a common imperative programming language.</p> <p>The script uses the <code>workflow.onComplete</code> event handler to print a confirmation message when the script completes.</p> <p>Try to run it by using the following command:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#email-notifications","title":"Email notifications","text":"<p>Send a notification email when the workflow execution completes using the <code>-N &lt;email address&gt;</code> command-line option.</p> <p>Note: this requires the configuration of a SMTP server in the nextflow config file. Below is an example <code>nextflow.config</code> file showing the settings you would have to configure:</p> <pre><code>mail {\nfrom = 'info@nextflow.io'\nsmtp.host = 'email-smtp.eu-west-1.amazonaws.com'\nsmtp.port = 587\nsmtp.user = \"xxxxx\"\nsmtp.password = \"yyyyy\"\nsmtp.auth = true\nsmtp.starttls.enable = true\nsmtp.starttls.required = true\n}\n</code></pre> <p>See mail documentation for details.</p>"},{"location":"basic_training/rnaseq_pipeline/#custom-scripts","title":"Custom scripts","text":"<p>Real-world workflows use a lot of custom user scripts (BASH, R, Python, etc.). Nextflow allows you to consistently use and manage these scripts. Simply put them in a directory named <code>bin</code> in the workflow project root. They will be automatically added to the workflow execution <code>PATH</code>.</p> <p>For example, create a file named <code>fastqc.sh</code> with the following content:</p> <pre><code>#!/bin/bash\nset -e\nset -u\n\nsample_id=${1}\nreads=${2}\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n</code></pre> <p>Save it, give execute permission, and move it into the <code>bin</code> directory as shown below:</p> <pre><code>chmod +x fastqc.sh\nmkdir -p bin\nmv fastqc.sh bin\n</code></pre> <p>Then, open the <code>script7.nf</code> file and replace the <code>FASTQC</code> process\u2019 script with the following code:</p> <pre><code>script:\n\"\"\"\nfastqc.sh \"$sample_id\" \"$reads\"\n\"\"\"\n</code></pre> <p>Run it as before:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline/#summary_5","title":"Summary","text":"<p>In this step you have learned:</p> <ol> <li>How to write or use existing custom scripts in your Nextflow workflow.</li> <li>How to avoid the use of absolute paths by having your scripts in the <code>bin/</code> folder.</li> </ol>"},{"location":"basic_training/rnaseq_pipeline/#metrics-and-reports","title":"Metrics and reports","text":"<p>Nextflow can produce multiple reports and charts providing several runtime metrics and execution information.</p> <p>Run the rnaseq-nf workflow previously introduced as shown below:</p> <pre><code>nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png\n</code></pre> <p>The <code>-with-docker</code> option launches each task of the execution as a Docker container run command.</p> <p>The <code>-with-report</code> option enables the creation of the workflow execution report. Open the file <code>report.html</code> with a browser to see the report created with the above command.</p> <p>The <code>-with-trace</code> option enables the creation of a tab separated value (TSV) file containing runtime information for each executed task. Check the <code>trace.txt</code> for an example.</p> <p>The <code>-with-timeline</code> option enables the creation of the workflow timeline report showing how processes were executed over time. This may be useful to identify the most time consuming tasks and bottlenecks. See an example at this link.</p> <p>Finally, the <code>-with-dag</code> option enables the rendering of the workflow execution direct acyclic graph representation. Note: This feature requires the installation of Graphviz on your computer. See here for further details. Then try running:</p> <pre><code>open dag.png\n</code></pre> <p>Warning</p> <p>Run time metrics may be incomplete for runs with short running tasks, as in the case of this tutorial.</p> <p>Info</p> <p>You view the HTML files by right-clicking on the file name in the left side-bar and choosing the Preview menu item.</p>"},{"location":"basic_training/rnaseq_pipeline/#run-a-project-from-github","title":"Run a project from GitHub","text":"<p>Nextflow allows the execution of a workflow project directly from a GitHub repository (or similar services, e.g., BitBucket and GitLab).</p> <p>This simplifies the sharing and deployment of complex projects and tracking changes in a consistent manner.</p> <p>The following GitHub repository hosts a complete version of the workflow introduced in this tutorial: https://github.com/nextflow-io/rnaseq-nf</p> <p>You can run it by specifying the project name and launching each task of the execution as a Docker container run command:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -with-docker\n</code></pre> <p>It automatically downloads the container and stores it in the <code>$HOME/.nextflow</code> folder.</p> <p>Use the command <code>info</code> to show the project information:</p> <pre><code>nextflow info nextflow-io/rnaseq-nf\n</code></pre> <p>Nextflow allows the execution of a specific revision of your project by using the <code>-r</code> command line option. For example:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -r v2.1 -with-docker\n</code></pre> <p>Revision are defined by using Git tags or branches defined in the project repository.</p> <p>Tags enable precise control of the changes in your project files and dependencies over time.</p>"},{"location":"basic_training/rnaseq_pipeline/#more-resources","title":"More resources","text":"<ul> <li>Nextflow documentation - The Nextflow docs home.</li> <li>Nextflow patterns - A collection of Nextflow implementation patterns.</li> <li>CalliNGS-NF - A Variant calling workflow implementing GATK best practices.</li> <li>nf-core - A community collection of production ready genomic workflows.</li> </ul>"},{"location":"basic_training/rnaseq_pipeline.pt/","title":"Fluxo de trabalho simples de RNA-Seq","text":"<p>Para demonstrar um cen\u00e1rio biom\u00e9dico da vida real, n\u00f3s iremos implementar uma prova de conceito de fluxo de trabalho RNA-Seq que:</p> <ol> <li>Cria arquivo de \u00edndice de transcriptoma</li> <li>Realiza controles de qualidade</li> <li>Realiza quantifica\u00e7\u00e3o</li> <li>Cria um relat\u00f3rio MultiQC</li> </ol> <p>Isso ser\u00e1 feito usando uma s\u00e9rie de sete scripts, cada um se baseando no script anterior, para criar um fluxo de trabalho completo. Voc\u00ea poder\u00e1 encontr\u00e1-los no diret\u00f3rio do tutorial (<code>script1.nf</code> - <code>script7.nf</code>). Esses scripts far\u00e3o uso de ferramentas de terceiros que s\u00e3o conhecidas por bioinformatas, mas que podem ser novas para voc\u00ea, ent\u00e3o vamos apresent\u00e1-las brevemente abaixo.</p> <ol> <li>Salmon \u00e9 uma ferramenta para quantificar mol\u00e9culas conhecidas como transcritos por meio de um tipo de dados chamado dados de RNA-seq.</li> <li>FastQC \u00e9 uma ferramenta para executar o controle de qualidade para dados de sequenciamento de alta vaz\u00e3o. Voc\u00ea pode pensar nisso como uma forma de avaliar a qualidade de seus dados.</li> <li>MultiQC pesquisa um determinado diret\u00f3rio por logs de an\u00e1lises e compila um relat\u00f3rio HTML. \u00c9 uma ferramenta de uso geral, perfeita para resumir a sa\u00edda de v\u00e1rias ferramentas de bioinform\u00e1tica.</li> </ol> <p>Embora essas ferramentas possam n\u00e3o ser as que voc\u00ea usar\u00e1 em seu pipeline, elas podem ser substitu\u00eddas por qualquer ferramenta comum de sua \u00e1rea. Esse \u00e9 o poder do Nextflow!</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#defina-os-parametros-do-fluxo-de-trabalho","title":"Defina os par\u00e2metros do fluxo de trabalho","text":"<p>Par\u00e2metros s\u00e3o entradas e op\u00e7\u00f5es que podem ser modificadas quando um fluxo de trabalho \u00e9 executado.</p> <p>O script <code>script1.nf</code> define os par\u00e2metros de entrada do fluxo de trabalho.</p> <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nprintln \"reads: $params.reads\"\n</code></pre> <p>Execute-o usando o comando a seguir:</p> <pre><code>nextflow run script1.nf\n</code></pre> <p>Tente especificar um par\u00e2metro de entrada diferente no seu comando de execu\u00e7\u00e3o, por exemplo:</p> <pre><code>nextflow run script1.nf --reads '/workspace/gitpod/nf-training/data/ggal/lung_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#exercicios","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Modifique o <code>script1.nf</code> ao adicionar um quarto par\u00e2metro chamado <code>outdir</code> e defina-o como um caminho padr\u00e3o que ser\u00e1 usado como o diret\u00f3rio de sa\u00edda do fluxo de trabalho.</p> Solution <pre><code>params.reads = \"$projectDir/data/ggal/gut_{1,2}.fq\"\nparams.transcriptome_file = \"$projectDir/data/ggal/transcriptome.fa\"\nparams.multiqc = \"$projectDir/multiqc\"\nparams.outdir = \"results\"\n</code></pre> <p>Exercise</p> <p>Modifique o <code>script1.nf</code> para imprimir todos os par\u00e2metros do fluxo de trabalho usando um \u00fanico comando<code>log.info</code> como uma declara\u00e7\u00e3o de string multilinha.</p> <p> Veja um exemplo aqui.</p> Solution <p>Adicione o c\u00f3digo abaixo para seu arquivo de script:</p> <pre><code>log.info \"\"\"\\\n    R N A S E Q - N F   P I P E L I N E\n    ===================================\n    transcriptome: ${params.transcriptome_file}\n    reads        : ${params.reads}\n    outdir       : ${params.outdir}\n    \"\"\"\n.stripIndent(true)\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#resumo","title":"Resumo","text":"<p>Nesta etapa voc\u00ea aprendeu:</p> <ol> <li>Como definir par\u00e2metros em seu script de fluxo de trabalho</li> <li>Como atribuir par\u00e2metros usando a linha de comando</li> <li>O uso de <code>$var</code> e <code>${var}</code> como espa\u00e7o reservado para vari\u00e1veis</li> <li>Como usar strings multilinhas</li> <li>Como usar <code>log.info</code> para imprimir informa\u00e7\u00f5es e salv\u00e1-las no arquivo de execu\u00e7\u00e3o de log</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.pt/#crie-um-arquivo-de-indice-de-transcriptoma","title":"Crie um arquivo de \u00edndice de transcriptoma","text":"<p>O Nextflow permite a execu\u00e7\u00e3o de qualquer comando ou script usando uma defini\u00e7\u00e3o de processo (<code>process</code>).</p> <p>Um processo \u00e9 definido por tr\u00eas principais declara\u00e7\u00f5es: as entradas (<code>input</code>), sa\u00eddas (<code>output</code>) e comandos de <code>script</code> do processo.</p> <p>Para adicionar uma etapa de processamento de \u00edndice do transcriptoma (<code>INDEX</code>), tente adicionar os blocos de c\u00f3digo a seguir no seu <code>script1.nf</code>. Como alternativa, esses blocos de c\u00f3digo j\u00e1 foram adicionados ao <code>script2.nf</code>.</p> <pre><code>/*\n * define o processo INDEX que cria um \u00edndice bin\u00e1rio\n * dado um arquivo de transcriptoma\n */\nprocess INDEX {\ninput:\npath transcriptome\noutput:\npath 'salmon_index'\nscript:\n\"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_index\n    \"\"\"\n}\n</code></pre> <p>Al\u00e9m disso, adicione um escopo <code>workflow</code> contendo uma defini\u00e7\u00e3o de canal de entrada e o processo de \u00edndice:</p> <pre><code>workflow {\nindex_ch = INDEX(params.transcriptome_file)\n}\n</code></pre> <p>Aqui, o par\u00e2metro <code>params.transcriptome_file</code> \u00e9 usado como entrada para o processo <code>INDEX</code>. O processo <code>INDEX</code> (usando a ferramenta <code>salmon</code>) cria um arquivo chamado <code>salmon_index</code>, que \u00e9 um arquivo de \u00edndice de transcriptoma que \u00e9 passado como sa\u00edda ao canal <code>index_ch</code>.</p> <p>Info</p> <p>A declara\u00e7\u00e3o de entrada define a vari\u00e1vel <code>transcriptome</code> com o qualificador <code>path</code> que \u00e9 usada no <code>script</code> como uma refer\u00eancia (usando o s\u00edmbolo de cifr\u00e3o) na linha de comando do Salmon.</p> <p>Warning</p> <p>Os requisitos de recursos, como CPUs e limites de mem\u00f3ria, podem mudar com diferentes execu\u00e7\u00f5es do fluxo de trabalho e plataformas. O Nextflow pode usar <code>$task.cpus</code> como uma vari\u00e1vel para o n\u00famero de CPUs. Veja a documenta\u00e7\u00e3o de diretivas de processo para mais detalhes.</p> <p>Execute-o usando o comando:</p> <pre><code>nextflow run script2.nf\n</code></pre> <p>A execu\u00e7\u00e3o ir\u00e1 falhar porque o <code>salmon</code> n\u00e3o est\u00e1 instalado em seu ambiente.</p> <p>Adicione a op\u00e7\u00e3o de linha de comando <code>-with-docker</code> para iniciar a execu\u00e7\u00e3o atrav\u00e9s do cont\u00eainer Docker, como mostrado abaixo:</p> <pre><code>nextflow run script2.nf -with-docker\n</code></pre> <p>Dessa vez a execu\u00e7\u00e3o vai funcionar porque usa o cont\u00eainer Docker <code>nextflow/rnaseq-nf</code> que \u00e9 definido no arquivo <code>nextflow.config</code> do seu diret\u00f3rio atual. Se voc\u00ea est\u00e1 executando esse script localmente, voc\u00ea precisar\u00e1 baixar o Docker em seu computador, fazer login e ativar o Docker, e permitir que o script baixe o cont\u00eainer contendo os scripts de execu\u00e7\u00e3o. Voc\u00ea pode aprender mais sobre o Docker na documenta\u00e7\u00e3o oficial do Nextflow aqui.</p> <p>Para evitar adicionar <code>-with-docker</code> cada vez que voc\u00ea executar o script, adicione a linha a seguir ao arquivo <code>nextflow.config</code>:</p> <pre><code>docker.enabled = true\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#exercicios_1","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Ative a execu\u00e7\u00e3o do Docker por padr\u00e3o adicionando a configura\u00e7\u00e3o acima no arquivo <code>nextflow.config</code>.</p> <p>Exercise</p> <p>Imprima a sa\u00edda do canal <code>index_ch</code> usando o operador view.</p> Solution <p>Adicione o c\u00f3digo a seguir ao final do bloco <code>workflow</code> em seu arquivo de script</p> <pre><code>index_ch.view()\n</code></pre> <p>Exercise</p> <p>Se voc\u00ea tiver mais CPUs dispon\u00edveis, tente alterar seu script para solicitar mais recursos para este processo. Por exemplo, consulte os documentos de diretivas. <code>$task.cpus</code> j\u00e1 est\u00e1 especificado no script, portanto definir o n\u00famero de CPUs como uma diretiva informar\u00e1 ao Nextflow para executar este processo levando isso em considera\u00e7\u00e3o.</p> Solution <p>Adicione <code>cpus 2</code> no top do processo de \u00edndice:</p> <pre><code>process INDEX {\ncpus 2\ninput:\n...\n</code></pre> <p>Em seguida verifique se funcionou observando o script executado no diret\u00f3rio de trabalho. Procure pelo hexadecimal (por exemplo, <code>work/7f/f285b80022d9f61e82cd7f90436aa4/</code>), depois use <code>cat</code> no arquivo <code>.command.sh</code>.</p> <p>Bonus Exercise</p> <p>Use o comando <code>tree work</code> para observar como o Nextflow organiza o diret\u00f3rio de trabalho do processo. Leia mais aqui se voc\u00ea precisa baixar o <code>tree</code>.</p> Solution <p>Voc\u00ea deve ver algo parecido com a sa\u00edda abaixo:</p> <pre><code>work\n\u251c\u2500\u2500 17\n\u2502   \u2514\u2500\u2500 263d3517b457de4525513ae5e34ea8\n\u2502       \u251c\u2500\u2500 index\n\u2502       \u2502   \u251c\u2500\u2500 complete_ref_lens.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctable.bin\n\u2502       \u2502   \u251c\u2500\u2500 ctg_offsets.bin\n\u2502       \u2502   \u251c\u2500\u2500 duplicate_clusters.tsv\n\u2502       \u2502   \u251c\u2500\u2500 eqtable.bin\n\u2502       \u2502   \u251c\u2500\u2500 info.json\n\u2502       \u2502   \u251c\u2500\u2500 mphf.bin\n\u2502       \u2502   \u251c\u2500\u2500 pos.bin\n\u2502       \u2502   \u251c\u2500\u2500 pre_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 rank.bin\n\u2502       \u2502   \u251c\u2500\u2500 refAccumLengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 ref_indexing.log\n\u2502       \u2502   \u251c\u2500\u2500 reflengths.bin\n\u2502       \u2502   \u251c\u2500\u2500 refseq.bin\n\u2502       \u2502   \u251c\u2500\u2500 seq.bin\n\u2502       \u2502   \u2514\u2500\u2500 versionInfo.json\n\u2502       \u2514\u2500\u2500 transcriptome.fa -&gt; /workspace/Gitpod_test/data/ggal/transcriptome.fa\n\u251c\u2500\u2500 7f\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#resumo_1","title":"Resumo","text":"<p>Nesta etapa voc\u00ea aprendeu:</p> <ol> <li>Como definir um processo executando um comando personalizado</li> <li>Como as entradas do processo s\u00e3o declaradas</li> <li>Como as sa\u00eddas do processo s\u00e3o declaradas</li> <li>Como imprimir o conte\u00fado de um canal</li> <li>Como acessar o n\u00famero de CPUs dispon\u00edveis</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.pt/#colete-arquivos-de-leitura-por-pares","title":"Colete arquivos de leitura por pares","text":"<p>Essa etapa mostra como combinar arquivos de leitura em pares, para que eles possam ser mapeados pelo Salmon.</p> <p>Edite o script <code>script3.nf</code> adicionando a seguinte declara\u00e7\u00e3o como a \u00faltima linha do arquivo:</p> <pre><code>read_pairs_ch.view()\n</code></pre> <p>Salve-o e execute-o com o comando a seguir:</p> <pre><code>nextflow run script3.nf\n</code></pre> <p>Isso ir\u00e1 imprimir algo semelhante a isso:</p> <pre><code>[gut, [/.../data/ggal/gut_1.fq, /.../data/ggal/gut_2.fq]]\n</code></pre> <p>O exemplo acima mostra como o canal <code>read_pairs_ch</code> emite tuplas compostas de dois elementos, onde o primeiro \u00e9 o prefixo do par de leitura e o segundo \u00e9 uma lista que representa os arquivos de fato.</p> <p>Tente novamente especificando arquivos de leituras diferentes usando um padr\u00e3o glob:</p> <pre><code>nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Warning</p> <p>Caminhos de arquivo que incluem um ou mais caracteres especiais, como <code>*</code>, <code>?</code> etc., DEVEM ser colocados entre aspas simples para evitar que o Bash expanda o glob.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#exercicios_2","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Use o operador set no lugar da atribui\u00e7\u00e3o <code>=</code> para definir o canal <code>read_pairs_ch</code>.</p> Solution <pre><code>Channel\n.fromFilePairs(params.reads)\n.set { read_pairs_ch }\n</code></pre> <p>Exercise</p> <p>Use a op\u00e7\u00e3o <code>checkIfExists</code> para a f\u00e1brica de canal fromFilePairs para checar se o caminho especificado cont\u00e9m os pares de arquivos.</p> Solution <pre><code>Channel\n.fromFilePairs(params.reads, checkIfExists: true)\n.set { read_pairs_ch }\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#resumo_2","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como usar <code>fromFilePairs</code> para lidar com pares de arquivos de leituras</li> <li>Como usar a op\u00e7\u00e3o <code>checkIfExists</code> para checar a exist\u00eancia de arquivos de entrada</li> <li>Como usar o operador <code>set</code> para definir uma uma nova vari\u00e1vel de canal</li> </ol> <p>Info</p> <p>A declara\u00e7\u00e3o de um canal pode ser feita antes do escopo <code>workflow</code> ou dentro dele. Desde que a declara\u00e7\u00e3o esteja acima do processo que requer o canal espec\u00edfico.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#realize-a-quantificacao-da-expressao","title":"Realize a quantifica\u00e7\u00e3o da express\u00e3o","text":"<p>O script <code>script4.nf</code> adiciona um processo de quantifica\u00e7\u00e3o de express\u00e3o g\u00eanica (<code>QUANTIFICATION</code>) e uma chamada para esse processo dentro do escopo <code>workflow</code>. A quantifica\u00e7\u00e3o requer o arquivo de \u00edndice de transcriptoma e os arquivos fastq do par de leitura de RNA-Seq.</p> <p>No escopo <code>workflow</code>, observe como o canal <code>index_ch</code> \u00e9 designado como sa\u00edda do processo <code>INDEX</code>.</p> <p>A seguir, note que o primeiro canal de entrada para o processo de <code>QUANTIFICATION</code> \u00e9 o <code>index_ch</code> declarado previamente, que cont\u00e9m o caminho para o arquivo <code>salmon_index</code>.</p> <p>Al\u00e9m disso, observe que o segundo canal de entrada para o processo <code>QUANTIFICATION</code> \u00e9 o <code>read_pair_ch</code> que acabamos de criar. Este sendo uma <code>tupla</code> composta de dois elementos (um valor: <code>sample_id</code> e a lista de caminhos para os arquivos de leituras fastq: <code>reads</code>) para corresponder \u00e0 estrutura dos itens emitidos pela f\u00e1brica de canais <code>fromFilePairs</code>.</p> <p>Execute-o usando o comando a seguir:</p> <pre><code>nextflow run script4.nf -resume\n</code></pre> <p>Voc\u00ea ir\u00e1 ver a execu\u00e7\u00e3o do processo <code>QUANTIFICATION</code>.</p> <p>Ao usar a op\u00e7\u00e3o <code>-resume</code>, qualquer etapa que j\u00e1 foi processada \u00e9 ignorada.</p> <p>Tente executar o mesmo script novamente com mais arquivos de leituras, como mostrado abaixo:</p> <pre><code>nextflow run script4.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Voc\u00ea ir\u00e1 perceber que o processo <code>QUANTIFICATION</code> \u00e9 executado m\u00faltiplas vezes.</p> <p>O Nextflow paraleliza a execu\u00e7\u00e3o de seu fluxo de trabalho simplesmente fornecendo v\u00e1rios conjuntos de dados de entrada para seu script.</p> <p>Tip</p> <p>Pode ser \u00fatil aplicar configura\u00e7\u00f5es opcionais a um processo espec\u00edfico usando diretivas especificando-as no corpo do processo.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#exercicios_3","title":"Exerc\u00edcios","text":"<p>Exercise</p> <p>Adicione uma diretiva de tag, ao processo <code>QUANTIFICATION</code> para fornecer um log de execu\u00e7\u00e3o mais leg\u00edvel.</p> Solution <p>Adicione o c\u00f3digo a seguir antes da declara\u00e7\u00e3o de entrada:</p> <pre><code>tag \"Salmon on $sample_id\"\n</code></pre> <p>Exercise</p> <p>Adicione a diretiva publishDir para o processo <code>QUANTIFICATION</code> para armazenar os resultados do processo em um diret\u00f3rio de sua escolha.</p> Solution <p>Adicione o c\u00f3digo a seguir antes da declara\u00e7\u00e3o de entrada no processo de <code>QUANTIFICATION</code>:</p> <pre><code>publishDir params.outdir, mode: 'copy'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#resumo_3","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como conectar dois processos juntos usando declara\u00e7\u00f5es de canal</li> <li>Como retomar a execu\u00e7\u00e3o do script e pular etapas em cache</li> <li>Como usar a diretiva <code>tag</code> para fornecer uma sa\u00edda de execu\u00e7\u00e3o mais leg\u00edvel</li> <li>Como usar a diretiva <code>publishDir</code> para armazenar os resultados do processo em um caminho da sua escolha</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.pt/#controle-de-qualidade","title":"Controle de qualidade","text":"<p>A seguir, n\u00f3s implementamos uma etapa de controle de qualidade <code>FASTQC</code> para seus arquivos de leituras de entrada (usando a etiqueta <code>fastqc</code>). As entradas s\u00e3o as mesmas que os pares de arquivos de leituras na etapa <code>QUANTIFICATION</code>.</p> <p>Voc\u00ea pode execut\u00e1-lo usando o comando a seguir:</p> <pre><code>nextflow run script5.nf -resume\n</code></pre> <p>O Nextflow DSL2 sabe como dividir <code>reads_pair_ch</code> em dois canais id\u00eanticos, j\u00e1 que eles s\u00e3o requiridos duas vezes como entrada para os processos <code>FASTQC</code> e <code>QUANTIFICATION</code>.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#relatorio-multiqc","title":"Relat\u00f3rio MultiQC","text":"<p>Essa etapa coleta as sa\u00eddas dos processos <code>QUANTIFICATION</code> e <code>FASTQC</code> para criar um relat\u00f3rio final usando a ferramenta MultiQC.</p> <p>Execute o pr\u00f3ximo script com o comando a seguir:</p> <pre><code>nextflow run script6.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre> <p>Isso cria um relat\u00f3rio final na pasta <code>results</code> no diret\u00f3rio de trabalho atual.</p> <p>Neste script, observe o uso dos operadores mix e collect de forma encadeada para reunir as sa\u00eddas dos processos <code>QUANTIFICATION</code> e <code>FASTQC</code> como uma \u00fanica entrada. Operadores podem ser usados para combinar e transformar canais.</p> <pre><code>MULTIQC(quant_ch.mix(fastqc_ch).collect())\n</code></pre> <p>Queremos que apenas uma tarefa do MultiQC seja executada para produzir um relat\u00f3rio. Portanto, usamos o operador de canal <code>mix</code> para combinar os dois canais, seguido pelo operador <code>collect</code> para retornar os conte\u00fados completos do canal como um \u00fanico elemento.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#resumo_4","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como coletar v\u00e1rias sa\u00eddas para uma \u00fanica entrada com o operador <code>collect</code></li> <li>Como combinar com <code>mix</code> dois canais em um \u00fanico canal</li> <li>Como encadear dois ou mais operadores juntos</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.pt/#lide-com-evento-de-conclusao","title":"Lide com evento de conclus\u00e3o","text":"<p>Essa etapa mostra como executar uma a\u00e7\u00e3o quando o fluxo de trabalho completa a execu\u00e7\u00e3o.</p> <p>Observe que processos do Nextflow definem a execu\u00e7\u00e3o de tarefas ass\u00edncronas, ou seja, elas n\u00e3o s\u00e3o executadas uma ap\u00f3s a outra como se elas fossem escritas no script do fluxo de trabalho em uma linguagem de programa\u00e7\u00e3o imperativa comum.</p> <p>O script usa o manipulador de evento <code>workflow.onComplete</code> para imprimir uma mensagem de confirma\u00e7\u00e3o quando o script for conclu\u00eddo.</p> <p>Tente execut\u00e1-lo usando o comando a seguir:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#notificacoes-por-email","title":"Notifica\u00e7\u00f5es por email","text":"<p>Envie uma notifica\u00e7\u00e3o por email quando a execu\u00e7\u00e3o do fluxo de trabalho for conclu\u00edda usando a op\u00e7\u00e3o de linha de comando <code>-N &lt;endere\u00e7o de email&gt;</code>.</p> <p>Nota: isso requer a configura\u00e7\u00e3o de um servidor SMTP no arquivo de configura\u00e7\u00e3o do Nextflow. Abaixo h\u00e1 um exemplo de um arquivo <code>nextflow.config</code> mostrando as configura\u00e7\u00f5es que voc\u00ea teria que configurar:</p> <pre><code>mail {\nfrom = 'info@nextflow.io'\nsmtp.host = 'email-smtp.eu-west-1.amazonaws.com'\nsmtp.port = 587\nsmtp.user = \"xxxxx\"\nsmtp.password = \"yyyyy\"\nsmtp.auth = true\nsmtp.starttls.enable = true\nsmtp.starttls.required = true\n}\n</code></pre> <p>Veja a documenta\u00e7\u00e3o de email para mais detalhes.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#scripts-personalizados","title":"Scripts personalizados","text":"<p>Os fluxos de trabalho do mundo real usam muitos scripts de usu\u00e1rio personalizados (BASH, R, Python, etc.). O Nextflow permite que voc\u00ea use e gerencie consistentemente esses scripts. Simplesmente os coloque em um diret\u00f3rio chamado <code>bin</code> na raiz do projeto do fluxo de trabalho. Eles ser\u00e3o automaticamente adicionados para o <code>PATH</code> da execu\u00e7\u00e3o do fluxo de trabalho.</p> <p>Por exemplo, crie um arquivo chamado de <code>fastqc.sh</code> com o conte\u00fado a seguir:</p> <pre><code>#!/bin/bash\nset -e\nset -u\n\nsample_id=${1}\nreads=${2}\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n</code></pre> <p>Salve-o, d\u00ea permiss\u00e3o de execu\u00e7\u00e3o e mova-o para o diret\u00f3rio <code>bin</code> conforme mostrado abaixo:</p> <pre><code>chmod +x fastqc.sh\nmkdir -p bin\nmv fastqc.sh bin\n</code></pre> <p>Ent\u00e3o, abra o arquivo <code>script7.nf</code> e substitua o script do processo <code>FASTQC</code> com o c\u00f3digo a seguir:</p> <pre><code>script:\n\"\"\"\nfastqc.sh \"$sample_id\" \"$reads\"\n\"\"\"\n</code></pre> <p>Execute-o como anteriormente:</p> <pre><code>nextflow run script7.nf -resume --reads 'data/ggal/*_{1,2}.fq'\n</code></pre>"},{"location":"basic_training/rnaseq_pipeline.pt/#resumo_5","title":"Resumo","text":"<p>Nessa etapa voc\u00ea aprendeu:</p> <ol> <li>Como escrever ou usar scripts personalizados existentes em seu fluxo de trabalho do Nextflow.</li> <li>Como evitar o uso de caminhos absolutos tendo seus scripts na pasta <code>bin/</code>.</li> </ol>"},{"location":"basic_training/rnaseq_pipeline.pt/#metricas-e-relatorios","title":"M\u00e9tricas e relat\u00f3rios","text":"<p>O Nextflow pode produzir v\u00e1rios relat\u00f3rios e gr\u00e1ficos fornecendo v\u00e1rias m\u00e9tricas de tempo de execu\u00e7\u00e3o e informa\u00e7\u00f5es de execu\u00e7\u00e3o.</p> <p>Execute o fluxo de trabalho rnaseq-nf introduzido anteriormente, conforme mostrado abaixo:</p> <pre><code>nextflow run rnaseq-nf -with-docker -with-report -with-trace -with-timeline -with-dag dag.png\n</code></pre> <p>A op\u00e7\u00e3o <code>-with-docker</code> inicia cada tarefa da execu\u00e7\u00e3o como um comando de execu\u00e7\u00e3o de cont\u00eainer do Docker.</p> <p>A op\u00e7\u00e3o <code>-with-report</code> permite a cria\u00e7\u00e3o do relat\u00f3rio de execu\u00e7\u00e3o do fluxo de trabalho. Abra o arquivo <code>report.html</code> com um navegador para ver o relat\u00f3rio criado com o comando acima.</p> <p>A op\u00e7\u00e3o <code>-with-trace</code> permite a cria\u00e7\u00e3o de um arquivo separado por tabula\u00e7\u00f5es (TSV) contendo informa\u00e7\u00f5es de tempo de execu\u00e7\u00e3o para cada tarefa executada. Verifique o <code>trace.txt</code> para um exemplo.</p> <p>A op\u00e7\u00e3o <code>-with-timeline</code> permite a cria\u00e7\u00e3o do relat\u00f3rio da linha do tempo do fluxo de trabalho mostrando como os processos foram executados ao longo do tempo. Isso pode ser \u00fatil para identificar as tarefas e gargalos que consomem mais tempo. Veja um exemplo neste link.</p> <p>Por fim, a op\u00e7\u00e3o <code>-with-dag</code> permite a renderiza\u00e7\u00e3o da representa\u00e7\u00e3o de grafo ac\u00edclico direcionado da execu\u00e7\u00e3o do fluxo de trabalho. Nota: Este recurso requer a instala\u00e7\u00e3o do Graphviz em seu computador. Veja aqui para mais detalhes. Ent\u00e3o tente executar:</p> <pre><code>open dag.png\n</code></pre> <p>Warning</p> <p>As m\u00e9tricas de tempo de execu\u00e7\u00e3o podem estar incompletas para execu\u00e7\u00f5es com tarefas com curto tempo de execu\u00e7\u00e3o, como no caso deste tutorial.</p> <p>Info</p> <p>Voc\u00ea visualiza os arquivos HTML clicando com o bot\u00e3o direito do mouse no nome do arquivo na barra lateral esquerda e escolhendo o item de menu Show Preview.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#execute-um-projeto-do-github","title":"Execute um projeto do GitHub","text":"<p>O Nextflow permite a execu\u00e7\u00e3o de um projeto de fluxo de trabalho diretamente de um reposit\u00f3rio do GitHub (ou servi\u00e7os semelhantes, por exemplo, BitBucket e GitLab).</p> <p>Isso simplifica o compartilhamento e implanta\u00e7\u00e3o de projetos complexos e o rastreamento de mudan\u00e7as de uma maneira consistente.</p> <p>O reposit\u00f3rio do GitHub a seguir hospeda uma vers\u00e3o completa do fluxo de trabalho apresentado neste tutorial: https://github.com/nextflow-io/rnaseq-nf</p> <p>Voc\u00ea pode execut\u00e1-lo especificando o nome do projeto e com isso iniciar a execu\u00e7\u00e3o de cada tarefa como um comando de execu\u00e7\u00e3o de cont\u00eainer do Docker:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -with-docker\n</code></pre> <p>Ele baixa automaticamente o cont\u00eainer e o armazena na pasta <code>$HOME/.nextflow</code>.</p> <p>Use o comando <code>info</code> para mostrar as informa\u00e7\u00f5es do projeto:</p> <pre><code>nextflow info nextflow-io/rnaseq-nf\n</code></pre> <p>O Nextflow permite a execu\u00e7\u00e3o de uma revis\u00e3o espec\u00edfica do seu projeto usando a op\u00e7\u00e3o de linha de comando <code>-r</code>. Por exemplo:</p> <pre><code>nextflow run nextflow-io/rnaseq-nf -r v2.1 -with-docker\n</code></pre> <p>As revis\u00f5es s\u00e3o definidas usando etiquetas do Git ou ramos definidos no reposit\u00f3rio do projeto.</p> <p>As etiquetas permitem o controle preciso das altera\u00e7\u00f5es nos arquivos e depend\u00eancias do projeto ao longo do tempo.</p>"},{"location":"basic_training/rnaseq_pipeline.pt/#mais-recursos","title":"Mais recursos","text":"<ul> <li>Documenta\u00e7\u00e3o do Nextflow - A p\u00e1gina inicial dos documentos do Nextflow.</li> <li>Nextflow patterns - Uma cole\u00e7\u00e3o de padr\u00f5es de implementa\u00e7\u00e3o do Nextflow.</li> <li>CalliNGS-NF - Um fluxo de trabalho de chamada de variante implementando as melhores pr\u00e1ticas recomendadas do GATK.</li> <li>nf-core - Uma cole\u00e7\u00e3o comunit\u00e1ria de fluxos de trabalho gen\u00f4micos prontos para produ\u00e7\u00e3o.</li> </ul>"},{"location":"basic_training/setup.fr/","title":"Installation de l'environnement","text":"<p>Il y a deux fa\u00e7ons principales de commencer avec le cours de formation communautaire de Nextflow.</p> <p>La premi\u00e8re consiste \u00e0 installer les exigences localement, ce qui est pr\u00e9f\u00e9rable si vous \u00eates d\u00e9j\u00e0 familier avec Git et Docker, ou si vous travaillez hors ligne.</p> <p>La seconde consiste \u00e0 utiliser Gitpod, ce qui est pr\u00e9f\u00e9rable pour les d\u00e9butants car cette plateforme contient tous les programmes et donn\u00e9es n\u00e9cessaires. Il suffit de cliquer sur le lien et de se connecter \u00e0 l'aide de son compte GitHub pour commencer le tutoriel :</p> <p></p>"},{"location":"basic_training/setup.fr/#installation-locale","title":"Installation locale","text":"<p>Nextflow peut \u00eatre utilis\u00e9 sur n'importe quel syst\u00e8me compatible POSIX (Linux, macOS, Windows Subsystem for Linux, etc.).</p>"},{"location":"basic_training/setup.fr/#exigences","title":"Exigences","text":"<ul> <li>Bash</li> <li>Java 11 (or later, up to 18)</li> <li>Git</li> <li>Docker</li> </ul>"},{"location":"basic_training/setup.fr/#exigences-optionnelles-pour-ce-tutoriel","title":"Exigences optionnelles pour ce tutoriel","text":"<ul> <li>Singularity 2.5.x (ou plus)</li> <li>Conda 4.5 (ou plus)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>Un environnement AWS Batch configur\u00e9</li> </ul>"},{"location":"basic_training/setup.fr/#telecharger-nextflow","title":"T\u00e9l\u00e9charger Nextflow","text":"<p>Entrez cette commande dans votre terminal :</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Ou, vous pr\u00e9f\u00e9rez <code>curl</code>:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Then ensure that the downloaded binary is executable:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>Et mettez l'ex\u00e9cutable <code>nextflow</code> dans votre <code>$PATH</code> (par exemple <code>/usr/local/bin</code> ou <code>/bin/</code>)</p>"},{"location":"basic_training/setup.fr/#docker","title":"Docker","text":"<p>Assurez-vous que Docker Desktop fonctionne sur votre machine. T\u00e9l\u00e9chargez Docker ici.</p>"},{"location":"basic_training/setup.fr/#materiel-de-formation","title":"Mat\u00e9riel de formation","text":"<p>Vous pouvez consulter le mat\u00e9riel de formation ici : https://training.nextflow.io/</p> <p>Pour t\u00e9l\u00e9charger le mat\u00e9riel, utilisez la commande :</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Puis <code>cd</code> dans le r\u00e9pertoire <code>nf-training</code>.</p>"},{"location":"basic_training/setup.fr/#verifier-votre-installation","title":"V\u00e9rifier votre installation","text":"<p>V\u00e9rifiez l'installation correcte de <code>nextflow</code> en ex\u00e9cutant la commande suivante :</p> <pre><code>nextflow info\n</code></pre> <p>Cela devrait indiquer la version actuelle, le syst\u00e8me et la dur\u00e9e d'ex\u00e9cution.</p>"},{"location":"basic_training/setup.fr/#gitpod","title":"Gitpod","text":"<p>Un environnement de d\u00e9veloppement Nextflow pr\u00e9configur\u00e9 est disponible via Gitpod.</p>"},{"location":"basic_training/setup.fr/#exigences_1","title":"Exigences","text":"<ul> <li>Un compte GitHub</li> <li>Navigateur web (Google Chrome, Firefox)</li> <li>Une connexion internet</li> </ul>"},{"location":"basic_training/setup.fr/#demarrage-rapide-de-gitpod","title":"D\u00e9marrage rapide de Gitpod","text":"<p>Pour ex\u00e9cuter Gitpod :</p> <ul> <li>Cliquez sur l'URL suivante : https://gitpod.io/#https://github.com/nextflow-io/training<ul> <li>Il s'agit de l'URL de notre repositoire GitHub, pr\u00e9fix\u00e9e par <code>https://gitpod.io/#</code>.</li> </ul> </li> <li>Connectez-vous \u00e0 votre compte GitHub (et autorisez l'acc\u00e8s).</li> </ul> <p>Une fois que vous vous \u00eates connect\u00e9, Gitpod devrait se charger (sautez le prebuild si on vous le demande).</p>"},{"location":"basic_training/setup.fr/#explorez-votre-ide-gitpod","title":"Explorez votre IDE Gitpod","text":"<p>Vous devriez maintenant voir quelque chose de similaire \u00e0 ce qui suit :</p> <p></p> <ul> <li>La barre lat\u00e9rale vous permet de personnaliser votre environnement Gitpod et d'effectuer des t\u00e2ches de base (copier, coller, ouvrir des fichiers, rechercher, git, etc.) Cliquez sur le bouton Explorer pour voir quels fichiers se trouvent dans ce d\u00e9p\u00f4t.</li> <li>Le terminal vous permet d'ex\u00e9cuter tous les programmes du repositoire. Par exemple, <code>nextflow</code> et <code>docker</code> sont install\u00e9s et peuvent \u00eatre ex\u00e9cut\u00e9s</li> <li>La fen\u00eatre principale vous permet de visualiser et d'\u00e9diter des fichiers. En cliquant sur un fichier dans l'explorateur, vous l'ouvrez dans la fen\u00eatre principale. Vous devriez \u00e9galement voir le materiel du navigateur de formation nf-training (&lt;https://training.nextflow.io</li> </ul> <p>Pour v\u00e9rifier que l'environnement fonctionne correctement, tapez ce qui suit dans le terminal :</p> <pre><code>nextflow info\n</code></pre> <p>Vous devriez obtenir la version de Nextflow et des informations sur la dur\u00e9e d'ex\u00e9cution :</p> <pre><code>Version: 22.10.4 build 5836\nCreated: 09-12-2022 09:58 UTC\nSystem: Linux 5.15.0-47-generic\nRuntime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 17.0.3-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"basic_training/setup.fr/#resources-de-gitpod","title":"Resources de Gitpod","text":"<ul> <li>Gitpod vous offre 500 cr\u00e9dits gratuits par mois, ce qui \u00e9quivaut \u00e0 50 heures d'utilisation gratuite de l'environnement en utilisant l'espace de travail standard (jusqu'\u00e0 4 c\u0153urs, 8 Go de RAM et 30 Go d'espace de stockage).</li> <li>Il existe \u00e9galement une option de grand espace de travail qui vous permet d'utiliser jusqu'\u00e0 8 c\u0153urs, 16 Go de RAM et 50 Go d'espace de stockage. Cependant, le grand espace de travail utilisera vos cr\u00e9dits gratuits plus rapidement et vous aurez moins d'heures d'acc\u00e8s \u00e0 cet espace.</li> <li>Gitpod s'arr\u00eate au bout de 30 minutes d'inactivit\u00e9 et conserve les modifications jusqu'\u00e0 deux semaines (voir la section suivante pour r\u00e9ouvrir une session interrompue).</li> </ul> <p>Voir gitpod.io pour plus de details.</p>"},{"location":"basic_training/setup.fr/#reouvrir-une-session-gitpod","title":"R\u00e9ouvrir une session Gitpod","text":"<p>Vous pouvez reouvrir un environnement \u00e0 partir de https://gitpod.io/workspaces. Recherchez votre ancien environnement dans la liste, puis s\u00e9lectionnez l'ellipse (ic\u00f4ne \u00e0 trois points) et s\u00e9lectionnez Ouvrir.</p> <p>Si vous avez sauvegard\u00e9 l'URL de votre pr\u00e9c\u00e9dent environnement Gitpod, vous pouvez simplement l'ouvrir dans votre navigateur.</p> <p>Vous pouvez \u00e9galement d\u00e9marrer un nouvel espace de travail en suivant l'URL de Gitpod : https://gitpod.io/#https://github.com/nextflow-io/training</p> <p>Si vous avez perdu votre environnement, vous pouvez trouver les principaux scripts utilis\u00e9s dans ce tutoriel dans le r\u00e9pertoire <code>nf-training</code>.</p>"},{"location":"basic_training/setup.fr/#sauvegarde-des-fichiers-de-gitpod-sur-votre-machine-locale","title":"Sauvegarde des fichiers de Gitpod sur votre machine locale","text":"<p>Pour enregistrer un fichier \u00e0 partir du panneau de l'explorateur, cliquez avec le bouton droit de la souris sur le fichier et s\u00e9lectionnez T\u00e9l\u00e9charger.</p>"},{"location":"basic_training/setup.fr/#materiel-de-formation_1","title":"Mat\u00e9riel de formation","text":"<p>Le cours de formation est accessible dans votre navigateur \u00e0 l'adresse suivante : https://training.nextflow.io/</p>"},{"location":"basic_training/setup.fr/#selection-dune-version-de-nextflow","title":"S\u00e9lection d'une version de Nextflow","text":"<p>Par d\u00e9faut, Nextflow int\u00e8gre la derni\u00e8re version stable dans votre environnement.</p> <p>Cependant, Nextflow est en constante \u00e9volution car nous apportons des am\u00e9liorations et corrigeons des bugs.</p> <p>Les derni\u00e8res versions peuvent \u00eatre consult\u00e9es sur GitHub ici.</p> <p>Si vous souhaitez utiliser une version sp\u00e9cifique de Nextflow, vous pouvez d\u00e9finir la variable <code>NXF_VER</code> comme indiqu\u00e9 ci-dessous :</p> <pre><code>export NXF_VER=23.04.1\n</code></pre> <p>Remarque</p> <p>Cet atelier tutoriel n\u00e9cessite <code>NXF_VER=23.04.1</code>, ou une version plus r\u00e9cente. Cette version utilise DSL2 par d\u00e9faut.</p> <p>Ex\u00e9cutez <code>nextflow -version</code> \u00e0 nouveau pour confirmer que le changement a pris effet.</p>"},{"location":"basic_training/setup/","title":"Environment setup","text":"<p>There are two main ways to get started with Nextflow's community training course.</p> <p>The first is to install the requirements locally, which is best if you are already familiar with Git and Docker, or working offline.</p> <p>The second is to use Gitpod, which is best for first-timers as this platform contains all the programs and data required. Simply click the link and log in using your GitHub account to start the tutorial:</p> <p></p>"},{"location":"basic_training/setup/#local-installation","title":"Local installation","text":"<p>Nextflow can be used on any POSIX-compatible system (Linux, macOS, Windows Subsystem for Linux, etc.).</p>"},{"location":"basic_training/setup/#requirements","title":"Requirements","text":"<ul> <li>Bash</li> <li>Java 11 (or later, up to 18)</li> <li>Git</li> <li>Docker</li> </ul>"},{"location":"basic_training/setup/#optional-requirements-for-this-tutorial","title":"Optional requirements for this tutorial","text":"<ul> <li>Singularity 2.5.x (or later)</li> <li>Conda 4.5 (or later)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>A configured AWS Batch computing environment</li> </ul>"},{"location":"basic_training/setup/#download-nextflow","title":"Download Nextflow","text":"<p>Enter this command in your terminal:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Or, if you prefer <code>curl</code>:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Then ensure that the downloaded binary is executable:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>And put the <code>nextflow</code> executable into your <code>$PATH</code> (e.g. <code>/usr/local/bin</code> or <code>/bin/</code>)</p>"},{"location":"basic_training/setup/#docker","title":"Docker","text":"<p>Ensure you have Docker Desktop running on your machine. Download Docker here.</p>"},{"location":"basic_training/setup/#training-material","title":"Training material","text":"<p>You can view the training material here: https://training.nextflow.io/</p> <p>To download the material use the command:</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Then <code>cd</code> into the <code>nf-training</code> directory.</p>"},{"location":"basic_training/setup/#checking-your-installation","title":"Checking your installation","text":"<p>Check the correct installation of <code>nextflow</code> by running the following command:</p> <pre><code>nextflow info\n</code></pre> <p>This should show the current version, system and runtime.</p>"},{"location":"basic_training/setup/#gitpod","title":"Gitpod","text":"<p>A preconfigured Nextflow development environment is available using Gitpod.</p>"},{"location":"basic_training/setup/#requirements_1","title":"Requirements","text":"<ul> <li>A GitHub account</li> <li>Web browser (Google Chrome, Firefox)</li> <li>Internet connection</li> </ul>"},{"location":"basic_training/setup/#gitpod-quick-start","title":"Gitpod quick start","text":"<p>To run Gitpod:</p> <ul> <li>Click the following URL: https://gitpod.io/#https://github.com/nextflow-io/training<ul> <li>This is our GitHub repository URL, prefixed with <code>https://gitpod.io/#</code></li> </ul> </li> <li>Log in to your GitHub account (and allow authorization).</li> </ul> <p>Once you have signed in, Gitpod should load (skip prebuild if asked).</p>"},{"location":"basic_training/setup/#explore-your-gitpod-ide","title":"Explore your Gitpod IDE","text":"<p>You should now see something similar to the following:</p> <p></p> <ul> <li>The sidebar allows you to customize your Gitpod environment and perform basic tasks (copy, paste, open files, search, git, etc.). Click the Explorer button to see which files are in this repository.</li> <li>The terminal allows you to run all the programs in the repository. For example, both <code>nextflow</code> and <code>docker</code> are installed and can be executed.</li> <li>The main window allows you to view and edit files. Clicking on a file in the explorer will open it within the main window. You should also see the nf-training material browser (https://training.nextflow.io/).</li> </ul> <p>To test that the environment is working correctly, type the following into the terminal:</p> <pre><code>nextflow info\n</code></pre> <p>This should come up with the Nextflow version and runtime information:</p> <pre><code>Version: 22.10.4 build 5836\nCreated: 09-12-2022 09:58 UTC\nSystem: Linux 5.15.0-47-generic\nRuntime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 17.0.3-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"basic_training/setup/#gitpod-resources","title":"Gitpod resources","text":"<ul> <li>Gitpod gives you 500 free credits per month, which is equivalent to 50 hours of free environment runtime using the standard workspace (up to 4 cores, 8 GB RAM and 30 GB storage).</li> <li>There is also a large workspace option that gives you up to 8 cores, 16GB RAM, and 50GB storage. However, the large workspace will use your free credits faster and you will have fewer hours of access to this space.</li> <li>Gitpod will time out after 30 minutes of inactivity and will save your changes for up to 2 weeks (see the next section for reopening a timed-out session).</li> </ul> <p>See gitpod.io for more details.</p>"},{"location":"basic_training/setup/#reopening-a-gitpod-session","title":"Reopening a Gitpod session","text":"<p>You can reopen an environment from https://gitpod.io/workspaces. Find your previous environment in the list, then select the ellipsis (three dots icon) and select Open.</p> <p>If you have saved the URL for your previous Gitpod environment, you can simply open it in your browser.</p> <p>Alternatively, you can start a new workspace by following the Gitpod URL: https://gitpod.io/#https://github.com/nextflow-io/training</p> <p>If you have lost your environment, you can find the main scripts used in this tutorial in the <code>nf-training</code> directory.</p>"},{"location":"basic_training/setup/#saving-files-from-gitpod-to-your-local-machine","title":"Saving files from Gitpod to your local machine","text":"<p>To save any file from the explorer panel, right-click the file and select Download.</p>"},{"location":"basic_training/setup/#training-material_1","title":"Training material","text":"<p>The training course can be accessed in your browser from https://training.nextflow.io/</p>"},{"location":"basic_training/setup/#selecting-a-nextflow-version","title":"Selecting a Nextflow version","text":"<p>By default, Nextflow will pull the latest stable version into your environment.</p> <p>However, Nextflow is constantly evolving as we make improvements and fix bugs.</p> <p>The latest releases can be viewed on GitHub here.</p> <p>If you want to use a specific version of Nextflow, you can set the <code>NXF_VER</code> variable as shown below:</p> <pre><code>export NXF_VER=23.04.1\n</code></pre> <p>Note</p> <p>This tutorial workshop requires <code>NXF_VER=23.04.1</code>, or later. This version will use DSL2 as default.</p> <p>Run <code>nextflow -version</code> again to confirm that the change has taken effect.</p>"},{"location":"basic_training/setup.pt/","title":"Configura\u00e7\u00e3o do ambiente","text":"<p>Existem duas principais maneiras de come\u00e7ar este treinamento da Comunidade do Nextflow.</p> <p>A primeira \u00e9 instalar os requisitos localmente, o que \u00e9 melhor se voc\u00ea j\u00e1 estiver familiarizado com Git e Docker ou trabalhando offline.</p> <p>A segunda \u00e9 usar o Gitpod, o que \u00e9 melhor para iniciantes, pois esta plataforma cont\u00e9m todos os programas e dados necess\u00e1rios. Basta clicar no link abaixo e fazer login usando sua conta do GitHub para iniciar o tutorial:</p> <p></p>"},{"location":"basic_training/setup.pt/#instalacao-local","title":"Instala\u00e7\u00e3o local","text":"<p>O Nextflow pode ser usado em qualquer sistema compat\u00edvel com POSIX (Linux, macOS, Windows Subsystem for Linux, etc.).</p>"},{"location":"basic_training/setup.pt/#requisitos","title":"Requisitos","text":"<ul> <li>Bash</li> <li>Java 11 (ou uma vers\u00e3o posterior, at\u00e9 a 18)</li> <li>Git</li> <li>Docker</li> </ul>"},{"location":"basic_training/setup.pt/#requisitos-opcionais-para-este-tutorial","title":"Requisitos opcionais para este tutorial","text":"<ul> <li>Singularity 2.5.x (ou uma vers\u00e3o posterior)</li> <li>Conda 4.5 (ou uma vers\u00e3o posterior)</li> <li>Graphviz</li> <li>AWS CLI</li> <li>Um ambiente de computa\u00e7\u00e3o do AWS Batch configurado</li> </ul>"},{"location":"basic_training/setup.pt/#baixe-o-nextflow","title":"Baixe o Nextflow","text":"<p>Digite este comando no seu terminal:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\n</code></pre> <p>Ou, se preferir o <code>curl</code>:</p> <pre><code>curl -s https://get.nextflow.io | bash\n</code></pre> <p>Em seguida, certifique-se que o bin\u00e1rio baixado \u00e9 execut\u00e1vel:</p> <pre><code>chmod +x nextflow\n</code></pre> <p>E coloque o execut\u00e1vel <code>nextflow</code> em seu <code>$PATH</code> (por exemplo, <code>/usr/local/bin</code> ou <code>/bin/</code>)</p>"},{"location":"basic_training/setup.pt/#docker","title":"Docker","text":"<p>Certifique-se de ter o Docker Desktop em execu\u00e7\u00e3o em sua m\u00e1quina. Baixe o Docker aqui.</p>"},{"location":"basic_training/setup.pt/#material-de-treinamento","title":"Material de treinamento","text":"<p>Voc\u00ea pode ver o material de treinamento aqui: https://training.nextflow.io/</p> <p>Para baixar o material use o comando:</p> <pre><code>git clone https://github.com/nextflow-io/training.git\n</code></pre> <p>Em seguida, <code>cd</code> no diret\u00f3rio <code>nf-training</code>.</p>"},{"location":"basic_training/setup.pt/#verificando-sua-instalacao","title":"Verificando sua instala\u00e7\u00e3o","text":"<p>Verifique se o <code>nextflow</code> foi instalado corretamente executando o seguinte comando:</p> <pre><code>nextflow info\n</code></pre> <p>Isso deve mostrar a vers\u00e3o atual, sistema operacional e sistema de tempo de execu\u00e7\u00e3o.</p>"},{"location":"basic_training/setup.pt/#gitpod","title":"Gitpod","text":"<p>Um ambiente de desenvolvimento Nextflow pr\u00e9-configurado est\u00e1 dispon\u00edvel no Gitpod.</p>"},{"location":"basic_training/setup.pt/#requisitos_1","title":"Requisitos","text":"<ul> <li>Uma conta no GitHub</li> <li>Navegador Web (Google Chrome, Firefox)</li> <li>Conex\u00e3o com a Internet</li> </ul>"},{"location":"basic_training/setup.pt/#gitpod-quick-start","title":"Gitpod quick start","text":"<p>Para executar o Gitpod:</p> <ul> <li>Clique na URL a seguir: https://gitpod.io/#https://github.com/nextflow-io/training<ul> <li>Essa URL \u00e9 o link do reposit\u00f3rio do treinamento no GitHub, prefixado com <code>https://gitpod.io/#</code></li> </ul> </li> <li>Fa\u00e7a login na sua conta do GitHub (e permita a autoriza\u00e7\u00e3o).</li> </ul> <p>Depois de fazer login, o Gitpod deve carregar (pule a pr\u00e9-compila\u00e7\u00e3o, se solicitado).</p>"},{"location":"basic_training/setup.pt/#explore-a-interface-de-desenvolvimento-do-gitpod","title":"Explore a interface de desenvolvimento do Gitpod","text":"<p>Agora voc\u00ea deve ver algo semelhante a imagem a seguir:</p> <p></p> <ul> <li>A barra lateral permite que voc\u00ea personalize seu ambiente Gitpod e execute tarefas b\u00e1sicas (copiar, colar, abrir arquivos, pesquisar, git, etc.). Clique no bot\u00e3o Explorer para ver quais arquivos est\u00e3o neste reposit\u00f3rio.</li> <li>O terminal permite que voc\u00ea execute todos os programas mencionados no reposit\u00f3rio. Por exemplo, tanto <code>nextflow</code> quanto <code>docker</code> est\u00e3o instalados e podem ser executados.</li> <li>A janela principal permite visualizar e editar arquivos. Clique em um arquivo no explorer para abri-lo na janela principal. Voc\u00ea tamb\u00e9m deve ver o material de treinamento em uma das janelas (https://training.nextflow.io/).</li> </ul> <p>Para testar se o ambiente est\u00e1 funcionando corretamente, digite o seguinte comando no terminal:</p> <pre><code>nextflow info\n</code></pre> <p>Isso deve mostrar a vers\u00e3o do Nextflow e as informa\u00e7\u00f5es do sistema de tempo de execu\u00e7\u00e3o:</p> <pre><code>Version: 22.10.4 build 5836\nCreated: 09-12-2022 09:58 UTC\nSystem: Linux 5.15.0-47-generic\nRuntime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 17.0.3-internal+0-adhoc..src\nEncoding: UTF-8 (UTF-8)\n</code></pre>"},{"location":"basic_training/setup.pt/#recursos-do-gitpod","title":"Recursos do Gitpod","text":"<ul> <li>O Gitpod oferece 500 cr\u00e9ditos gratuitos por m\u00eas, o que equivale a 50 horas de tempo de execu\u00e7\u00e3o no ambiente de trabalho padr\u00e3o (at\u00e9 4 n\u00facleos, 8 GB de RAM e 30 GB de armazenamento).</li> <li>H\u00e1 tamb\u00e9m uma op\u00e7\u00e3o de ambiente de trabalho que oferece at\u00e9 8 n\u00facleos, 16 GB de RAM e 50 GB de armazenamento. No entanto, o ambiente de trabalho grande usar\u00e1 seus cr\u00e9ditos gratuitos mais rapidamente e voc\u00ea ter\u00e1 menos horas de acesso a esse ambiente.</li> <li>Sua sess\u00e3o no Gitpod expirar\u00e1 ap\u00f3s 30 minutos de inatividade e suas altera\u00e7\u00f5es ser\u00e3o salvas por at\u00e9 2 semanas (consulte a pr\u00f3xima se\u00e7\u00e3o para reabrir uma sess\u00e3o expirada).</li> </ul> <p>Acesse gitpod.io para mais detalhes.</p>"},{"location":"basic_training/setup.pt/#reabrindo-uma-sessao-do-gitpod","title":"Reabrindo uma sess\u00e3o do Gitpod","text":"<p>Voc\u00ea pode reabrir um ambiente em https://gitpod.io/workspaces. Encontre seu ambiente anterior na lista, selecione as retic\u00eancias (\u00edcone de tr\u00eas pontos) e selecione Abrir.</p> <p>Se voc\u00ea salvou a URL do seu ambiente Gitpod anterior, basta abri-lo em seu navegador.</p> <p>Como alternativa, voc\u00ea pode iniciar um novo ambiente de trabalho seguindo a seguinte URL do Gitpod: https://gitpod.io/#https://github.com/nextflow-io/training</p> <p>Se voc\u00ea perdeu seu ambiente, pode encontrar os principais scripts usados neste tutorial no diret\u00f3rio <code>nf-training</code>.</p>"},{"location":"basic_training/setup.pt/#salvando-arquivos-do-gitpod-em-sua-maquina-local","title":"Salvando arquivos do Gitpod em sua m\u00e1quina local","text":"<p>Para salvar qualquer arquivo a partir do Explorar na barra lateral, clique com o bot\u00e3o direito do mouse no arquivo e selecione Download.</p>"},{"location":"basic_training/setup.pt/#material-do-treinamento","title":"Material do treinamento","text":"<p>O material do treinamento pode ser acessado atrav\u00e9s do seu navegador a partir de https://training.nextflow.io/</p>"},{"location":"basic_training/setup.pt/#selecionando-a-versao-do-nextflow","title":"Selecionando a vers\u00e3o do Nextflow","text":"<p>Por padr\u00e3o, o Nextflow baixar\u00e1 a vers\u00e3o est\u00e1vel mais recente para o seu ambiente.</p> <p>No entanto, o Nextflow est\u00e1 em constante evolu\u00e7\u00e3o \u00e0 medida que fazemos melhorias e corrigimos bugs.</p> <p>Os \u00faltimos lan\u00e7amentos podem ser vistos no GitHub aqui.</p> <p>Se voc\u00ea deseja usar uma vers\u00e3o espec\u00edfica do Nextflow, pode definir a vari\u00e1vel <code>NXF_VER</code> conforme mostrado abaixo:</p> <pre><code>export NXF_VER=23.04.1\n</code></pre> <p>Note</p> <p>Este treinamento requer <code>NXF_VER=23.04.1</code>, ou posterior. Esta vers\u00e3o usar\u00e1 a DSL2 como padr\u00e3o.</p> <p>Execute <code>nextflow -version</code> novamente para confirmar que a altera\u00e7\u00e3o entrou em vigor.</p>"},{"location":"basic_training/tower.fr/","title":"Demarrer avec Nextflow Tower","text":""},{"location":"basic_training/tower.fr/#concept-de-base","title":"Concept de Base","text":"<p>Nextflow Tower est le poste de commande centralis\u00e9 pour la gestion des donn\u00e9es et des workflows. Il apporte la surveillance, la journalisation et l'observabilit\u00e9 aux flux de travail distribu\u00e9s et simplifie le d\u00e9ploiement des flux de travail sur n'importe quel cloud, cluster ou ordinateur portable. Dans la terminologie de Tower, un flux de travail est ce sur quoi nous avons travaill\u00e9 jusqu'\u00e0 pr\u00e9sent, et les pipelines sont des workflows pr\u00e9configur\u00e9s qui peuvent \u00eatre utilis\u00e9s par tous les utilisateurs d'un espace de travail. Il est compos\u00e9 d'un repositoire de workflows, de param\u00e8tres de lancement et d'un environnement de calcul. Nous nous en tiendrons \u00e0 ces d\u00e9finitions dans cette section.</p> <p>Les principales caract\u00e9ristiques de Nextflow tower sont les suivantes</p> <ul> <li>Le lancement de pipelines pr\u00e9configur\u00e9s en toute simplicit\u00e9.</li> <li>L'int\u00e9gration programmatique pour r\u00e9pondre aux besoins d'une organisation.</li> <li>La publication de pipelines dans des espaces de travail partag\u00e9s.</li> <li>La gestion de l'infrastructure n\u00e9cessaire \u00e0 l'ex\u00e9cution d'analyses de donn\u00e9es \u00e0 grande \u00e9chelle.</li> </ul> <p>Conseil</p> <p>Inscrivez-vous pour essayer Tower gratuitement ou demander une demo pour des d\u00e9ploiements dans votre propre environnement sur site ou en could.</p>"},{"location":"basic_training/tower.fr/#utilisation","title":"Utilisation","text":"<p>Vous pouvez utiliser Tower via l'option <code>-with-tower</code> lors de l'utilisation de la commande <code>nextflow run</code>, via l'interface graphique en ligne ou via l'interface utilisateur API.</p>"},{"location":"basic_training/tower.fr/#via-la-commande-nextflow-run","title":"Via la commande <code>nextflow run</code>.","text":"<p>Cr\u00e9ez un compte et connectez-vous \u00e0 Tower.</p> <p>1. Cr\u00e9er un nouveau jeton</p> <p>Vous pouvez acc\u00e9der \u00e0 vos jetons \u00e0 partir du menu d\u00e9roulant R\u00e9glages :</p> <p></p> <p>2. Nommez votre jeton</p> <p></p> <p>3. Sauvegardez votre jeton en toute s\u00e9curit\u00e9</p> <p>Copiez et conservez votre nouveau jeton en lieu s\u00fbr.</p> <p></p> <p>4. Exporter votre jeton</p> <p>Une fois que votre jeton a \u00e9t\u00e9 cr\u00e9\u00e9, ouvrez un terminal et tapez :</p> <pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>O\u00f9 <code>eyxxxxxxxxxxxQ1ZTE=</code> est le jeton que vous venez de cr\u00e9er.</p> <p>Remarque</p> <p>V\u00e9rifiez votre <code>nextflow -version</code>. Les porteurs de jetons n\u00e9cessitent la version 20.10.0 ou plus r\u00e9cente de Nextflow et peuvent \u00eatre configur\u00e9s avec la seconde commande ci-dessus. Vous pouvez changer la version si n\u00e9cessaire.</p> <p>Pour soumettre un pipeline \u00e0 un espace de travail en utilisant l'outil de ligne de commande Nextflow, ajoutez l'ID de l'espace de travail \u00e0 votre environnement. Par exemple :</p> <pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>L'identifiant de l'espace de travail se trouve sur la page de pr\u00e9sentation des espaces de travail de l'organisation.</p> <p>5. Ex\u00e9cuter Nextflow avec Tower</p> <p>Ex\u00e9cutez vos workflows Nextflow comme d'habitude avec l'ajout de la commande <code>-with-tower</code> :</p> <pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>Vous verrez et pourrez suivre vos Nextflow jobs dans Tower.</p> <p>Pour configurer et ex\u00e9cuter des jobs Nextflow dans des environnements Cloud, visitez la section Environnements de calcul.</p> <p>Exercise</p> <p>Ex\u00e9cutez le fichier RNA-Seq <code>script7.nf</code> en utilisant le flag <code>-with-tower</code>, apr\u00e8s avoir correctement compl\u00e9t\u00e9 les param\u00e8tres de jetons d\u00e9crits ci-dessus.</p> Conseil <p>Allez sur https://tower.nf/, connectez-vous, cliquez sur l'onglet run et s\u00e9lectionnez le run que vous venez de soumettre. Si vous ne le trouvez pas, v\u00e9rifiez que votre jeton a \u00e9t\u00e9 saisi correctement.</p>"},{"location":"basic_training/tower.fr/#via-linterface-graphique-en-ligne","title":"Via l'interface graphique en ligne","text":"<p>L'ex\u00e9cution \u00e0 l'aide de l'interface graphique se fait en trois \u00e9tapes principales :</p> <ol> <li>Cr\u00e9er un compte et se connecter \u00e0 Tower, disponible gratuitement \u00e0 l'adresse tower.nf.</li> <li>Cr\u00e9er et configurer un nouvel environnement de calcul.</li> <li>Lancez le lancement des pipelines.</li> </ol>"},{"location":"basic_training/tower.fr/#configurer-votre-environnement-informatique","title":"Configurer votre environnement informatique","text":"<p>ower utilise le concept de Compute Environments pour d\u00e9finir la plateforme d'ex\u00e9cution o\u00f9 un flux de travail sera ex\u00e9cut\u00e9.</p> <p>Il permet de lancer des flux de travail dans un nombre croissant d'infrastructures cloud et on-premise.</p> <p></p> <p>Chaque environnement de calcul doit \u00eatre pr\u00e9configur\u00e9 pour permettre \u00e0 Tower de soumettre des t\u00e2ches. Pour en savoir plus sur la configuration de chaque environnement, cliquez sur les liens ci-dessous.</p> <p>!!! conseil \"Les guides suivants d\u00e9crivent comment configurer chacun de ces environnements informatiques\".</p> <pre><code>* [AWS Batch](https://help.tower.nf/compute-envs/aws-batch/)\n* [Azure Batch](https://help.tower.nf/compute-envs/azure-batch/)\n* [Google Batch](https://help.tower.nf/compute-envs/google-cloud-batch/)\n* [Google Life Sciences](https://help.tower.nf/compute-envs/google-cloud-lifesciences/)\n* [IBM LSF](https://help.tower.nf/compute-envs/lsf/)\n* [Slurm](https://help.tower.nf/compute-envs/slurm/)\n* [Grid Engine](https://help.tower.nf/compute-envs/altair-grid-engine/)\n* [Altair PBS Pro](https://help.tower.nf/compute-envs/altair-pbs-pro/)\n* [Amazon Kubernetes (EKS)](https://help.tower.nf/compute-envs/eks/)\n* [Google Kubernetes (GKE)](https://help.tower.nf/compute-envs/gke/)\n* [Hosted Kubernetes](https://help.tower.nf/compute-envs/k8s/)\n</code></pre>"},{"location":"basic_training/tower.fr/#selection-dun-environnement-informatique-par-defaut","title":"S\u00e9lection d'un environnement informatique par d\u00e9faut","text":"<p>Si vous avez plus d'un Environnement informatique, vous pouvez s\u00e9lectionner celui qui sera utilis\u00e9 par d\u00e9faut lors du lancement d'un pipeline.</p> <ol> <li>Naviguez vers vos environnements de informatique.</li> <li>Choisissez votre environnement par d\u00e9faut en s\u00e9lectionnant le bouton Make primary.</li> </ol> <p>Felicitations!</p> <p>Vous \u00eates maintenant pr\u00eat \u00e0 lancer des flux de travail avec votre environnement de calcul principal.</p>"},{"location":"basic_training/tower.fr/#launchpad","title":"Launchpad","text":"<p>Launchpad permet \u00e0 tout utilisateur de l'espace de travail de lancer facilement un pipeline pr\u00e9configur\u00e9.</p> <p></p> <p>Un pipeline est un repositoire contenant un flux de travail Nextflow, un environnement de calcul et des param\u00e8tres de workflow.</p>"},{"location":"basic_training/tower.fr/#formulaire-des-parametres-du-pipeline","title":"Formulaire des param\u00e8tres du pipeline","text":"<p>Launchpad d\u00e9tecte automatiquement la pr\u00e9sence d'un <code>nextflow_schema.json</code> dans la racine du r\u00e9f\u00e9rentiel et cr\u00e9e dynamiquement un formulaire o\u00f9 les utilisateurs peuvent facilement mettre \u00e0 jour les param\u00e8tres.</p> <p>Info</p> <p>La vue des formulaires de param\u00e8tres appara\u00eetra si le pipeline dispose d'un fichier de sch\u00e9ma Nextflow pour les param\u00e8tres. Veuillez vous r\u00e9f\u00e9rer au Guide du sch\u00e9ma Nextflow pour en savoir plus sur les cas d'utilisation des fichiers de sch\u00e9ma et sur la mani\u00e8re de les cr\u00e9er.</p> <p>Cela permet aux utilisateurs qui n'ont pas d'expertise en Nextflow de saisir les param\u00e8tres de leur workflow et de le lancer.</p> <p></p>"},{"location":"basic_training/tower.fr/#ajouter-une-nouveau-pipeline","title":"Ajouter une nouveau pipeline","text":"<p>L'ajout d'un pipeline \u00e0 la zone de lancement de l'espace de travail pr\u00e9enregistr\u00e9 est d\u00e9crit en d\u00e9tail dans la documentation de la page web de la tower.</p> <p>En bref, voici les \u00e9tapes \u00e0 suivre pour mettre en place une fili\u00e8re.</p> <ol> <li>S\u00e9lectionnez le bouton Launchpad dans la barre de navigation. Le Launch Form s'ouvre alors.</li> <li>S\u00e9lectionnez un environnement informatique.</li> <li>Saisissez le repositoire du workflow que vous souhaitez lancer, par exemple https://github.com/nf-core/rnaseq.git.</li> <li>S\u00e9lectionner un workflow Num\u00e9ro de r\u00e9vision. La branche par d\u00e9faut de Git (main/master) ou <code>manifest.defaultBranch</code> dans la configuration de Nextflow sera utilis\u00e9e par d\u00e9faut.</li> <li>D\u00e9finir l'emplacement R\u00e9pertoire de travail du r\u00e9pertoire de travail de Nextflow. L'emplacement associ\u00e9 \u00e0 l'environnement informatique sera s\u00e9lectionn\u00e9 par d\u00e9faut.</li> <li>Entrez le(s) nom(s) de chacun des profils de configuration de Nextflow suivi de la touche <code>Enter</code>. Voir la documentation Nextflow Configuration des profiles pour plus de d\u00e9tails.</li> <li> <p>Saisissez les param\u00e8tres du workflow au format YAML ou JSON. Exemple YAML :</p> <pre><code>reads: \"s3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2\"\npaired_end: true\n</code></pre> </li> <li> <p>S\u00e9lectionnez Launch (Lancer) pour commencer l'ex\u00e9cution du pipeline.</p> </li> </ol> <p>Info</p> <p>Les workflows Nextflow sont simplement des repositoires Git et peuvent \u00eatre chang\u00e9s pour n'importe quelle plateforme d'h\u00e9bergement Git publique ou priv\u00e9e. Voir Git Integration dans la documentation Tower et Partage de Pipeline  dans la documentation Nextflow pour plus de d\u00e9tails.</p> <p>Remarque</p> <p>Les informations d'identification associ\u00e9es \u00e0 l'environnement de informatique doivent pouvoir acc\u00e9der au r\u00e9pertoire de travail.</p> <p>Info</p> <p>Dans la configuration, le chemin d'acc\u00e8s complet \u00e0 un godet doit \u00eatre sp\u00e9cifi\u00e9 entre guillemets simples pour les cha\u00eenes de caract\u00e8res et sans guillemets pour les bool\u00e9ens ou les nombres.</p> <p>Conseil</p> <p>Pour cr\u00e9er votre propre sch\u00e9ma Nextflow personnalis\u00e9 pour votre workflow, consultez les exemples des workflows <code>nf-core</code> qui ont adopt\u00e9 cette approche. Par exemple, eager et rnaseq.</p> <p>Pour les options de param\u00e9trage avanc\u00e9es, consultez cette page.</p> <p>Un soutien communautaire est \u00e9galement disponible en cas de probl\u00e8me, rejoignez le Slack Nextflow en suivant ce lien.</p>"},{"location":"basic_training/tower.fr/#api","title":"API","text":"<p>Pour en savoir plus sur l'utilisation de l'API Tower, consultez la section API de cette documentation.</p>"},{"location":"basic_training/tower.fr/#espaces-de-travail-et-organisations","title":"Espaces de travail et organisations","text":"<p>Nextflow Tower simplifie le d\u00e9veloppement et l'ex\u00e9cution des pipelines en fournissant une interface centralis\u00e9e pour les utilisateurs et les organisations.</p> <p>Chaque utilisateur dispose d'un espace de travail unique o\u00f9 il peut interagir et g\u00e9rer toutes les ressources telles que les flux de travail, les environnements informatiques et les credits. Les d\u00e9tails sont disponibles ici.</p> <p>Les organisations peuvent avoir plusieurs espaces de travail avec un acc\u00e8s personnalis\u00e9 pour les membres et collaborateurs de l'organisation.</p>"},{"location":"basic_training/tower.fr/#ressources-de-lorganisation","title":"Ressources de l'organisation","text":"<p>Vous pouvez cr\u00e9er votre propre organisation et votre propre espace de travail pour les participants en suivant la documentation \u00e0 tower.</p> <p>Tower permet la cr\u00e9ation de plusieurs organisations, chacune pouvant contenir plusieurs espaces de travail avec des utilisateurs et des ressources partag\u00e9s. Cela permet \u00e0 toute organisation de personnaliser et d'organiser l'utilisation des ressources tout en maintenant une couche de contr\u00f4le d'acc\u00e8s pour les utilisateurs associ\u00e9s \u00e0 un espace de travail.</p>"},{"location":"basic_training/tower.fr/#organisation-utilisateurs","title":"Organisation utilisateurs","text":"<p>Tout utilisateur peut \u00eatre ajout\u00e9 ou supprim\u00e9 d'une organisation particuli\u00e8re ou d'un espace de travail et peut se voir attribuer un r\u00f4le d'acc\u00e8s sp\u00e9cifique au sein de cet espace de travail.</p> <p>La fonction Teams (\u00c9quipes) permet aux organisations de regrouper divers utilisateurs et participants en \u00e9quipes. Par exemple, <code>d\u00e9veloppeurs de workflow</code> ou <code>analystes</code>, et d'appliquer le contr\u00f4le d'acc\u00e8s \u00e0 tous les utilisateurs de cette \u00e9quipe collectivement.</p> <p>Pour plus d'informations, veuillez vous referez \u00e0 la section Gestion des utilisateurs.</p>"},{"location":"basic_training/tower.fr/#mise-en-place-dune-nouvelle-organisation","title":"Mise en place d'une nouvelle organisation","text":"<p>Les organisations constituent la structure de premier niveau et contiennent des espaces de travail, des membres, des \u00e9quipes et des collaborateurs.</p> <p>Pour cr\u00e9er une nouvelle organisation :</p> <ol> <li>Cliquez sur le menu d\u00e9roulant \u00e0 c\u00f4t\u00e9 de votre nom et s\u00e9lectionnez Nouvelle organisation pour ouvrir la bo\u00eete de dialogue de cr\u00e9ation.</li> <li> <p>Dans la bo\u00eete de dialogue, remplissez les champs en fonction de votre organisation. Les champs Nom et Nom complet sont obligatoires.</p> <p>Remarque</p> <p>Un nom valide pour l'organisation doit suivre un mod\u00e8le sp\u00e9cifique. Veuillez vous r\u00e9f\u00e9rer \u00e0 l'interface utilisateur pour plus d'informations.</p> </li> <li> <p>Les autres champs, tels que la description, la localisation, l'URL du site web et l'URL du logo, sont facultatifs.</p> </li> <li> <p>Une fois les d\u00e9tails renseign\u00e9s, vous pouvez acc\u00e9der \u00e0 l'organisation nouvellement cr\u00e9\u00e9e en utilisant la page de l'organisation, qui r\u00e9pertorie toutes vos organisations.</p> <p>Remarque</p> <p>Il est possible de modifier les valeurs des champs facultatifs soit en utilisant l'option Modifier sur la page de l'organisation, soit en utilisant l'onglet Param\u00e8tres sur la page de l'organisation, \u00e0 condition d'\u00eatre le propri\u00e9taire de l'organisation.</p> <p>Conseil</p> <p>Une liste de tous les membres, \u00e9quipes et collaborateurs inclus se trouve sur la page de l'organisation.</p> </li> </ol>"},{"location":"basic_training/tower/","title":"Get started with Nextflow Tower","text":""},{"location":"basic_training/tower/#basic-concepts","title":"Basic concepts","text":"<p>Nextflow Tower is the centralized command post for data management and workflows. It brings monitoring, logging and observability to distributed workflows and simplifies the deployment of workflows on any cloud, cluster or laptop. In Tower terminology, a workflow is what we've been working on so far, and pipelines are pre-configured workflows that can be used by all users in a workspace. It is composed of a workflow repository, launch parameters, and a compute environment. We'll stick to these definitions in this section.</p> <p>Nextflow tower core features include:</p> <ul> <li>The launching of pre-configured pipelines with ease.</li> <li>Programmatic integration to meet the needs of an organization.</li> <li>Publishing pipelines to shared workspaces.</li> <li>Management of the infrastructure required to run data analysis at scale.</li> </ul> <p>Tip</p> <p>Sign up to try Tower for free or request a demo for deployments in your own on-premise or cloud environment.</p>"},{"location":"basic_training/tower/#usage","title":"Usage","text":"<p>You can use Tower via either the <code>-with-tower</code> option while using the <code>nextflow run</code> command, through the online GUI or through the API.</p>"},{"location":"basic_training/tower/#via-the-nextflow-run-command","title":"Via the <code>nextflow run</code> command","text":"<p>Create an account and login into Tower.</p> <p>1. Create a new token</p> <p>You can access your tokens from the Settings drop-down menu:</p> <p></p> <p>2. Name your token</p> <p></p> <p>3. Save your token safely</p> <p>Copy and keep your new token in a safe place.</p> <p></p> <p>4. Export your token</p> <p>Once your token has been created, open a terminal and type:</p> <pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>Where <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> is the token you have just created.</p> <p>Note</p> <p>Check your <code>nextflow -version</code>. Bearer tokens require Nextflow version 20.10.0 or later and can be set with the second command shown above. You can change the version if necessary.</p> <p>To submit a pipeline to a Workspace using the Nextflow command-line tool, add the workspace ID to your environment. For example:</p> <pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>The workspace ID can be found on the organization\u2019s Workspaces overview page.</p> <p>5. Run Nextflow with tower</p> <p>Run your Nextflow workflows as usual with the addition of the <code>-with-tower</code> command:</p> <pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>You will see and be able to monitor your Nextflow jobs in Tower.</p> <p>To configure and execute Nextflow jobs in Cloud environments, visit the Compute environments section.</p> <p>Exercise</p> <p>Run the RNA-Seq <code>script7.nf</code> using the <code>-with-tower</code> flag, after correctly completing the token settings outlined above.</p> Tip <p>Go to https://tower.nf/, login, then click the run tab, and select the run that you just submitted. If you can\u2019t find it, double check your token was entered correctly.</p>"},{"location":"basic_training/tower/#via-online-gui","title":"Via online GUI","text":"<p>To run using the GUI, there are three main steps:</p> <ol> <li>Create an account and login into Tower, available free of charge, at tower.nf.</li> <li>Create and configure a new compute environment.</li> <li>Start launching pipelines.</li> </ol>"},{"location":"basic_training/tower/#configuring-your-compute-environment","title":"Configuring your compute environment","text":"<p>Tower uses the concept of Compute Environments to define the execution platform where a workflow will run.</p> <p>It supports the launching of workflows into a growing number of cloud and on-premise infrastructures.</p> <p></p> <p>Each compute environment must be pre-configured to enable Tower to submit tasks. You can read more on how to set up each environment using the links below.</p> <p>The following guides describe how to configure each of these compute environments.</p> <ul> <li>AWS Batch</li> <li>Azure Batch</li> <li>Google Batch</li> <li>Google Life Sciences</li> <li>IBM LSF</li> <li>Slurm</li> <li>Grid Engine</li> <li>Altair PBS Pro</li> <li>Amazon Kubernetes (EKS)</li> <li>Google Kubernetes (GKE)</li> <li>Hosted Kubernetes</li> </ul>"},{"location":"basic_training/tower/#selecting-a-default-compute-environment","title":"Selecting a default compute environment","text":"<p>If you have more than one Compute Environment, you can select which one will be used by default when launching a pipeline.</p> <ol> <li>Navigate to your compute environments.</li> <li>Choose your default environment by selecting the Make primary button.</li> </ol> <p>Congratulations!</p> <p>You are now ready to launch workflows with your primary compute environment.</p>"},{"location":"basic_training/tower/#launchpad","title":"Launchpad","text":"<p>Launchpad makes it easy for any workspace user to launch a pre-configured pipeline.</p> <p></p> <p>A pipeline is a repository containing a Nextflow workflow, a compute environment and workflow parameters.</p>"},{"location":"basic_training/tower/#pipeline-parameters-form","title":"Pipeline Parameters Form","text":"<p>Launchpad automatically detects the presence of a <code>nextflow_schema.json</code> in the root of the repository and dynamically creates a form where users can easily update the parameters.</p> <p>Info</p> <p>The parameter forms view will appear if the pipeline has a Nextflow schema file for the parameters. Please refer to the Nextflow Schema guide to learn more about the schema file use-cases and how to create them.</p> <p>This makes it trivial for users without any expertise in Nextflow to enter their workflow parameters and launch.</p> <p></p>"},{"location":"basic_training/tower/#adding-a-new-pipeline","title":"Adding a new pipeline","text":"<p>Adding a pipeline to the pre-saved workspace launchpad is detailed in full on the tower webpage docs.</p> <p>In brief, these are the steps you need to follow to set up a pipeline.</p> <ol> <li>Select the Launchpad button in the navigation bar. This will open the Launch Form.</li> <li>Select a compute environment.</li> <li>Enter the repository of the workflow you want to launch. e.g. https://github.com/nf-core/rnaseq.git</li> <li>Select a workflow Revision number. The Git default branch (main/master) or <code>manifest.defaultBranch</code> in the Nextflow configuration will be used by default.</li> <li>Set the Work directory location of the Nextflow work directory. The location associated with the compute environment will be selected by default.</li> <li>Enter the name(s) of each of the Nextflow Config profiles followed by the <code>Enter</code> key. See the Nextflow Config profiles documentation for more details.</li> <li> <p>Enter any workflow parameters in YAML or JSON format. YAML example:</p> <pre><code>reads: \"s3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2\"\npaired_end: true\n</code></pre> </li> <li> <p>Select Launch to begin the pipeline execution.</p> </li> </ol> <p>Info</p> <p>Nextflow workflows are simply Git repositories and can be changed to any public or private Git-hosting platform. See Git Integration in the Tower docs and Pipeline Sharing in the Nextflow docs for more details.</p> <p>Note</p> <p>The credentials associated with the compute environment must be able to access the work directory.</p> <p>Info</p> <p>In the configuration, the full path to a bucket must be specified with single quotes around strings and no quotes around booleans or numbers.</p> <p>Tip</p> <p>To create your own customized Nextflow Schema for your workflow, see the examples from the <code>nf-core</code> workflows that have adopted this approach. For example, eager and rnaseq.</p> <p>For advanced settings options check out this page.</p> <p>There is also community support available if you get into trouble, join the Nextflow Slack by following this link.</p>"},{"location":"basic_training/tower/#api","title":"API","text":"<p>To learn more about using the Tower API, visit the API section in this documentation.</p>"},{"location":"basic_training/tower/#workspaces-and-organizations","title":"Workspaces and Organizations","text":"<p>Nextflow Tower simplifies the development and execution of pipeline by providing a centralized interface for users and organizations.</p> <p>Each user has a unique workspace where they can interact and manage all resources such as workflows, compute environments and credentials. Details of this can be found here.</p> <p>Organisations can have multiple workspaces with customized access for specific organisation members and collaborators.</p>"},{"location":"basic_training/tower/#organization-resources","title":"Organization resources","text":"<p>You can create your own organization and participant workspace by following the docs at tower.</p> <p>Tower allows the creation of multiple organizations, each of which can contain multiple workspaces with shared users and resources. This allows any organization to customize and organize the usage of resources while maintaining an access control layer for users associated with a workspace.</p>"},{"location":"basic_training/tower/#organization-users","title":"Organization users","text":"<p>Any user can be added or removed from a particular organization or a workspace and can be allocated a specific access role within that workspace.</p> <p>The Teams feature provides a way for organizations to group various users and participants together into teams. For example, <code>workflow-developers</code> or <code>analysts</code>, and apply access control to all the users within this team collectively.</p> <p>For further information, please refer to the User Management section.</p>"},{"location":"basic_training/tower/#setting-up-a-new-organization","title":"Setting up a new organization","text":"<p>Organizations are the top-level structure and contain Workspaces, Members, Teams and Collaborators.</p> <p>To create a new Organization:</p> <ol> <li>Click on the dropdown next to your name and select New organization to open the creation dialog.</li> <li> <p>On the dialog, fill in the fields as per your organization. The Name and Full name fields are compulsory.</p> <p>Note</p> <p>A valid name for the organization must follow a specific pattern. Please refer to the UI for further instructions.</p> </li> <li> <p>The rest of the fields such as Description, Location, Website URL and Logo Url are optional.</p> </li> <li> <p>Once the details are filled in, you can access the newly created organization using the organization\u2019s page, which lists all of your organizations.</p> <p>Note</p> <p>It is possible to change the values of the optional fields either using the Edit option on the organization\u2019s page or by using the Settings tab within the organization page, provided that you are the Owner of the organization.</p> <p>Tip</p> <p>A list of all the included Members, Teams and Collaborators can be found on the organization page.</p> </li> </ol>"},{"location":"basic_training/tower.pt/","title":"Comece a usar o Nextflow Tower","text":""},{"location":"basic_training/tower.pt/#conceitos-basicos","title":"Conceitos B\u00e1sicos","text":"<p>O Nextflow Tower \u00e9 o posto de comando centralizado para gerenciamento de dados e fluxos de trabalho. Ele traz monitoramento, gerenciamento de logs e observabilidade para fluxos de trabalho distribu\u00eddos e simplifica a implanta\u00e7\u00e3o de fluxos de trabalho em qualquer nuvem, cluster ou laptop. Na terminologia do Tower, um fluxo de trabalho \u00e9 o que temos trabalhado at\u00e9 agora, e os pipelines s\u00e3o fluxos de trabalho pr\u00e9-configurados que podem ser usados por todos os usu\u00e1rios em um espa\u00e7o de trabalho. Ele \u00e9 composto por um reposit\u00f3rio de fluxo de trabalho, par\u00e2metros de inicializa\u00e7\u00e3o e um ambiente de computa\u00e7\u00e3o. Vamos nos ater a essas defini\u00e7\u00f5es nesta se\u00e7\u00e3o.</p> <p>Os principais recursos do Nextflow Tower incluem:</p> <ul> <li>O lan\u00e7amento de pipelines pr\u00e9-configurados com facilidade.</li> <li>Integra\u00e7\u00e3o program\u00e1tica para atender \u00e0s necessidades de uma organiza\u00e7\u00e3o.</li> <li>Disponibiliza\u00e7\u00e3o pipelines em \u00e1reas de trabalho compartilhadas.</li> <li>Gerenciamento da infraestrutura necess\u00e1ria para executar an\u00e1lise de dados em escala.</li> </ul> <p>Tip</p> <p>Registre-se para experimentar o Tower gratuitamente ou solicitar uma demonstra\u00e7\u00e3o para implanta\u00e7\u00f5es em seu pr\u00f3prio ambiente local ou na nuvem.</p>"},{"location":"basic_training/tower.pt/#como-usar","title":"Como usar","text":"<p>Voc\u00ea pode usar o Tower por meio da op\u00e7\u00e3o <code>-with-tower</code> ao usar o comando <code>nextflow run</code>, por meio da interface gr\u00e1fica online ou da API.</p>"},{"location":"basic_training/tower.pt/#com-o-comando-nextflow-run","title":"Com o comando <code>nextflow run</code>","text":"<p>Crie uma conta e fa\u00e7a login no Tower.</p> <p>1. Crie um novo token</p> <p>Voc\u00ea pode acessar seus tokens no menu suspenso Settings:</p> <p></p> <p>2. D\u00ea um nome para seu token</p> <p></p> <p>3. Salve seu token com seguran\u00e7a</p> <p>Copie e guarde seu novo token em um local seguro.</p> <p></p> <p>4. Exporte seu token</p> <p>Uma vez que seu token foi criado, abra um terminal e digite:</p> <pre><code>export TOWER_ACCESS_TOKEN=eyxxxxxxxxxxxxxxxQ1ZTE=\n</code></pre> <p>Onde <code>eyxxxxxxxxxxxxxxxQ1ZTE=</code> \u00e9 o token que voc\u00ea acabou de criar.</p> <p>Note</p> <p>Verifique seu <code>nextflow -version</code>. Os tokens de portador requerem ao menos a vers\u00e3o 20.10.0 do Nextflow ou vers\u00f5es posteriores e isso pode ser configurado com o segundo comando mostrado acima. Voc\u00ea pode alterar para uma outra vers\u00e3o, se necess\u00e1rio.</p> <p>Para enviar um pipeline para uma \u00e1rea de trabalho (workspace) usando a ferramenta de linha de comando do Nextflow, adicione o ID da \u00e1rea de trabalho em seu ambiente. Por exemplo:</p> <pre><code>export TOWER_WORKSPACE_ID=000000000000000\n</code></pre> <p>O ID da \u00e1rea de trabalho pode ser encontrado na p\u00e1gina de vis\u00e3o geral das \u00e1reas de trabalho (workspaces) da organiza\u00e7\u00e3o.</p> <p>5. Execute o Nextflow com o Tower</p> <p>Execute normalmente seus fluxos de trabalho do Nextflow com a adi\u00e7\u00e3o do comando <code>-with-tower</code>:</p> <pre><code>nextflow run hello.nf -with-tower\n</code></pre> <p>Voc\u00ea ver\u00e1 e poder\u00e1 monitorar suas tarefas do Nextflow no Tower.</p> <p>Para configurar e executar tarefas do Nextflow em ambientes na nuvem, visite a se\u00e7\u00e3o de ambientes de computa\u00e7\u00e3o (compute environments).</p> <p>Exercise</p> <p>Execute o script de RNA-Seq <code>script7.nf</code> usando o sinalizador <code>-with-tower</code>, depois de concluir corretamente as configura\u00e7\u00f5es de token descritas acima.</p> Tip <p>V\u00e1 para https://tower.nf/, fa\u00e7a o login, em seguida, clique na guia de execu\u00e7\u00f5es (Run) e selecione a execu\u00e7\u00e3o que voc\u00ea acabou de enviar. Se voc\u00ea n\u00e3o conseguir encontr\u00e1-la, verifique novamente se seu token foi adicionado corretamente ao ambiente.</p>"},{"location":"basic_training/tower.pt/#com-uma-interface-grafica-online","title":"Com uma interface gr\u00e1fica online","text":"<p>Para executar usando a interface gr\u00e1fica (GUI), existem tr\u00eas etapas principais:</p> <ol> <li>Crie uma conta e fa\u00e7a login no Tower, dispon\u00edvel gratuitamente, em tower.nf.</li> <li>Crie e configure um novo ambiente de computa\u00e7\u00e3o (compute environment).</li> <li>Comece a lan\u00e7ar pipelines.</li> </ol>"},{"location":"basic_training/tower.pt/#configurando-seu-ambiente-de-computacao","title":"Configurando seu ambiente de computa\u00e7\u00e3o","text":"<p>O Tower usa o conceito de Ambientes de Computa\u00e7\u00e3o (compute environments) para definir a plataforma de execu\u00e7\u00e3o onde um fluxo de trabalho ser\u00e1 executado.</p> <p>Ele suporta o lan\u00e7amento de fluxos de trabalho em um n\u00famero crescente de infraestruturas de nuvem e on-prem (infraestrutura dedicada).</p> <p></p> <p>Cada ambiente de computa\u00e7\u00e3o deve ser pr\u00e9-configurado para permitir que o Tower envie tarefas. Voc\u00ea pode ler mais sobre como configurar cada ambiente usando os links abaixo.</p> <p>Os guias a seguir descrevem como configurar cada um desses ambientes de computa\u00e7\u00e3o.</p> <ul> <li>AWS Batch</li> <li>Azure Batch</li> <li>Google Cloud</li> <li>Google Batch</li> <li>Google Life Sciences</li> <li>IBM LSF</li> <li>Slurm</li> <li>Grid Engine</li> <li>Altair PBS Pro</li> <li>Amazon Kubernetes (EKS)</li> <li>Google Kubernetes (GKE)</li> <li>Hosted Kubernetes</li> </ul>"},{"location":"basic_training/tower.pt/#selecionando-um-ambiente-de-computacao-padrao","title":"Selecionando um ambiente de computa\u00e7\u00e3o padr\u00e3o","text":"<p>Se voc\u00ea tiver mais de um Ambiente de computa\u00e7\u00e3o, poder\u00e1 selecionar qual deles ser\u00e1 usado por padr\u00e3o ao lan\u00e7ar um pipeline.</p> <ol> <li>Navegue at\u00e9 os seus ambientes de computa\u00e7\u00e3o.</li> <li>Escolha seu ambiente padr\u00e3o selecionando o bot\u00e3o Make primary.</li> </ol> <p>Parab\u00e9ns!</p> <p>Agora voc\u00ea est\u00e1 pronto para lan\u00e7ar fluxos de trabalho com seu ambiente de computa\u00e7\u00e3o principal.</p>"},{"location":"basic_training/tower.pt/#launchpad","title":"Launchpad","text":"<p>O Launchpad torna f\u00e1cil para qualquer usu\u00e1rio da \u00e1rea de trabalho lan\u00e7ar um pipeline pr\u00e9-configurado.</p> <p></p> <p>Um pipeline \u00e9 um reposit\u00f3rio que cont\u00e9m um fluxo de trabalho do Nextflow, um ambiente de computa\u00e7\u00e3o e par\u00e2metros de fluxo de trabalho.</p>"},{"location":"basic_training/tower.pt/#formulario-de-parametros-de-pipeline","title":"Formul\u00e1rio de Par\u00e2metros de Pipeline","text":"<p>O Launchpad detecta automaticamente a presen\u00e7a de um <code>nextflow_schema.json</code> na raiz do reposit\u00f3rio e cria dinamicamente um formul\u00e1rio onde os usu\u00e1rios podem facilmente atualizar os par\u00e2metros.</p> <p>Info</p> <p>A exibi\u00e7\u00e3o de formul\u00e1rios de par\u00e2metro aparecer\u00e1 se o pipeline tiver um arquivo de esquema do Nextflow para os par\u00e2metros. Consulte o Guia do esquema do Nextflow para saber mais sobre os casos de uso do arquivo de esquema e como cri\u00e1-los.</p> <p>Isso torna trivial para usu\u00e1rios sem experi\u00eancia em Nextflow inserir seus par\u00e2metros de fluxo de trabalho e lan\u00e7\u00e1-lo.</p> <p></p>"},{"location":"basic_training/tower.pt/#adicionando-um-novo-pipeline","title":"Adicionando um novo pipeline","text":"<p>A adi\u00e7\u00e3o de um pipeline ao launchpad da \u00e1rea de trabalho \u00e9 detalhada na \u00edntegra na documenta\u00e7\u00e3o do Tower.</p> <p>Em resumo, essas s\u00e3o as etapas que voc\u00ea precisa seguir para configurar um pipeline.</p> <ol> <li>Selecione o bot\u00e3o Launchpad na barra de navega\u00e7\u00e3o. Isso abrir\u00e1 o Formul\u00e1rio de inicializa\u00e7\u00e3o.</li> <li>Selecione um ambiente de computa\u00e7\u00e3o.</li> <li>Insira o reposit\u00f3rio do fluxo de trabalho que voc\u00ea deseja iniciar. e.g. https://github.com/nf-core/rnaseq.git</li> <li>Selecione um n\u00famero de revis\u00e3o para o fluxo de trabalho. O branching padr\u00e3o do Git (main/master) ou <code>manifest.defaultBranch</code> na configura\u00e7\u00e3o do Nextflow ser\u00e1 usada por padr\u00e3o.</li> <li>Defina o local do diret\u00f3rio de trabalho (<code>workDir</code>) do Nextflow. O local associado ao ambiente de computa\u00e7\u00e3o ser\u00e1 selecionado por padr\u00e3o.</li> <li>Digite o(s) nome(s) de cada um dos perfis de configura\u00e7\u00e3o do Nextflow seguido da tecla <code>enter</code>. Veja mais na documenta\u00e7\u00e3o oficial sobre a configura\u00e7\u00e3o de perfis.</li> <li> <p>Insira quaisquer par\u00e2metros do fluxo de trabalho no formato YAML ou JSON. Exemplo com YAML:</p> <pre><code>leituras: \"s3://nf-bucket/exome-data/ERR013140_{1,2}.fastq.bz2\"\npares_de_leituras: true\n</code></pre> </li> <li> <p>Selecione Launch para iniciar a execu\u00e7\u00e3o do pipeline.</p> </li> </ol> <p>Info</p> <p>Os fluxos de trabalho do Nextflow s\u00e3o simplesmente reposit\u00f3rios Git e podem ser alterados para qualquer plataforma de hospedagem Git p\u00fablica ou privada. Consulte Integra\u00e7\u00e3o com o Git nos documentos do Tower e Compartilhamento de Pipelines na documenta\u00e7\u00e3o do Nextflow para obter mais detalhes.</p> <p>Note</p> <p>As credenciais associadas ao ambiente de computa\u00e7\u00e3o devem ser capazes de acessar o diret\u00f3rio de trabalho.</p> <p>Info</p> <p>Na configura\u00e7\u00e3o, o caminho completo para um bucket deve ser especificado com aspas simples em torno de strings e sem aspas em booleanas ou n\u00fameros.</p> <p>Tip</p> <p>Para criar seu pr\u00f3prio esquema de Nextflow personalizado para seu fluxo de trabalho, veja os exemplos dos fluxos de trabalho do <code>nf-core</code> que adotaram essa abordagem. Por exemplo, o eager e o rnaseq.</p> <p>Para op\u00e7\u00f5es de configura\u00e7\u00f5es avan\u00e7adas, confira essa p\u00e1gina.</p> <p>Tamb\u00e9m h\u00e1 suporte da comunidade dispon\u00edvel se voc\u00ea tiver problemas, junte-se ao Slack do Nextflow seguindo este link.</p>"},{"location":"basic_training/tower.pt/#api","title":"API","text":"<p>Para saber mais sobre como usar a API do Tower, visite a se\u00e7\u00e3o da API na documenta\u00e7\u00e3o.</p>"},{"location":"basic_training/tower.pt/#areas-de-trabalho-e-organizacoes","title":"\u00c1reas de trabalho e Organiza\u00e7\u00f5es","text":"<p>O Nextflow Tower simplifica o desenvolvimento e a execu\u00e7\u00e3o de pipelines, fornecendo uma interface centralizada para usu\u00e1rios e organiza\u00e7\u00f5es.</p> <p>Cada usu\u00e1rio tem uma \u00e1rea de trabalho exclusiva onde pode interagir e gerenciar todos os recursos, como fluxos de trabalho, ambientes de computa\u00e7\u00e3o e credenciais. Detalhes disso podem ser encontrados aqui.</p> <p>As organiza\u00e7\u00f5es podem ter v\u00e1rios espa\u00e7os de trabalho com acesso personalizado para membros e colaboradores espec\u00edficos da organiza\u00e7\u00e3o.</p>"},{"location":"basic_training/tower.pt/#recursos-de-organizacao","title":"Recursos de organiza\u00e7\u00e3o","text":"<p>Voc\u00ea pode criar sua pr\u00f3pria organiza\u00e7\u00e3o e \u00e1rea de trabalho de participante seguindo a documenta\u00e7\u00e3o aqui.</p> <p>O Tower permite a cria\u00e7\u00e3o de v\u00e1rias organiza\u00e7\u00f5es, cada uma das quais pode conter v\u00e1rias \u00e1reas de trabalho com usu\u00e1rios e recursos compartilhados. Isso permite que qualquer organiza\u00e7\u00e3o personalize e organize o uso de recursos enquanto mant\u00e9m uma camada de controle de acesso para usu\u00e1rios associados a uma \u00e1rea de trabalho.</p>"},{"location":"basic_training/tower.pt/#usuarios-da-organizacao","title":"Usu\u00e1rios da organiza\u00e7\u00e3o","text":"<p>Qualquer usu\u00e1rio pode ser adicionado ou removido de uma determinada organiza\u00e7\u00e3o ou \u00e1rea de trabalho e pode receber um papel de acesso espec\u00edfico dentro dessa \u00e1rea de trabalho.</p> <p>O recurso Equipes fornece uma maneira para as organiza\u00e7\u00f5es agruparem v\u00e1rios usu\u00e1rios e participantes em equipes. Por exemplo, <code>desenvolvedores-fluxos de trabalho</code> ou <code>analistas</code>, e aplicar controle de acesso a todos os usu\u00e1rios dentro desta equipe coletivamente.</p> <p>Para mais informa\u00e7\u00f5es, consulte a se\u00e7\u00e3o de Gerenciamento de Usu\u00e1rio.</p>"},{"location":"basic_training/tower.pt/#configurando-uma-nova-organizacao","title":"Configurando uma nova organiza\u00e7\u00e3o","text":"<p>As organiza\u00e7\u00f5es s\u00e3o a estrutura de n\u00edvel mais alto e cont\u00eam \u00e1reas de trabalho, membros, equipes e colaboradores.</p> <p>Para criar uma nova Organiza\u00e7\u00e3o:</p> <ol> <li>Clique no menu suspenso ao lado do seu nome e selecione New organization para abrir a caixa de di\u00e1logo de cria\u00e7\u00e3o.</li> <li> <p>Na caixa de di\u00e1logo, preencha os campos de acordo com sua organiza\u00e7\u00e3o. Os campos Name e Full name s\u00e3o obrigat\u00f3rios.</p> <p>Note</p> <p>Um nome v\u00e1lido para a organiza\u00e7\u00e3o deve seguir um padr\u00e3o espec\u00edfico. Consulte a interface de usu\u00e1rio para obter mais instru\u00e7\u00f5es.</p> </li> <li> <p>O restante dos campos, como Description, Location, Website URL e logo URL, s\u00e3o opcionais.</p> </li> <li> <p>Depois que os detalhes forem preenchidos, voc\u00ea poder\u00e1 acessar a organiza\u00e7\u00e3o rec\u00e9m-criada usando a p\u00e1gina da organiza\u00e7\u00e3o, que lista todas as suas organiza\u00e7\u00f5es.</p> <p>Note</p> <p>\u00c9 poss\u00edvel alterar os valores dos campos opcionais usando a op\u00e7\u00e3o Edit na p\u00e1gina da organiza\u00e7\u00e3o ou usando a guia Settings na p\u00e1gina da organiza\u00e7\u00e3o, desde que voc\u00ea seja o Propriet\u00e1rio (Owner) da organiza\u00e7\u00e3o.</p> <p>Tip</p> <p>Uma lista de todos os Membros, Equipes e Colaboradores inclu\u00eddos pode ser encontrada na p\u00e1gina da organiza\u00e7\u00e3o.</p> </li> </ol>"},{"location":"hands_on/","title":"Nextflow course - Hands-on","text":"<p>This hands-on session shows how to implement a Nextflow Variant Calling analysis pipeline for RNA-seq data that is based on GATK best practices.</p> <p>Warning</p> <p>The content and pipeline in this course is aimed at teaching Nextflow, not bioinformatics. Software versions are outdated, among other things, so if you want a variant calling Nextflow pipeline for production you should use nf-core/sarek, nf-core/viralrecon or nf-core/rnavar instead.</p>"},{"location":"hands_on/01_datasets/","title":"Data Description","text":"<p>The input data used to test the pipeline implementation is described below. For the purpose of this project, only a subset of the original data is used for most of the data types.</p>"},{"location":"hands_on/01_datasets/#genome-assembly","title":"Genome assembly","text":"<p><code>genome.fa</code></p> <p>The human genome assembly hg19 (GRCh37) from GenBank, chromosome 22 only.</p>"},{"location":"hands_on/01_datasets/#rna-seq-reads","title":"RNA-seq reads","text":"<p><code>ENCSR000COQ[12]_[12].fastq.gz</code></p> <p>The RNA-seq data comes from the human GM12878 cell line from whole cell, cytosol and nucleus extraction (see table below).</p> <p>The libraries are stranded PE76 Illumina GAIIx RNA-Seq from rRNA-depleted Poly-A+ long RNA (<code>&gt; 200</code> nucleotides in size).</p> <p>Only reads mapped to the 22q11^ locus of the human genome (<code>chr22:16000000-18000000</code>) are used.</p> ENCODE ID Cellular fraction Replicate ID File names ENCSR000COQ Whole Cell 12 <code>ENCSR000COQ1_1.fastq.gz</code><code>ENCSR000COQ2_1.fastq.gz</code> <code>ENCSR000COQ1_2.fastq.gz</code><code>ENCSR000COQ2_2.fastq.gz</code> ENCSR000CPO Nuclear 12 <code>ENCSR000CPO1_1.fastq.gz</code><code>ENCSR000CPO2_1.fastq.gz</code> <code>ENCSR000CPO1_2.fastq.gz</code><code>ENCSR000CPO2_2.fastq.gz</code> ENCSR000COR Cytosolic 12 <code>ENCSR000COR1_1.fastq.gz</code><code>ENCSR000COR2_1.fastq.gz</code> <code>ENCSR000COR1_2.fastq.gz</code><code>ENCSR000COR2_2.fastq.gz</code>"},{"location":"hands_on/01_datasets/#known-variants","title":"\"Known\" variants","text":"<p><code>known_variants.vcf.gz</code></p> <p>Known variants come from high confident variant calls for GM12878 from the Illumina Platinum Genomes project. These variant calls were obtained by taking into account pedigree information and the concordance of calls across different methods.</p> <p>We\u2019re using the subset from chromosome 22 only.</p>"},{"location":"hands_on/01_datasets/#blacklisted-regions","title":"Blacklisted regions","text":"<p><code>blacklist.bed</code></p> <p>Blacklisted regions are regions of the genomes with anomalous coverage. We use regions for the hg19 assembly, taken from the ENCODE project portal. These regions were identified with DNAse and ChiP-seq samples over ~60 human tissues/cell types, and had a very high ratio of multi-mapping to unique-mapping reads and high variance in mappability.</p>"},{"location":"hands_on/02_workflow/","title":"Workflow Description","text":"<p>The aim of the pipeline is to process raw RNA-seq data (in FASTQ format) and obtain the list of small variants, SNVs (SNPs and INDELs) for the downstream analysis. The pipeline is based on the GATK best practices for variant calling with RNAseq data and includes all major steps. In addition the pipeline includes SNVs postprocessing and quantification for allele specific expression.</p> <p>Samples processing is done independently for each replicate. This includes mapping of the reads, splitting at the CIGAR, reassigning mapping qualities and recalibrating base qualities.</p> <p>Variant calling is done simultaneously on bam files from all replicates. This allows to improve coverage of genomic regions and obtain more reliable results.</p>"},{"location":"hands_on/02_workflow/#software-manuals","title":"Software manuals","text":"<p>Documentation for all software used in the workflow can be found at the following links:</p> <ul> <li>samtools</li> <li>picard <code>CreateSequenceDictionary</code></li> <li>STAR</li> <li>vcftools</li> <li>GATK tools<ul> <li><code>SplitNCigarReads</code></li> <li><code>BaseRecalibrator</code></li> <li><code>PrintReads</code></li> <li><code>HaplotypeCaller</code></li> <li><code>VariantFiltration</code></li> <li><code>ASEReadCounter</code></li> </ul> </li> </ul>"},{"location":"hands_on/02_workflow/#pipeline-steps","title":"Pipeline steps","text":"<p>In order to get a general idea of the workflow, all the composing steps, together with the corresponding commands, are explained in the next sections.</p>"},{"location":"hands_on/02_workflow/#preparing-data","title":"Preparing data","text":"<p>This step prepares input files for the analysis. Genome indexes are created and variants overlapping blacklisted regions are filtered out.</p> <p>Genome indices with <code>samtools</code> and <code>picard</code> are produced first. They will be needed for GATK commands such as <code>Split'N'Trim</code>:</p> <pre><code>samtools faidx genome.fa\npicard CreateSequenceDictionary R= genome.fa O= genome.dict\n</code></pre> <p>Genome index for <code>STAR</code>, needed for RNA-seq reads mappings, is created next. Index files are written to the folder <code>genome_dir</code> :</p> <pre><code>STAR --runMode genomeGenerate \\\n--genomeDir genome_dir \\\n--genomeFastaFiles genome.fa \\\n--runThreadN 4\n</code></pre> <p>Variants overlapping blacklisted regions are then filtered in order to reduce false positive calls [optional]:</p> <pre><code>vcftools --gzvcf known_variants.vcf.gz -c \\\n--exclude-bed blacklist.bed \\\n--recode | bgzip -c \\\n&gt; known_variants.filtered.recode.vcf.gz\n</code></pre>"},{"location":"hands_on/02_workflow/#mapping-rna-seq-reads-to-the-reference","title":"Mapping RNA-seq reads to the reference","text":"<p>To align RNA-seq reads to the genome we\u2019re using STAR 2-pass approach. The first alignment creates a table with splice-junctions that is used to guide final alignments. The alignments at both steps are done with default parameters.</p> <p>Additional fields with the read groups, libraries and sample information are added into the final bam file at the second mapping step. As a result we do not need to run Picard processing step from GATK best practices.</p> <p>STAR 1-pass:</p> <pre><code>STAR --genomeDir genome_dir \\\n--readFilesIn ENCSR000COQ1_1.fastq.gz ENCSR000COQ1_2.fastq.gz \\\n--runThreadN 4 \\\n--readFilesCommand zcat \\\n--outFilterType BySJout \\\n--alignSJoverhangMin 8 \\\n--alignSJDBoverhangMin 1 \\\n--outFilterMismatchNmax 999\n</code></pre> <p>Create new genome index using splice-junction table:</p> <pre><code>STAR --runMode genomeGenerate \\\n--genomeDir genome_dir \\\n--genomeFastaFiles genome.fa \\\n--sjdbFileChrStartEnd SJ.out.tab \\\n--sjdbOverhang 75 \\\n--runThreadN 4\n</code></pre> <p>STAR 2-pass, final alignments:</p> <pre><code>STAR --genomeDir genome_dir \\\n--readFilesIn ENCSR000COQ1_1.fastq.gz ENCSR000COQ1_2.fastq.gz \\\n--runThreadN 4 \\\n--readFilesCommand zcat \\\n--outFilterType BySJout \\\n--alignSJoverhangMin 8 \\\n--alignSJDBoverhangMin 1 \\\n--outFilterMismatchNmax 999 \\\n--outSAMtype BAM SortedByCoordinate \\\n--outSAMattrRGline ID:ENCSR000COQ1 LB:library PL:illumina PU:machine SM:GM12878\n</code></pre> <p>Index the resulting bam file:</p> <pre><code>samtools index final_alignments.bam\n</code></pre>"},{"location":"hands_on/02_workflow/#splitntrim-and-reassign-mapping-qualities","title":"Split\u2019N'Trim and reassign mapping qualities","text":"<p>The RNA-seq reads overlapping exon-intron junctions can produce false positive variants due to inaccurate splicing. To solve this problem the GATK team recommend to hard-clip any sequence that overlap intronic regions and developed a special tool for this purpose: <code>SplitNCigarReads</code>. The tool identifies Ns in the CIGAR string of the alignment and split reads at this position so that few new reads are created.</p> <p>At this step we also reassign mapping qualities to the alignments. This is important because STAR assign the value <code>255</code> (high quality) to \u201cunknown\u201d mappings that are meaningless to GATK and to variant calling in general.</p> <p>This step is done with recommended parameters from the GATK best practices.</p> <pre><code>java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\\n-R genome.fa -I final_alignments.bam \\\n-o split.bam \\\n-rf ReassignOneMappingQuality \\\n-RMQF 255 -RMQT 60 \\\n-U ALLOW_N_CIGAR_READS \\\n--fix_misencoded_quality_scores\n</code></pre>"},{"location":"hands_on/02_workflow/#base-recalibration","title":"Base Recalibration","text":"<p>The proposed workflow does not include an indel re-alignment step, which is an optional step in the GATK best practices. We excluded that since it is quite time-intensive and does not really improve variant calling.</p> <p>We instead include a base re-calibration step. This step allows to remove possible systematic errors introduced by the sequencing machine during the assignment of read qualities. To do this, the list of known variants is used as a training set to the machine learning algorithm that models possible errors. Base quality scores are then adjusted based on the obtained results.</p> <pre><code>gatk3 -T BaseRecalibrator \\\n--default_platform illumina \\\n-cov ReadGroupCovariate \\\n-cov QualityScoreCovariate \\\n-cov CycleCovariate \\\n-knownSites known_variants.filtered.recode.vcf.gz\\\n-cov ContextCovariate \\\n-R genome.fa -I split.bam \\\n--downsampling_type NONE \\\n-nct 4 \\\n-o final.rnaseq.grp\n</code></pre> <pre><code>gatk3 -T PrintReads \\\n-R genome.fa -I split.bam \\\n-BQSR final.rnaseq.grp \\\n-nct 4 \\\n-o final.bam\n</code></pre>"},{"location":"hands_on/02_workflow/#variant-calling-and-variant-filtering","title":"Variant Calling and Variant filtering","text":"<p>The variant calling is done on the uniquely aligned reads only in order to reduce the number of false positive variants called:</p> <pre><code>(samtools view -H final.bam; samtools view final.bam| grep -w 'NH:i:1') \\\n| samtools view -Sb -  &gt; final.uniq.bam\n</code></pre> <pre><code>samtools index final.uniq.bam\n</code></pre> <p>For variant calling we\u2019re using the GATK tool <code>HaplotypeCaller</code> with default parameters:</p> <pre><code>ls final.uniq.bam  &gt; bam.list\njava -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\\n-R genome.fa -I bam.list \\\n-dontUseSoftClippedBases \\\n-stand_call_conf 20.0 \\\n-o output.gatk.vcf.gz\n</code></pre> <p>Variant filtering is done as recommended in the GATK best practices:</p> <ul> <li>keep clusters of at least 3 SNPs that are within a window of 35 bases between them</li> <li>estimate strand bias using Fisher\u2019s Exact Test with values &gt; 30.0 (Phred-scaled p-value)</li> <li>use variant call confidence score <code>QualByDepth</code> (QD) with values \\&lt; 2.0. The QD is the QUAL score normalized by allele depth (AD) for a variant.</li> </ul> <pre><code>java -jar /usr/gitc/GATK35.jar -T VariantFiltration \\\n-R genome.fa -V output.gatk.vcf.gz \\\n-window 35 -cluster 3 \\\n-filterName FS -filter \"FS &gt; 30.0\" \\\n-filterName QD -filter \"QD &lt; 2.0\" \\\n-o final.vcf\n</code></pre>"},{"location":"hands_on/02_workflow/#variant-post-processing","title":"Variant Post-processing","text":"<p>For downstream analysis we\u2019re considering only sites that pass all filters and are covered with at least 8 reads:</p> <pre><code>grep -v '#' final.vcf \\\n| awk '$7~/PASS/' \\\n| perl -ne 'chomp($_); ($dp)=$_=~/DP\\\\=(\\\\d+)\\\\;/; if($dp&gt;=8){print $_.\"\\\\n\"};' \\\n&gt; result.DP8.vcf\n</code></pre> <p>Filtered RNA-seq variants are compared with those obtained from DNA sequencing (from Illumina platinum genome project). Variants that are common to these two datasets are \"known\" SNVs. The ones present only in the RNA-seq cohort only are \"novel\".</p> <p>Note</p> <p>Known SNVs will be used for allele specific expression analysis.</p> <p>Novel variants will be used to detect RNA-editing events.</p> <p>We compare two variants files to detect common and different sites:</p> <pre><code>vcftools --vcf result.DP8.vcf --gzdiff known_SNVs.filtered.recode.vcf.gz --diff-site --out commonSNPs\n</code></pre> <p>Here we select sites present in both files (\"known\" SNVs only):</p> <pre><code>awk 'BEGIN{OFS=\"\\t\"} $4~/B/{print $1,$2,$3}' commonSNPs.diff.sites_in_files  &gt; test.bed\n</code></pre> <pre><code>vcftools --vcf final.vcf --bed test.bed --recode --keep-INFO-all --stdout &gt; known_snps.vcf\n</code></pre> <p>Plot a histogram with allele frequency distribution for \"known\" SNVs:</p> <pre><code>grep -v '#'  known_snps.vcf \\\n| awk -F '\\\\t' '{print $10}' \\\n| awk -F ':' '{print $2}'\\\n| perl -ne 'chomp($_); \\\n    @v=split(/\\\\,/,$_); if($v[0]!=0 ||$v[1] !=0) \\\n    {print  $v[1]/($v[1]+$v[0]).\"\\\\n\"; }' \\\n| awk '$1!=1' \\\n&gt; AF.4R\n\ngghist.R -i AF.4R -o AF.histogram.pdf\n</code></pre> <p>Calculate read counts for each \"known\" SNVs per allele for allele specific expression analysis:</p> <pre><code>java -jar /usr/gitc/GATK35.jar \\\n-R genome.fa \\\n-T ASEReadCounter \\\n-o ASE.tsv \\\n-I bam.list \\\n-sites known_snps.vcf\n</code></pre>"},{"location":"hands_on/03_setup/","title":"Environment Setup","text":""},{"location":"hands_on/03_setup/#gitpod","title":"Gitpod","text":"<p>This material intends to be a quick hands-on tutorial on Nextflow, so we prepared a Gitpod environment with everything you need to follow it. Gitpod offers a virtual machine with everything already set up for you, accessible from your web browser or built into your code editor (eg. VSCode). To start, click on the button bellow.</p> <p></p> <p>In the gitpod window, you'll see a terminal. Type the following command to switch to the folder of this training material:</p> <pre><code>cd /workspace/gitpod/hands-on\n</code></pre>"},{"location":"hands_on/03_setup/#pipeline-data","title":"Pipeline data","text":"<p>All the files needed for the hands-on activity are stored in the directory shown below:</p> <pre><code>tree /workspace/gitpod/hands-on\n</code></pre> <pre><code>/workspace/gitpod/hands-on\nhands-on\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 bin\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gghist.R\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blacklist.bed\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 genome.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 known_variants.vcf.gz\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 reads\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ1_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ1_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ2_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COQ2_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR1_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR1_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR2_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000COR2_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000CPO1_1.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000CPO1_2.fastq.gz\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ENCSR000CPO2_1.fastq.gz\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ENCSR000CPO2_2.fastq.gz\n\u251c\u2500\u2500 final_main.nf\n\u2514\u2500\u2500 nextflow.config\n</code></pre> <p>4 directories, 19 files</p>"},{"location":"hands_on/03_setup/#pulling-the-docker-image","title":"Pulling the Docker image","text":"<p>Nextflow can pull Docker images at runtime, which is very useful as we usually work with multiple container images. The best practice when it comes to containers and Nextflow is to have a light container image for each process. This makes pulling/running/stopping containers faster and it's easier to debug, when compared to a bulky container image. Nextflow will make sure these container images are pulled, when not found locally, ran as containers, with volumes mounted plus many other things that you don't have to worry.</p> <p>Even though Nextflow takes care of that for you, let\u2019s just download manually one of the container images that we will use to see how Docker works:</p> <pre><code>docker pull cbcrg/callings-with-gatk:latest\n</code></pre> <p>You should see the progress of the download:</p> <pre><code>sha256:93910bf77bc197cb790eca776e42950bc8eff117bdc6e67157295e09c98fc381: Pulling from cbcrg/callings-with-gatk\n915665fee719: Downloading [=============================================&gt;     ] 47.08 MB/51.36 MB\nf332de2321e6: Downloading [===========&gt;                                       ] 41.96 MB/187.8 MB\n1577a6dd9e43: Downloading [===============================&gt;                   ] 46.72 MB/73.45 MB\n7059d9bb5245: Waiting\n71863f70269f: Waiting\nce2a2879246d: Waiting\ne38ba5d5f9fb: Waiting\n90158da87bb2: Waiting\n</code></pre> <p>and the following message when the pull is completed:</p> <pre><code>Digest: sha256:93910bf77bc197cb790eca776e42950bc8eff117bdc6e67157295e09c98fc381\nStatus: Downloaded newer image for cbcrg/callings-with-gatk:latest\n</code></pre> <p>You can run this container and launch bash to interact with it by typing the following command:</p> <pre><code>docker run -ti --rm cbcrg/callings-with-gatk:latest bash\n</code></pre> <p>Once inside, you can check the version of R, for example. You should see something like:</p> <pre><code>root@3e832700345f:/home/pditommaso/projects/callings-nf# Rscript --version\nR scripting front-end version 3.1.1 (2014-07-10)\n</code></pre> <p>Type <code>exit</code> to exit the container and come back to your shell.</p>"},{"location":"hands_on/03_setup/#script-permission","title":"Script permission","text":"<p>Make sure the following R script has execute permissions:</p> <pre><code>chmod +x /workspace/gitpod/hands-on/bin/gghist.R\n</code></pre>"},{"location":"hands_on/04_implementation/","title":"Pipeline Implementation","text":""},{"location":"hands_on/04_implementation/#data-preparation","title":"Data preparation","text":"<p>A first step in any pipeline is to prepare the input data. You will find all the data required to run the pipeline in the folder <code>data</code> within the <code>/workspace/gitpod/hands-on</code> repository directory.</p> <p>There are four data inputs that we will use in this tutorial:</p> <ol> <li>Genome File (<code>data/genome.fa</code>)<ul> <li>Human chromosome 22 in FASTA file format</li> </ul> </li> <li>Read Files (<code>data/reads/</code>)<ul> <li>Sample ENCSR000COQ1: 76bp paired-end reads (<code>ENCSR000COQ1_1.fq.gz</code> and <code>ENCSR000COQ1_2.fq.gz</code>).</li> </ul> </li> <li>Variants File (<code>data/known_variants.vcf.gz</code>)<ul> <li>Known variants, gzipped as a Variant Calling File (VCF) format.</li> </ul> </li> <li>Blacklist File (<code>data/blacklist.bed</code>)<ul> <li>Genomic locations which are known to produce artifacts and spurious variants in Browser Extensible Data (BED) format.</li> </ul> </li> </ol>"},{"location":"hands_on/04_implementation/#input-parameters","title":"Input parameters","text":"<p>We can begin writing the pipeline by creating and editing a text file called <code>main.nf</code> from the <code>/workspace/gitpod/hands-on</code> repository directory with your favourite text editor. In this example we are using <code>code</code>:</p> <pre><code>cd /workspace/gitpod/hands-on\ncode main.nf\n</code></pre> <p>Edit this file to specify the input files as script parameters. Using this notation allows you to override them by specifying different values when launching the pipeline execution.</p> <p>Info</p> <p>Click the  icons in the code for explanations.</p> <pre><code>/*\n * Define the default parameters (1)\n */\nparams.genome     = \"$baseDir/data/genome.fa\" // (2)!\nparams.variants   = \"$baseDir/data/known_variants.vcf.gz\"\nparams.blacklist  = \"$baseDir/data/blacklist.bed\"\nparams.reads      = \"$baseDir/data/reads/ENCSR000COQ1_{1,2}.fastq.gz\" // (3)!\nparams.results    = \"results\" // (4)!\n</code></pre> <ol> <li>The <code>/*</code>, <code>*</code> and <code>*/</code> specify comment lines which are ignored by Nextflow.</li> <li>The <code>baseDir</code> variable represents the main script path location.</li> <li>The <code>reads</code> parameter uses a glob pattern to specify the forward (<code>ENCSR000COQ1_1.fq.gz</code>) and reverse (<code>ENCSR000COQ1_2.fq.gz</code>) reads (paired-end) of a sample.</li> <li>The <code>results</code> parameter is used to specify a directory called <code>results</code>.</li> </ol> <p>Tip</p> <p>You can copy the above text ( top right or Cmd+C), then move in the terminal window, open <code>code</code> and paste using the keyboard Cmd+V shortcut.</p> <p>Once you have the default parameters in the <code>main.nf</code> file, you can save and run the main script for the first time.</p> <p>Tip</p> <p>With <code>code</code> you can save and close the file with Ctrl+O, then Enter, followed by Ctrl+X.</p> <p>To run the main script use the following command:</p> <pre><code>nextflow run main.nf\n</code></pre> <p>You should see the script execute, print Nextflow version and pipeline revision and then exit.</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `main.nf` [elated_davinci] DSL2 - revision: 5187dd3166\n</code></pre> <p>Problem #1</p> <p>Great, now we need to define a channel variable to handle the read-pair files. To do that open the <code>main.nf</code> file and copy the lines below at the end of the file.</p> <p>Tip</p> <p>In <code>code</code> you can move to the end of the file using Ctrl+W and then Ctrl+V.</p> <p>This time you must fill the <code>BLANK</code> space with a channel factory that will create a channel out of the <code>params.reads</code> information.</p> <pre><code>workflow {\nreads_ch = BLANK\n}\n</code></pre> <p>Tip</p> <p>Use the fromFilePairs channel factory.</p> <p>Once you think you have data organised, you can again run the pipeline. However this time, we can use the the <code>-resume</code> flag.</p> <pre><code>nextflow run main.nf -resume\n</code></pre> <p>Tip</p> <p>See here for more details about using the <code>resume</code> option.</p> Solution <pre><code>workflow {\nreads_ch = Channel.fromFilePairs(params.reads) // (1)!\n}\n</code></pre> <ol> <li>Creates a channel using the fromFilePairs() channel factory.</li> </ol>"},{"location":"hands_on/04_implementation/#process-1a-create-a-fasta-genome-index","title":"Process 1A: Create a FASTA genome index","text":"<p>Now we have our inputs set up we can move onto the processes. In our first process we will create a genome index using samtools.</p> <p>The first process has the following structure:</p> <ul> <li>Name: <code>prepare_genome_samtools</code></li> <li>Command: create a genome index for the genome fasta with samtools</li> <li>Input: the genome fasta file</li> <li>Output: the samtools genome index file</li> </ul> <p>Problem #2</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block. Be careful not to accidently have multiple workflow blocks.</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <pre><code>/*\n * Process 1A: Create a FASTA genome index with samtools\n */\nprocess prepare_genome_samtools {\ncontainer 'quay.io/biocontainers/samtools:1.3.1--h0cf4675_11'\ninput:\npath genome\noutput:\npath \"${genome}.fai\"\nscript:\n\"\"\"\n    samtools faidx ${genome}\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nBLANK\n}\n</code></pre> <p>In plain english, the process could be written as:</p> <ul> <li>A process called <code>prepare_genome_samtools</code></li> <li>takes as input the genome file</li> <li>and creates as output a genome index file</li> <li>script: using samtools create the genome index from the genome file</li> </ul> <p>Now when we run the pipeline, we see that the process <code>prepare_genome_samtools</code> is submitted:</p> <p><pre><code>nextflow run main.nf -resume\n</code></pre> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `main.nf` [cranky_bose] - revision: d1df5b7267\nexecutor &gt;  local (1)\n[cd/47f882] process &gt; prepare_genome_samtools [100%] 1 of 1 \u2714\n</code></pre></p> Solution <pre><code>/*\n * Process 1A: Create a FASTA genome index with samtools\n */\nprocess prepare_genome_samtools {\ncontainer 'quay.io/biocontainers/samtools:1.3.1--h0cf4675_11'\ninput:\npath genome\noutput:\npath \"${genome}.fai\"\nscript:\n\"\"\"\n    samtools faidx ${genome}\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome) // (1)!\n}\n</code></pre> <ol> <li>The solution is to provide <code>params.genome</code> as input to the <code>prepare_genome_samtools</code> process.</li> </ol> <p>Warning</p> <p><code>params.genome</code> is just a regular variable, not a channel, but when passed as input to a process, it's automatically converted into a value channel.</p>"},{"location":"hands_on/04_implementation/#process-1b-create-a-fasta-genome-sequence-dictionary-with-picard-for-gatk","title":"Process 1B: Create a FASTA genome sequence dictionary with Picard for GATK","text":"<p>Our first process created the genome index for GATK using samtools. For the next process we must do something very similar, this time creating a genome sequence dictionary using Picard.</p> <p>The next process should have the following structure:</p> <ul> <li>Name: <code>prepare_genome_picard</code></li> <li>Command: create a genome dictionary for the genome fasta with Picard tools</li> <li>Input: the genome fasta file</li> <li>Output: the genome dictionary file</li> </ul> <p>Problem #3</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>Info</p> <p>You can choose any channel output name that makes sense to you.</p> <pre><code>/*\n * Process 1B: Create a FASTA genome sequence dictionary with Picard for GATK\n */\nprocess prepare_genome_picard {\ncontainer 'quay.io/biocontainers/picard:1.141--hdfd78af_6'\ninput:\npath genome\noutput:\npath \"${genome.baseName}.dict\"\nscript:\n\"\"\"\n    picard CreateSequenceDictionary R= $genome O= ${genome.baseName}.dict\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nBLANK\n}\n</code></pre> <p>Note</p> <p><code>.baseName</code> returns the filename without the file suffix. If <code>\"${genome}\"</code> is <code>human.fa</code>, then <code>\"${genome.baseName}.dict\"</code> would be <code>human.dict</code>.</p> Solution <pre><code>/*\n * Process 1B: Create a FASTA genome sequence dictionary with Picard for GATK\n */\nprocess prepare_genome_picard {\ncontainer 'quay.io/biocontainers/picard:1.141--hdfd78af_6'\ninput:\npath genome\noutput:\npath \"${genome.baseName}.dict\"\nscript:\n\"\"\"\n    picard CreateSequenceDictionary R= $genome O= ${genome.baseName}.dict\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome) // (1)!\n}\n</code></pre> <ol> <li>The solution is to provide <code>params.genome</code> as input to the <code>prepare_genome_picard</code> process.</li> </ol>"},{"location":"hands_on/04_implementation/#process-1c-create-star-genome-index-file","title":"Process 1C: Create STAR genome index file","text":"<p>Next we must create a genome index for the STAR mapping software.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>prepare_star_genome_index</code></li> <li>Command: create a STAR genome index for the genome fasta</li> <li>Input: the genome fasta file</li> <li>Output: a directory containing the STAR genome index</li> </ul> <p>Problem #4</p> <p>This is a similar exercise as problem 3.</p> <pre><code>/*\n * Process 1C: Create the genome index file for STAR\n */\nprocess prepare_star_genome_index {\ncontainer 'quay.io/biocontainers/star:2.7.10b--h6b7c446_1'\ninput:\npath genome\noutput:\npath 'genome_dir'\nscript:\n\"\"\"\n    mkdir genome_dir\n    STAR --runMode genomeGenerate \\\n         --genomeDir genome_dir \\\n         --genomeFastaFiles ${genome} \\\n         --runThreadN ${task.cpus}\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nBLANK\n}\n</code></pre> <p>Info</p> <p>The output of the STAR genomeGenerate command is specified here as <code>genome_dir</code>.</p> Solution <pre><code>/*\n* Process 1C: Create the genome index file for STAR\n*/\nprocess prepare_star_genome_index {\ncontainer 'quay.io/biocontainers/star:2.7.10b--h6b7c446_1'\ninput:\npath genome // (1)!\noutput:\npath 'genome_dir' // (2)!\nscript: // (3)!\n\"\"\"\n    mkdir genome_dir\n    STAR --runMode genomeGenerate \\\n         --genomeDir genome_dir \\\n         --genomeFastaFiles ${genome} \\\n         --runThreadN ${task.cpus}\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\n}\n</code></pre> <ol> <li>Take as input the <code>genome</code> file from the <code>params.genome</code> parameter.</li> <li>The <code>output</code> is a <code>path</code> called <code>genome_dir</code></li> <li>Creates the output directory that will contain the resulting STAR genome index.</li> </ol> <p>Note</p> <p>The path in this case is a directory however it makes no difference, it could be a text file, for example.</p>"},{"location":"hands_on/04_implementation/#process-1d-filtered-and-recoded-set-of-variants","title":"Process 1D: Filtered and recoded set of variants","text":"<p>Next on to something a little more tricky. The next process takes two inputs: the variants file and the blacklist file.</p> <p>Info</p> <p>In Nextflow, tuples can be defined in the input or output using the <code>tuple</code> qualifier.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>prepare_vcf_file</code></li> <li>Command: create a filtered and recoded set of variants</li> <li>Input:<ul> <li>the variants file</li> <li>the blacklisted regions file</li> </ul> </li> <li>Output: a tuple containing the filtered/recoded VCF file and the tab index (TBI) file.</li> </ul> <p>Problem #5</p> <p>You must fill in the <code>BLANK</code>.</p> <pre><code>/*\n * Process 1D: Create a file containing the filtered and recoded set of variants\n */\nprocess prepare_vcf_file {\ncontainer 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\ninput:\npath variantsFile\npath blacklisted\noutput:\ntuple path(\"${variantsFile.baseName}.filtered.recode.vcf.gz\"), \\\npath(\"${variantsFile.baseName}.filtered.recode.vcf.gz.tbi\")\nscript:\n\"\"\"\n    vcftools --gzvcf $variantsFile -c \\ #\n             --exclude-bed ${blacklisted} \\\n             --recode | bgzip -c \\\n             &gt; ${variantsFile.baseName}.filtered.recode.vcf.gz\n    tabix ${variantsFile.baseName}.filtered.recode.vcf.gz\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nBLANK\n}\n</code></pre> <p>Broken down, here is what the script is doing:</p> <pre><code>vcftools --gzvcf $variantsFile -c \\ # (1)!\n--exclude-bed ${blacklisted} \\ # (2)!\n--recode | bgzip -c \\\n&gt; ${variantsFile.baseName}.filtered.recode.vcf.gz # (3)!\ntabix ${variantsFile.baseName}.filtered.recode.vcf.gz # (4)!\n</code></pre> <ol> <li>The <code>$variantsFile</code> variable contains the path to the file with the known variants</li> <li>The <code>$blacklisted</code> variable contains the path to the file with the genomic locations which are known to produce artifacts and spurious variants</li> <li>The <code>&gt;</code> symbol is used to redirect the output to the file specified after it</li> <li><code>tabix</code> is used here to create the second output that we want to consider from this process</li> </ol> <p>Try run the pipeline from the project directory with:</p> <pre><code>nextflow run main.nf -resume\n</code></pre> Solution <pre><code>/*\n * Process 1D: Create a file containing the filtered and recoded set of variants\n */\nprocess prepare_vcf_file {\ncontainer 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\ninput:\npath variantsFile\npath blacklisted\noutput:\ntuple path(\"${variantsFile.baseName}.filtered.recode.vcf.gz\"), \\\npath(\"${variantsFile.baseName}.filtered.recode.vcf.gz.tbi\")\nscript:\n\"\"\"\n    vcftools --gzvcf $variantsFile -c \\\n             --exclude-bed ${blacklisted} \\\n             --recode | bgzip -c \\\n             &gt; ${variantsFile.baseName}.filtered.recode.vcf.gz\n    tabix ${variantsFile.baseName}.filtered.recode.vcf.gz\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\n}\n</code></pre> <ul> <li>Take as input the variants file, assigning the name <code>${variantsFile}</code>.</li> <li>Take as input the blacklisted file, assigning the name <code>${blacklisted}</code>.</li> <li>Out a tuple of two files</li> <li>Defines the name of the first output file.</li> <li>Generates the second output file (with <code>.tbi</code> suffix).</li> </ul> <p>Congratulations! Part 1 is now complete.</p> <p>We have all the data prepared and into channels ready for the more serious steps.</p>"},{"location":"hands_on/04_implementation/#process-2-star-mapping","title":"Process 2: STAR Mapping","text":"<p>In this process, for each sample, we align the reads to our genome using the STAR index we created previously.</p> <p>The process has the following structure:</p> <ul> <li>Name: <code>rnaseq_mapping_star</code></li> <li>Command: mapping of the RNA-Seq reads using STAR</li> <li>Input:<ul> <li>the genome fasta file</li> <li>the STAR genome index</li> <li>a tuple containing the replicate id and paired read files</li> </ul> </li> <li>Output: a tuple containing replicate id, aligned bam file &amp; aligned bam file index</li> </ul> <p>Problem #6</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>You must fill the <code>BLANK</code> space with the correct process call and inputs.</p> <pre><code>/*\n * Process 2: Align RNA-Seq reads to the genome with STAR\n */\nprocess rnaseq_mapping_star {\ncontainer 'quay.io/biocontainers/mulled-v2-52f8f283e3c401243cee4ee45f80122fbf6df3bb:e3bc54570927dc255f0e580cba1789b64690d611-0'\ninput:\npath genome\npath genomeDir\ntuple val(replicateId), path(reads)\noutput:\ntuple val(replicateId), path('Aligned.sortedByCoord.out.bam'), path('Aligned.sortedByCoord.out.bam.bai')\nscript:\n\"\"\"\n    # ngs-nf-dev Align reads to genome\n    STAR --genomeDir $genomeDir \\\n         --readFilesIn $reads \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999\n    # 2nd pass (improve alignments using table of splice junctions and create a new index)\n    mkdir genomeDir\n    STAR --runMode genomeGenerate \\\n         --genomeDir genomeDir \\\n         --genomeFastaFiles $genome \\\n         --sjdbFileChrStartEnd SJ.out.tab \\\n         --sjdbOverhang 75 \\\n         --runThreadN ${task.cpus}\n    # Final read alignments\n    STAR --genomeDir genomeDir \\\n         --readFilesIn $reads \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999 \\\n         --outSAMtype BAM SortedByCoordinate \\\n         --outSAMattrRGline ID:$replicateId LB:library PL:illumina PU:machine SM:GM12878\n    # Index the BAM file\n    samtools index Aligned.sortedByCoord.out.bam\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nBLANK\n}\n</code></pre> <p>Info</p> <p>The final command produces an bam index which is the full filename with an additional <code>.bai</code> suffix.</p> Solution <pre><code>/*\n * Process 2: Align RNA-Seq reads to the genome with STAR\n */\nprocess rnaseq_mapping_star {\ncontainer 'quay.io/biocontainers/mulled-v2-52f8f283e3c401243cee4ee45f80122fbf6df3bb:e3bc54570927dc255f0e580cba1789b64690d611-0'\ninput:\npath genome\npath genomeDir\ntuple val(replicateId), path(reads)\noutput:\ntuple val(replicateId), path('Aligned.sortedByCoord.out.bam'), path('Aligned.sortedByCoord.out.bam.bai')\nscript:\n\"\"\"\n    # ngs-nf-dev Align reads to genome\n    STAR --genomeDir $genomeDir \\\n         --readFilesIn $reads \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999\n    # 2nd pass (improve alignments using table of splice junctions and create a new index)\n    mkdir genomeDir\n    STAR --runMode genomeGenerate \\\n         --genomeDir genomeDir \\\n         --genomeFastaFiles $genome \\\n         --sjdbFileChrStartEnd SJ.out.tab \\\n         --sjdbOverhang 75 \\\n         --runThreadN ${task.cpus}\n    # Final read alignments\n    STAR --genomeDir genomeDir \\\n         --readFilesIn $reads \\\n         --runThreadN ${task.cpus} \\\n         --readFilesCommand zcat \\\n         --outFilterType BySJout \\\n         --alignSJoverhangMin 8 \\\n         --alignSJDBoverhangMin 1 \\\n         --outFilterMismatchNmax 999 \\\n         --outSAMtype BAM SortedByCoordinate \\\n         --outSAMattrRGline ID:$replicateId LB:library PL:illumina PU:machine SM:GM12878\n    # Index the BAM file\n    samtools index Aligned.sortedByCoord.out.bam\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\n}\n</code></pre> <p>The next step is a filtering step using GATK. For each sample, we split all the reads that contain N characters in their CIGAR string.</p>"},{"location":"hands_on/04_implementation/#process-3-gatk-split-on-n","title":"Process 3: GATK Split on N","text":"<p>The process creates <code>k+1</code> new reads (where <code>k</code> is the number of <code>N</code> cigar elements) that correspond to the segments of the original read beside/between the splicing events represented by the <code>N</code>s in the original CIGAR.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>rnaseq_gatk_splitNcigar</code></li> <li>Command: split reads on Ns in CIGAR string using GATK</li> <li>Input:<ul> <li>the genome fasta file</li> <li>the genome index made with samtools</li> <li>the genome dictionary made with picard</li> <li>a tuple containing replicate id, aligned bam file and aligned bam file index from the STAR mapping</li> </ul> </li> <li>Output: a tuple containing the replicate id, the split bam file and the split bam index file</li> </ul> <p>Problem #7</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>You must fill the <code>BLANK</code> space with the correct process call and inputs.</p> <p>Warning</p> <p>There is an optional <code>tag</code> line added to the start of this process. The <code>tag</code> line allows you to assign a name to a specific task (single instance of a process). This is particularly useful when there are many samples/replicates which pass through the same process.</p> <pre><code>/*\n * Process 3: GATK Split on N\n */\nprocess rnaseq_gatk_splitNcigar {\ncontainer 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\ntag \"$replicateId\"\ninput:\npath genome\npath index\npath genome_dict\ntuple val(replicateId), path(bam), path(bai)\noutput:\ntuple val(replicateId), path('split.bam'), path('split.bai')\nscript:\n\"\"\"\n    # SplitNCigarReads and reassign mapping qualities\n    java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\\n                                   -R $genome -I $bam \\\n                                   -o split.bam \\\n                                   -rf ReassignOneMappingQuality \\\n                                   -RMQF 255 -RMQT 60 \\\n                                   -U ALLOW_N_CIGAR_READS \\\n                                   --fix_misencoded_quality_scores\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nBLANK\n}\n</code></pre> <p>Info</p> <p>The GATK command above automatically creates a bam index (<code>.bai</code>) of the <code>split.bam</code> output file</p> <p>Example</p> <p>A <code>tag</code> line would also be useful in Process 2</p> Solution <pre><code>/*\n * Process 3: GATK Split on N\n */\nprocess rnaseq_gatk_splitNcigar {\ncontainer 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\ntag \"$replicateId\"\ninput:\npath genome\npath index\npath genome_dict\ntuple val(replicateId), path(bam), path(bai)\noutput:\ntuple val(replicateId), path('split.bam'), path('split.bai')\nscript:\n\"\"\"\n    # SplitNCigarReads and reassign mapping qualities\n    java -jar /usr/gitc/GATK35.jar -T SplitNCigarReads \\\n                                   -R $genome -I $bam \\\n                                   -o split.bam \\\n                                   -rf ReassignOneMappingQuality \\\n                                   -RMQF 255 -RMQT 60 \\\n                                   -U ALLOW_N_CIGAR_READS \\\n                                   --fix_misencoded_quality_scores\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\n}\n</code></pre> <ul> <li><code>tag</code> line using the replicate id as the tag.</li> <li>the genome fasta file</li> <li>the genome index in the output channel from the <code>prepare_genome_samtools</code> process</li> <li>the genome dictionary in the output channel from the <code>prepare_genome_picard</code> process</li> <li>the set containing the aligned reads in the output channel from the <code>rnaseq_mapping_star</code> process</li> <li>a set containing the sample id, the split bam file and the split bam index</li> <li>specifies the input file names <code>$genome</code> and <code>$bam</code> to GATK</li> <li>specifies the output file names to GATK</li> </ul> <p>Next we perform a Base Quality Score Recalibration step using GATK.</p>"},{"location":"hands_on/04_implementation/#process-4-gatk-recalibrate","title":"Process 4: GATK Recalibrate","text":"<p>This step uses GATK to detect systematic errors in the base quality scores, select unique alignments and then index the resulting bam file with samtools. You can find details of the specific GATK BaseRecalibrator parameters here.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>rnaseq_gatk_recalibrate</code></li> <li>Command: recalibrate reads from each replicate using GATK</li> <li>Input<ul> <li>the genome fasta file</li> <li>the genome index made with samtools</li> <li>the genome dictionary made with picard</li> <li>a tuple containing replicate id, aligned bam file and aligned bam file index from process 3</li> <li>a tuple containing the filtered/recoded VCF file and the tab index (TBI) file from process 1D</li> </ul> </li> <li>Output: a tuple containing the sample id, the unique bam file and the unique bam index file</li> </ul> <p>Problem #8</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block.</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <pre><code>/*\n * Process 4: GATK Recalibrate\n */\nprocess rnaseq_gatk_recalibrate {\ncontainer 'quay.io/biocontainers/mulled-v2-aa1d7bddaee5eb6c4cbab18f8a072e3ea7ec3969:f963c36fd770e89d267eeaa27cad95c1c3dbe660-0'\ntag \"$replicateId\"\ninput:\npath genome\npath index\npath dict\ntuple val(replicateId), path(bam), path(bai)\ntuple path(prepared_variants_file), path(prepared_variants_file_index)\noutput:\ntuple val(sampleId), path(\"${replicateId}.final.uniq.bam\"), path(\"${replicateId}.final.uniq.bam.bai\")\nscript:\nsampleId = replicateId.replaceAll(/[12]$/,'')\n\"\"\"\n    # Indel Realignment and Base Recalibration\n    gatk3 -T BaseRecalibrator \\\n          --default_platform illumina \\\n          -cov ReadGroupCovariate \\\n          -cov QualityScoreCovariate \\\n          -cov CycleCovariate \\\n          -knownSites ${prepared_variants_file} \\\n          -cov ContextCovariate \\\n          -R ${genome} -I ${bam} \\\n          --downsampling_type NONE \\\n          -nct ${task.cpus} \\\n          -o final.rnaseq.grp\n    gatk3 -T PrintReads \\\n          -R ${genome} -I ${bam} \\\n          -BQSR final.rnaseq.grp \\\n          -nct ${task.cpus} \\\n          -o final.bam\n    # Select only unique alignments, no multimaps\n    (samtools view -H final.bam; samtools view final.bam| grep -w 'NH:i:1') \\\n    |samtools view -Sb -  &gt; ${replicateId}.final.uniq.bam\n    # Index BAM files\n    samtools index ${replicateId}.final.uniq.bam\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nBLANK\n}\n</code></pre> <ul> <li>The unique bam file</li> <li>The index of the unique bam file (bam file name + <code>.bai</code>)</li> </ul> Solution <pre><code>/*\n * Process 4: GATK Recalibrate\n */\nprocess rnaseq_gatk_recalibrate {\ncontainer 'quay.io/biocontainers/mulled-v2-aa1d7bddaee5eb6c4cbab18f8a072e3ea7ec3969:f963c36fd770e89d267eeaa27cad95c1c3dbe660-0'\ntag \"$replicateId\"\ninput:\npath genome\npath index\npath dict\ntuple val(replicateId), path(bam), path(bai)\ntuple path(prepared_variants_file), path(prepared_variants_file_index)\noutput:\ntuple val(sampleId), path(\"${replicateId}.final.uniq.bam\"), path(\"${replicateId}.final.uniq.bam.bai\")\nscript:\nsampleId = replicateId.replaceAll(/[12]$/,'')\n\"\"\"\n    # Indel Realignment and Base Recalibration\n    gatk3 -T BaseRecalibrator \\\n          --default_platform illumina \\\n          -cov ReadGroupCovariate \\\n          -cov QualityScoreCovariate \\\n          -cov CycleCovariate \\\n          -knownSites ${prepared_variants_file} \\\n          -cov ContextCovariate \\\n          -R ${genome} -I ${bam} \\\n          --downsampling_type NONE \\\n          -nct ${task.cpus} \\\n          -o final.rnaseq.grp\n    gatk3 -T PrintReads \\\n          -R ${genome} -I ${bam} \\\n          -BQSR final.rnaseq.grp \\\n          -nct ${task.cpus} \\\n          -o final.bam\n    # Select only unique alignments, no multimaps\n    (samtools view -H final.bam; samtools view final.bam| grep -w 'NH:i:1') \\\n    |samtools view -Sb -  &gt; ${replicateId}.final.uniq.bam\n    # Index BAM files\n    samtools index ${replicateId}.final.uniq.bam\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\n}\n</code></pre> <ul> <li>the genome fasta file.</li> <li>the genome index in the output channel from the <code>prepare_genome_samtools</code> process.</li> <li>the genome dictionary in the output channel from the <code>prepare_genome_picard</code> process.</li> <li>the set containing the split reads in the output channel from the <code>rnaseq_gatk_splitNcigar</code> process.</li> <li>the set containing the filtered/recoded VCF file and the tab index (TBI) file in the output channel from the <code>prepare_vcf_file</code> process.</li> <li>the set containing the replicate id, the unique bam file and the unique bam index file which goes into two channels.</li> <li>line specifying the filename of the output bam file</li> </ul> <p>Now we are ready to perform the variant calling with GATK.</p>"},{"location":"hands_on/04_implementation/#process-5-gatk-variant-calling","title":"Process 5: GATK Variant Calling","text":"<p>This steps call variants with GATK HaplotypeCaller. You can find details of the specific GATK HaplotypeCaller parameters here.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>rnaseq_call_variants</code></li> <li>Command: variant calling of each sample using GATK</li> <li>Input:<ul> <li>the genome fasta file</li> <li>the genome index made with samtools</li> <li>the genome dictionary made with picard</li> <li>a tuple containing replicate id, aligned bam file and aligned bam file index from process 4</li> </ul> </li> <li>Output: a tuple containing the sample id the resulting variant calling file (vcf)</li> </ul> <p>Problem #9</p> <p>Copy the code below and paste it at the end of <code>main.nf</code>, removing the previous workflow block. Be careful not to accidently have multiple workflow blocks.</p> <p>Warning</p> <p>Note that in process 4, we used the sampleID (not replicateID) as the first element of the tuple in the output. Now we combine the replicates by grouping them on the sample ID. It follows from this that process 4 is run one time per replicate and process 5 is run one time per sample.</p> <p>Your aim is to replace the <code>BLANK</code> placeholder with the the correct process call.</p> <pre><code>/*\n * Process 5: GATK Variant Calling\n */\nprocess rnaseq_call_variants {\ncontainer 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\ntag \"$sampleId\"\ninput:\npath genome\npath index\npath dict\ntuple val(sampleId), path(bam), path(bai)\noutput:\ntuple val(sampleId), path('final.vcf')\nscript:\n\"\"\"\n    echo \"${bam.join('\\n')}\" &gt; bam.list\n    # Variant calling\n    java -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\\n                                   -R $genome -I bam.list \\\n                                   -dontUseSoftClippedBases \\\n                                   -stand_call_conf 20.0 \\\n                                   -o output.gatk.vcf.gz\n    # Variant filtering\n    java -jar /usr/gitc/GATK35.jar -T VariantFiltration \\\n                                   -R $genome -V output.gatk.vcf.gz \\\n                                   -window 35 -cluster 3 \\\n                                   -filterName FS -filter \"FS &gt; 30.0\" \\\n                                   -filterName QD -filter \"QD &lt; 2.0\" \\\n                                   -o final.vcf\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\nBLANK\n}\n</code></pre> Solution <pre><code>/*\n * Process 5: GATK Variant Calling\n */\nprocess rnaseq_call_variants {\ntag \"$sampleId\"\ninput:\npath genome\npath index\npath dict\ntuple val(sampleId), path(bam), path(bai)\noutput:\ntuple val(sampleId), path('final.vcf')\nscript:\n\"\"\"\n    echo \"${bam.join('\\n')}\" &gt; bam.list\n    # Variant calling\n    java -jar /usr/gitc/GATK35.jar -T HaplotypeCaller \\\n                                   -R $genome -I bam.list \\\n                                   -dontUseSoftClippedBases \\\n                                   -stand_call_conf 20.0 \\\n                                   -o output.gatk.vcf.gz\n    # Variant filtering\n    java -jar /usr/gitc/GATK35.jar -T VariantFiltration \\\n                                   -R $genome -V output.gatk.vcf.gz \\\n                                   -window 35 -cluster 3 \\\n                                   -filterName FS -filter \"FS &gt; 30.0\" \\\n                                   -filterName QD -filter \"QD &lt; 2.0\" \\\n                                   -o final.vcf\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\nrnaseq_call_variants(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_recalibrate.out)\n}\n</code></pre> <ul> <li><code>tag</code> line with the using the sample id as the tag.</li> <li>the genome fasta file.</li> <li>the genome index in the output channel from the <code>prepare_genome_samtools</code> process.</li> <li>the genome dictionary in the output channel from the <code>prepare_genome_picard</code> process.</li> <li>the sets grouped by sampleID in the output channel from the <code>rnaseq_gatk_recalibrate</code> process.</li> <li>the set containing the sample ID and final VCF file.</li> <li>the line specifying the name resulting final VCF file.</li> </ul>"},{"location":"hands_on/04_implementation/#processes-6a-and-6b-ase-rna-editing","title":"Processes 6A and 6B: ASE &amp; RNA Editing","text":"<p>In the final steps we will create processes for Allele-Specific Expression and RNA Editing Analysis.</p> <p>We must process the VCF result to prepare variants file for allele specific expression (ASE) analysis. We will implement both processes together.</p> <p>You should implement two processes having the following structure:</p> <ul> <li>1st process<ul> <li>Name: <code>post_process_vcf</code></li> <li>Command: post-process the variant calling file (vcf) of each sample</li> <li>Input:<ul> <li>tuple containing the sample ID and vcf file</li> <li>a tuple containing the filtered/recoded VCF file and the tab index (TBI) file from process 1D</li> </ul> </li> <li>Output: a tuple containing the sample id, the variant calling file (vcf) and a file containing common SNPs</li> </ul> </li> <li>2nd process<ul> <li>Name: <code>prepare_vcf_for_ase</code></li> <li>Command: prepare the VCF for allele specific expression (ASE) and generate a figure in R.</li> <li>Input: a tuple containing the sample id, the variant calling file (vcf) and a file containing common SNPs</li> <li>Output:<ul> <li>a tuple containing the sample ID and known SNPs in the sample for ASE</li> <li>a figure of the SNPs generated in R as a PDF file</li> </ul> </li> </ul> </li> </ul> <p>Problem #10</p> <p>Here we introduce the <code>publishDir</code> directive. This allows us to specify a location for the outputs of the process. See here for more details.</p> <p>You must have the output of process 6A become the input of process 6B.</p> <pre><code>/*\n * Processes 6: ASE &amp; RNA Editing\n */\nprocess post_process_vcf {\ncontainer 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\ntag \"$sampleId\"\npublishDir \"$params.results/$sampleId\" // (1)!\ninput:\ntuple val(sampleId), path('final.vcf')\ntuple path('filtered.recode.vcf.gz'), path('filtered.recode.vcf.gz.tbi')\noutput:\ntuple val(sampleId), path('final.vcf'), path('commonSNPs.diff.sites_in_files')\nscript:\n'''\n    grep -v '#' final.vcf | awk '$7~/PASS/' |perl -ne 'chomp($_); ($dp)=$_=~/DP\\\\=(\\\\d+)\\\\;/; if($dp&gt;=8){print $_.\"\\\\n\"};' &gt; result.DP8.vcf\n    vcftools --vcf result.DP8.vcf --gzdiff filtered.recode.vcf.gz  --diff-site --out commonSNPs\n    '''\n}\nprocess prepare_vcf_for_ase {\ntag \"$sampleId\"\npublishDir \"$params.results/$sampleId\"\ninput:\ntuple val(sampleId), path('final.vcf'), path('commonSNPs.diff.sites_in_files')\noutput:\ntuple val(sampleId), path('known_snps.vcf'), emit: vcf_for_ASE\npath 'AF.histogram.pdf'                    , emit: gghist_pdfs\nscript:\n'''\n    awk 'BEGIN{OFS=\"\\t\"} $4~/B/{print $1,$2,$3}' commonSNPs.diff.sites_in_files  &gt; test.bed\n    vcftools --vcf final.vcf --bed test.bed --recode --keep-INFO-all --stdout &gt; known_snps.vcf\n    grep -v '#'  known_snps.vcf | awk -F '\\\\t' '{print $10}' \\\n                |awk -F ':' '{print $2}'|perl -ne 'chomp($_); \\\n                @v=split(/\\\\,/,$_); if($v[0]!=0 ||$v[1] !=0)\\\n                {print  $v[1]/($v[1]+$v[0]).\"\\\\n\"; }' |awk '$1!=1' \\\n                &gt;AF.4R\n    gghist.R -i AF.4R -o AF.histogram.pdf\n    '''\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\nrnaseq_call_variants(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_recalibrate.out)\nBLANK\n}\n</code></pre> <ol> <li>The path to the <code>publishDir</code> process directive consists of variables that will be evaluated before saving the files over there</li> </ol> Solution <pre><code>/*\n * Processes 6: ASE &amp; RNA Editing\n */\nprocess post_process_vcf {\ncontainer 'quay.io/biocontainers/mulled-v2-b9358559e3ae3b9d7d8dbf1f401ae1fcaf757de3:ac05763cf181a5070c2fdb9bb5461f8d08f7b93b-0'\ntag \"$sampleId\"\npublishDir \"$params.results/$sampleId\"\ninput:\ntuple val(sampleId), path('final.vcf')\ntuple path('filtered.recode.vcf.gz'), path('filtered.recode.vcf.gz.tbi')\noutput:\ntuple val(sampleId), path('final.vcf'), path('commonSNPs.diff.sites_in_files')\nscript:\n'''\n    grep -v '#' final.vcf | awk '$7~/PASS/' |perl -ne 'chomp($_); ($dp)=$_=~/DP\\\\=(\\\\d+)\\\\;/; if($dp&gt;=8){print $_.\"\\\\n\"};' &gt; result.DP8.vcf\n    vcftools --vcf result.DP8.vcf --gzdiff filtered.recode.vcf.gz  --diff-site --out commonSNPs\n    '''\n}\nprocess prepare_vcf_for_ase {\ncontainer 'cbcrg/callings-with-gatk:latest'\ntag \"$sampleId\"\npublishDir \"$params.results/$sampleId\"\ninput:\ntuple val(sampleId), path('final.vcf'), path('commonSNPs.diff.sites_in_files')\noutput:\ntuple val(sampleId), path('known_snps.vcf'), emit: vcf_for_ASE\npath 'AF.histogram.pdf'                    , emit: gghist_pdfs\nscript:\n'''\n    awk 'BEGIN{OFS=\"\\t\"} $4~/B/{print $1,$2,$3}' commonSNPs.diff.sites_in_files  &gt; test.bed\n    vcftools --vcf final.vcf --bed test.bed --recode --keep-INFO-all --stdout &gt; known_snps.vcf\n    grep -v '#'  known_snps.vcf | awk -F '\\\\t' '{print $10}' \\\n                |awk -F ':' '{print $2}'|perl -ne 'chomp($_); \\\n                @v=split(/\\\\,/,$_); if($v[0]!=0 ||$v[1] !=0)\\\n                {print  $v[1]/($v[1]+$v[0]).\"\\\\n\"; }' |awk '$1!=1' \\\n                &gt;AF.4R\n    gghist.R -i AF.4R -o AF.histogram.pdf\n    '''\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\nrnaseq_call_variants(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_recalibrate.out)\npost_process_vcf(rnaseq_call_variants.out,\nprepare_vcf_file.out)\nprepare_vcf_for_ase(post_process_vcf.out)\n}\n</code></pre> <p>The final step is the GATK ASEReadCounter.</p> <p>Problem #11</p> <p>We have seen the basics of using processes in Nextflow. Yet one of the features of Nextflow is the operations that can be performed on channels outside of processes. See here for details on the specific operators.</p> <p>Before we perform the GATK ASEReadCounter process, we must group the data for allele-specific expression. To do this we must combine channels.</p> <p>The <code>bam_for_ASE_ch</code> channel emites tuples having the following structure, holding the final BAM/BAI files:</p> <pre><code>&lt; sample_id, file_bam, file_bai &gt;\n</code></pre> <p>The <code>vcf_for_ASE</code> channel emits tuples having the following structure:</p> <pre><code>&lt; sample_id, output.vcf &gt;\n</code></pre> <p>In the first operation, the BAMs are grouped together by sample id.</p> <p>Next, this resulting channel is merged with the VCFs having the same sample id.</p> <p>We must take the merged channel and creates a channel named <code>grouped_vcf_bam_bai_ch</code> emitting the following tuples:</p> <pre><code>&lt; sample_id, file_vcf, List[file_bam], List[file_bai] &gt;\n</code></pre> <p>Your aim is to fill in the <code>BLANKS</code> below.</p> <pre><code>rnaseq_gatk_recalibrate\n.out\n.BLANK // (1)!\n.BLANK // (2)!\n.map { BLANK } // (3)!\n.set { BLANK } // (4)!\n</code></pre> <ol> <li>an operator that groups tuples that contain a common first element.</li> <li>an operator that joins two channels taking a key into consideration. See here for more details</li> <li>the map operator can apply any function to every item on a channel. In this case we take our tuple from the previous setp, define the separate elements and create a new tuple.</li> <li>rename the resulting as <code>grouped_vcf_bam_bai_ch</code></li> </ol> Solution <pre><code>workflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\nrnaseq_call_variants(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_recalibrate.out)\npost_process_vcf(rnaseq_call_variants.out,\nprepare_vcf_file.out)\nprepare_vcf_for_ase(post_process_vcf.out)\nrnaseq_gatk_recalibrate\n.out\n.groupTuple()\n.join(prepare_vcf_for_ase.out.vcf_for_ASE)\n.map { meta, bams, bais, vcf -&gt; [meta, vcf, bams, bais] }\n.set { grouped_vcf_bam_bai_ch }\n}\n</code></pre>"},{"location":"hands_on/04_implementation/#process-7-allele-specific-expression-analysis-with-gatk-asereadcounter","title":"Process 7: Allele-Specific Expression analysis with GATK ASEReadCounter","text":"<p>Now we are ready for the final process.</p> <p>The next process has the following structure:</p> <ul> <li>Name: <code>ASE_knownSNPs</code></li> <li>Command: calculate allele counts at a set of positions with GATK tools</li> <li>Input:<ul> <li>genome fasta file</li> <li>genome index file from samtools</li> <li>genome dictionary file</li> <li>the <code>grouped_vcf_bam_bai_ch</code> channel</li> </ul> </li> <li>Output: the allele specific expression file (<code>ASE.tsv</code>)</li> </ul> <p>Problem #12</p> <p>You should construct the process from scratch, add the process call and inputs to the workflow block and run the pipeline in its entirety.</p> <pre><code>echo \"${bam.join('\\n')}\" &gt; bam.list\n\njava -jar /usr/gitc/GATK35.jar -R ${genome} \\\n-T ASEReadCounter \\\n-o ASE.tsv \\\n-I bam.list \\\n-sites ${vcf}\n</code></pre> Solution <pre><code>/*\n * Processes 7: Allele-Specific Expression analysis with GATK ASEReadCounter\n */\nprocess ASE_knownSNPs {\ncontainer 'quay.io/broadinstitute/gotc-prod-gatk:1.0.0-4.1.8.0-1626439571'\ntag \"$sampleId\"\npublishDir \"$params.results/$sampleId\"\ninput:\npath genome\npath index\npath dict\ntuple val(sampleId), path(vcf), path(bam), path(bai)\noutput:\npath \"ASE.tsv\"\nscript:\n\"\"\"\n    echo \"${bam.join('\\n')}\" &gt; bam.list\n    java -jar /usr/gitc/GATK35.jar -R ${genome} \\\n                                   -T ASEReadCounter \\\n                                   -o ASE.tsv \\\n                                   -I bam.list \\\n                                   -sites ${vcf}\n    \"\"\"\n}\nworkflow {\nreads_ch = Channel.fromFilePairs(params.reads)\nprepare_genome_samtools(params.genome)\nprepare_genome_picard(params.genome)\nprepare_star_genome_index(params.genome)\nprepare_vcf_file(params.variants, params.blacklist)\nrnaseq_mapping_star(params.genome,\nprepare_star_genome_index.out,\nreads_ch)\nrnaseq_gatk_splitNcigar(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_mapping_star.out)\nrnaseq_gatk_recalibrate(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_splitNcigar.out,\nprepare_vcf_file.out)\nrnaseq_call_variants(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\nrnaseq_gatk_recalibrate.out)\npost_process_vcf(rnaseq_call_variants.out,\nprepare_vcf_file.out)\nprepare_vcf_for_ase(post_process_vcf.out)\nrnaseq_gatk_recalibrate\n.out\n.groupTuple()\n.join(prepare_vcf_for_ase.out.vcf_for_ASE)\n.map { meta, bams, bais, vcf -&gt; [meta, vcf, bams, bais] }\n.set { grouped_vcf_bam_bai_ch }\nASE_knownSNPs(params.genome,\nprepare_genome_samtools.out,\nprepare_genome_picard.out,\ngrouped_vcf_bam_bai_ch)\n}\n</code></pre> <p>Congratulations! If you made it this far you now have all the basics to create your own Nextflow workflows.</p>"},{"location":"hands_on/04_implementation/#results-overview","title":"Results overview","text":"<p>For each processed sample the pipeline stores results into a folder named after the sample identifier. These folders are created in the directory specified as a parameter in <code>params.results</code>.</p> <p>Result files for this workshop can be found in the folder <code>results</code> within the current folder. There you should see a directory called <code>ENCSR000COQ/</code> containing the following files:</p>"},{"location":"hands_on/04_implementation/#variant-calls","title":"Variant calls","text":"<p><code>final.vcf</code></p> <p>This file contains all somatic variants (SNVs) called from RNAseq data. You will see variants that pass all filters, with the <code>PASS</code> keyword in the 7th field of the vcf file (<code>filter status</code>), and also those that did not pass one or more filters.</p> <p><code>commonSNPs.diff.sites_in_files</code></p> <p>Tab-separated file with comparison between variants obtained from RNAseq and \"known\" variants from DNA.</p> <p>The file is sorted by genomic position and contains 8 fields:</p> 1 <code>CHROM</code> chromosome name; 2 <code>POS1</code> position of the SNV in file #1 (RNAseq data); 3 <code>POS2</code> position of SNV in file #2 (DNA \"known\" variants); 4 <code>IN_FILE</code> flag whether SNV is present in the file #1 1, in the file #2 2, or in both files B; 5 <code>REF1</code> reference sequence in the file 1; 6 <code>REF2</code> reference sequence in the file 2; 7 <code>ALT1</code> alternative sequence in the file 1; 8 <code>ALT2</code> alternative sequence in the file 2 <p><code>known_snps.vcf</code></p> <p>Variants that are common to RNAseq and \"known\" variants from DNA.</p>"},{"location":"hands_on/04_implementation/#allele-specific-expression-quantification","title":"Allele specific expression quantification","text":"<p><code>ASE.tsv</code></p> <p>Tab-separated file with allele counts at common SNVs positions (only SNVs from the file <code>known_snps.vcf</code>)</p> <p>The file is sorted by coordinates and contains 13 fields:</p> 1 <code>contig</code> contig, scaffold or chromosome name of the variant 2 <code>position</code> position of the variant 3 <code>variant ID</code> variant ID in the dbSNP 4 <code>refAllele</code> reference allele sequence 5 <code>altAllele</code> alternate allele sequence 6 <code>refCount</code> number of reads that support the reference allele 7 <code>altCount</code> number of reads that support the alternate allele 8 <code>totalCount</code> total number of reads at the site that support both reference and alternate allele and any other alleles present at the site 9 <code>lowMAPQDepth</code> number of reads that have low mapping quality 10 <code>lowBaseQDepth</code> number of reads that have low base quality 11 <code>rawDepth</code> total number of reads at the site that support both reference and alternate allele and any other alleles present at the site 12 <code>otherBases</code> number of reads that support bases other than reference and alternate bases 13 <code>improperPairs</code> number of reads that have malformed pairs"},{"location":"hands_on/04_implementation/#allele-frequency-histogram","title":"Allele frequency histogram","text":"<p><code>AF.histogram.pdf</code></p> <p>This file contains a histogram plot of allele frequency for SNVs common to RNA-seq and \"known\" variants from DNA.</p>"},{"location":"hands_on/04_implementation/#bonus-step","title":"Bonus step","text":"<p>Until now the pipeline has been executed using just a single sample (<code>ENCSR000COQ1</code>).</p> <p>Now we can re-execute the pipeline specifying a large set of samples by using the command shown below:</p> <pre><code>nextflow run main.nf -resume --reads 'data/reads/ENCSR000C*_{1,2}.fastq.gz'\n</code></pre> <p>Or run the final version of the Nextflow pipeline that is already prepared for you:</p> <pre><code>nextflow run final_main.nf -resume\n</code></pre> <p>It will print an output similar to the one below:</p> <pre><code>N E X T F L O W  ~  version 23.04.1\nLaunching `main.nf` [nice_kirch] DSL2 - revision: 45de8f41e4\nexecutor &gt;  local (43)\n[8f/0010ac] process &gt; prepare_genome_samtools                [100%] 1 of 1 \u2714\n[7e/9404fb] process &gt; prepare_genome_picard                  [100%] 1 of 1 \u2714\n[c1/52f4c2] process &gt; prepare_star_genome_index              [100%] 1 of 1 \u2714\n[d7/edc7d4] process &gt; prepare_vcf_file                       [100%] 1 of 1 \u2714\n[c6/469237] process &gt; rnaseq_mapping_star (5)                [100%] 6 of 6 \u2714\n[c4/c689ec] process &gt; rnaseq_gatk_splitNcigar (ENCSR000CPO1) [100%] 6 of 6 \u2714\n[8f/3b28cd] process &gt; rnaseq_gatk_recalibrate (ENCSR000CPO2) [100%] 6 of 6 \u2714\n[8e/3305f0] process &gt; rnaseq_call_variants (ENCSR000CPO)     [100%] 6 of 6 \u2714\n[fd/02e430] process &gt; post_process_vcf (ENCSR000CPO)         [100%] 6 of 6 \u2714\n[11/9af6f7] process &gt; prepare_vcf_for_ase (ENCSR000CPO)      [100%] 6 of 6 \u2714\n[33/6f0d90] process &gt; ASE_knownSNPs (ENCSR000CPO)            [100%] 3 of 3 \u2714\n</code></pre> <p>You can notice that this time the pipeline spawns the execution of more tasks because three samples have been provided instead of one.</p> <p>This shows the ability of Nextflow to implicitly handle multiple parallel task executions depending on the specified pipeline input dataset.</p> <p>A fully functional version of this pipeline is available at the following GitHub repository: CalliNGS-NF.</p>"}]}